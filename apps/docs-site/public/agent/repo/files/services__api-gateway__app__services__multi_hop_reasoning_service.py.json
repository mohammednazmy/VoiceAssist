{
  "path": "services/api-gateway/app/services/multi_hop_reasoning_service.py",
  "language": "python",
  "size": 29852,
  "last_modified": "2025-12-04T11:26:58.774Z",
  "lines": 899,
  "content": "\"\"\"\nMulti-Hop Reasoning Service\n\nProvides advanced RAG capabilities with multi-step reasoning:\n- Query decomposition into sub-questions\n- Iterative retrieval and answer generation\n- Chain-of-thought reasoning synthesis\n- Confidence scoring for medical queries\n\nThis service enables the AI assistant to answer complex medical\nquestions that require combining information from multiple sources.\n\"\"\"\n\nimport asyncio\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nfrom typing import Any, Callable, Dict, List, Optional\n\nfrom app.core.config import settings\nfrom app.core.logging import get_logger\n\nlogger = get_logger(__name__)\n\n\nclass ReasoningStrategy(Enum):\n    \"\"\"Reasoning strategies for different query types\"\"\"\n\n    DIRECT = \"direct\"  # Single-step retrieval and answer\n    MULTI_HOP = \"multi_hop\"  # Iterative sub-question answering\n    COMPARATIVE = \"comparative\"  # Compare multiple entities\n    CAUSAL = \"causal\"  # Explain cause-effect relationships\n    TEMPORAL = \"temporal\"  # Time-based reasoning\n\n\n@dataclass\nclass SearchResult:\n    \"\"\"Result from hybrid search\"\"\"\n\n    doc_id: str\n    content: str\n    score: float\n    metadata: Dict[str, Any]\n    source: str  # 'semantic', 'keyword', 'hybrid'\n\n\n@dataclass\nclass ReasoningStep:\n    \"\"\"A single step in the reasoning chain\"\"\"\n\n    step_number: int\n    question: str\n    retrieved_docs: List[str]\n    answer: str\n    confidence: float\n    sources: List[str]\n\n\n@dataclass\nclass ReasoningResult:\n    \"\"\"Complete result of multi-hop reasoning\"\"\"\n\n    original_query: str\n    strategy: ReasoningStrategy\n    reasoning_chain: List[ReasoningStep]\n    final_answer: str\n    confidence: float\n    sources: List[str]\n    metadata: Dict[str, Any] = field(default_factory=dict)\n\n\nclass HybridSearchEngine:\n    \"\"\"\n    Hybrid search engine combining semantic and keyword search.\n\n    Features:\n    - Vector similarity search using embeddings\n    - BM25 keyword search for lexical matching\n    - Reciprocal Rank Fusion for result combination\n    - Cross-encoder re-ranking for improved relevance\n    - Medical synonym expansion\n    \"\"\"\n\n    def __init__(self, lazy_load: bool = True):\n        \"\"\"\n        Initialize hybrid search engine.\n\n        Args:\n            lazy_load: If True, load models on first use\n        \"\"\"\n        self._embedding_service = None\n        self._reranker = None\n        self._bm25_index = None\n        self._lazy_load = lazy_load\n        self._loaded = False\n\n        if not lazy_load:\n            self._load_components()\n\n    def _load_components(self) -> bool:\n        \"\"\"Load search components.\"\"\"\n        if self._loaded:\n            return True\n\n        try:\n            # Try to import embedding service\n            from app.services.medical_embedding_service import MedicalEmbeddingService\n\n            self._embedding_service = MedicalEmbeddingService(lazy_load=True)\n            self._loaded = True\n            logger.info(\"HybridSearchEngine components loaded\")\n            return True\n\n        except ImportError as e:\n            logger.warning(f\"Could not load search components: {e}\")\n            return False\n\n    def _ensure_loaded(self) -> bool:\n        \"\"\"Ensure components are loaded.\"\"\"\n        if self._loaded:\n            return True\n        return self._load_components()\n\n    async def search(\n        self,\n        query: str,\n        top_k: int = 10,\n        alpha: float = 0.5,\n        filters: Optional[Dict[str, Any]] = None,\n        rerank: bool = True,\n        expand_query: bool = True,\n    ) -> List[SearchResult]:\n        \"\"\"\n        Perform hybrid search combining semantic and keyword search.\n\n        Args:\n            query: Search query\n            top_k: Number of results to return\n            alpha: Balance between semantic (1.0) and keyword (0.0)\n            filters: Metadata filters\n            rerank: Whether to apply cross-encoder re-ranking\n            expand_query: Whether to expand query with synonyms\n\n        Returns:\n            List of SearchResult objects\n        \"\"\"\n        if not self._ensure_loaded():\n            return []\n\n        # Expand query with medical synonyms\n        expanded_query = query\n        if expand_query:\n            expanded_query = await self._expand_query(query)\n\n        # Run semantic and keyword search in parallel\n        semantic_task = self._semantic_search(expanded_query, top_k * 2, filters)\n        keyword_task = self._keyword_search(expanded_query, top_k * 2, filters)\n\n        semantic_results, keyword_results = await asyncio.gather(semantic_task, keyword_task)\n\n        # Fuse results using Reciprocal Rank Fusion\n        fused_results = self._reciprocal_rank_fusion(semantic_results, keyword_results, alpha=alpha)\n\n        # Re-rank with cross-encoder if available\n        if rerank and fused_results:\n            fused_results = await self._rerank_results(query, fused_results, top_k)\n\n        return fused_results[:top_k]\n\n    async def _expand_query(self, query: str) -> str:\n        \"\"\"\n        Expand query with medical synonyms and related terms.\n\n        Args:\n            query: Original query\n\n        Returns:\n            Expanded query string\n        \"\"\"\n        # Medical synonym mappings (subset for common terms)\n        synonyms = {\n            \"heart attack\": [\"myocardial infarction\", \"MI\", \"cardiac infarction\"],\n            \"stroke\": [\"cerebrovascular accident\", \"CVA\", \"brain attack\"],\n            \"high blood pressure\": [\"hypertension\", \"HTN\", \"elevated BP\"],\n            \"diabetes\": [\"diabetes mellitus\", \"DM\", \"hyperglycemia\"],\n            \"cancer\": [\"malignancy\", \"neoplasm\", \"carcinoma\"],\n            \"pain\": [\"discomfort\", \"ache\", \"soreness\"],\n            \"infection\": [\"sepsis\", \"infectious disease\"],\n            \"breathing\": [\"respiration\", \"respiratory\"],\n            \"kidney\": [\"renal\", \"nephro\"],\n            \"liver\": [\"hepatic\", \"hepato\"],\n            \"brain\": [\"cerebral\", \"neuro\", \"cranial\"],\n            \"heart\": [\"cardiac\", \"cardio\", \"cardiovascular\"],\n            \"stomach\": [\"gastric\", \"gastro\", \"abdominal\"],\n            \"lung\": [\"pulmonary\", \"respiratory\"],\n            \"blood\": [\"hematologic\", \"hemo\"],\n        }\n\n        # Find and add synonyms\n        query_lower = query.lower()\n        additional_terms = []\n\n        for term, syns in synonyms.items():\n            if term in query_lower:\n                additional_terms.extend(syns[:2])  # Add up to 2 synonyms\n\n        if additional_terms:\n            return f\"{query} {' '.join(additional_terms)}\"\n\n        return query\n\n    async def _semantic_search(\n        self,\n        query: str,\n        top_k: int,\n        filters: Optional[Dict[str, Any]],\n    ) -> List[SearchResult]:\n        \"\"\"\n        Vector similarity search using embeddings.\n\n        Args:\n            query: Search query\n            top_k: Number of results\n            filters: Optional filters\n\n        Returns:\n            List of SearchResult\n        \"\"\"\n        results = []\n\n        try:\n            # Generate query embedding\n            if self._embedding_service:\n                embedding_result = await self._embedding_service.generate_embedding(query)\n                query_embedding = embedding_result.embedding\n\n                # In production, this would query Qdrant/Pinecone\n                # For now, return empty results - will be implemented with vector DB\n                logger.debug(f\"Semantic search for query (embedding dim: {len(query_embedding)})\")\n\n        except Exception as e:\n            logger.warning(f\"Semantic search failed: {e}\")\n\n        return results\n\n    async def _keyword_search(\n        self,\n        query: str,\n        top_k: int,\n        filters: Optional[Dict[str, Any]],\n    ) -> List[SearchResult]:\n        \"\"\"\n        BM25 keyword search for lexical matching.\n\n        Args:\n            query: Search query\n            top_k: Number of results\n            filters: Optional filters\n\n        Returns:\n            List of SearchResult\n        \"\"\"\n        results = []\n\n        # In production, this would use a BM25 index (Elasticsearch, etc.)\n        # For now, return empty results - will be implemented with search backend\n        logger.debug(f\"Keyword search for query: {query[:50]}...\")\n\n        return results\n\n    def _reciprocal_rank_fusion(\n        self,\n        semantic_results: List[SearchResult],\n        keyword_results: List[SearchResult],\n        alpha: float = 0.5,\n        k: int = 60,\n    ) -> List[SearchResult]:\n        \"\"\"\n        Fuse results using Reciprocal Rank Fusion.\n\n        RRF score = sum(1 / (k + rank_i))\n\n        Args:\n            semantic_results: Results from semantic search\n            keyword_results: Results from keyword search\n            alpha: Weight for semantic vs keyword\n            k: RRF constant (typically 60)\n\n        Returns:\n            Fused and sorted results\n        \"\"\"\n        scores: Dict[str, Dict[str, Any]] = {}\n\n        # Score semantic results\n        for rank, result in enumerate(semantic_results):\n            rrf_score = alpha * (1 / (k + rank + 1))\n            if result.doc_id not in scores:\n                scores[result.doc_id] = {\"result\": result, \"score\": 0}\n            scores[result.doc_id][\"score\"] += rrf_score\n\n        # Score keyword results\n        for rank, result in enumerate(keyword_results):\n            rrf_score = (1 - alpha) * (1 / (k + rank + 1))\n            if result.doc_id not in scores:\n                scores[result.doc_id] = {\"result\": result, \"score\": 0}\n            scores[result.doc_id][\"score\"] += rrf_score\n\n        # Sort by fused score\n        sorted_items = sorted(scores.values(), key=lambda x: x[\"score\"], reverse=True)\n\n        return [\n            SearchResult(\n                doc_id=item[\"result\"].doc_id,\n                content=item[\"result\"].content,\n                score=item[\"score\"],\n                metadata=item[\"result\"].metadata,\n                source=\"hybrid\",\n            )\n            for item in sorted_items\n        ]\n\n    async def _rerank_results(self, query: str, results: List[SearchResult], top_k: int) -> List[SearchResult]:\n        \"\"\"\n        Re-rank results using cross-encoder model.\n\n        Args:\n            query: Original query\n            results: Results to re-rank\n            top_k: Number of results to return\n\n        Returns:\n            Re-ranked results\n        \"\"\"\n        if not results:\n            return []\n\n        try:\n            from sentence_transformers import CrossEncoder\n\n            # Load cross-encoder if not already loaded\n            if self._reranker is None:\n                self._reranker = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n\n            # Prepare pairs for cross-encoder\n            pairs = [(query, doc.content) for doc in results]\n\n            # Get scores (run in thread pool for async)\n            loop = asyncio.get_event_loop()\n            scores = await loop.run_in_executor(None, lambda: self._reranker.predict(pairs))\n\n            # Update scores\n            for doc, score in zip(results, scores):\n                doc.score = float(score)\n\n            # Sort by re-ranked score\n            reranked = sorted(results, key=lambda x: x.score, reverse=True)\n            return reranked[:top_k]\n\n        except ImportError:\n            logger.debug(\"Cross-encoder not available, skipping re-ranking\")\n            return results[:top_k]\n        except Exception as e:\n            logger.warning(f\"Re-ranking failed: {e}\")\n            return results[:top_k]\n\n\nclass MultiHopReasoner:\n    \"\"\"\n    Multi-hop reasoning engine for complex medical queries.\n\n    This service decomposes complex questions into simpler sub-questions,\n    answers each iteratively with retrieval, and synthesizes a final\n    comprehensive answer.\n\n    Features:\n    - Query decomposition using LLM\n    - Iterative retrieval and answering\n    - Chain-of-thought reasoning\n    - Confidence scoring\n    - Source attribution\n    \"\"\"\n\n    def __init__(\n        self,\n        search_engine: Optional[HybridSearchEngine] = None,\n        llm_generator: Optional[Callable] = None,\n    ):\n        \"\"\"\n        Initialize multi-hop reasoner.\n\n        Args:\n            search_engine: Hybrid search engine instance\n            llm_generator: Async function for LLM text generation\n        \"\"\"\n        self._search_engine = search_engine or HybridSearchEngine()\n        self._llm_generator = llm_generator\n        self._openai_client = None\n\n    async def _get_llm_response(self, prompt: str, max_tokens: int = 500, temperature: float = 0.3) -> str:\n        \"\"\"\n        Generate response using LLM.\n\n        Args:\n            prompt: Input prompt\n            max_tokens: Maximum tokens in response\n            temperature: Sampling temperature\n\n        Returns:\n            Generated text\n        \"\"\"\n        # Use custom generator if provided\n        if self._llm_generator:\n            return await self._llm_generator(prompt, max_tokens, temperature)\n\n        # Fall back to OpenAI\n        try:\n            import httpx\n\n            async with httpx.AsyncClient(timeout=30.0) as client:\n                response = await client.post(\n                    \"https://api.openai.com/v1/chat/completions\",\n                    headers={\n                        \"Authorization\": f\"Bearer {settings.OPENAI_API_KEY}\",\n                        \"Content-Type\": \"application/json\",\n                    },\n                    json={\n                        \"model\": getattr(settings, \"OPENAI_MODEL\", \"gpt-4-turbo-preview\"),\n                        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n                        \"max_tokens\": max_tokens,\n                        \"temperature\": temperature,\n                    },\n                )\n                response.raise_for_status()\n                data = response.json()\n                return data[\"choices\"][0][\"message\"][\"content\"]\n\n        except Exception as e:\n            logger.error(f\"LLM generation failed: {e}\")\n            return \"\"\n\n    def _detect_strategy(self, query: str) -> ReasoningStrategy:\n        \"\"\"\n        Detect the appropriate reasoning strategy for a query.\n\n        Args:\n            query: User query\n\n        Returns:\n            ReasoningStrategy enum value\n        \"\"\"\n        query_lower = query.lower()\n\n        # Comparative queries\n        if any(word in query_lower for word in [\"compare\", \"difference\", \"versus\", \"vs\", \"better\", \"prefer\"]):\n            return ReasoningStrategy.COMPARATIVE\n\n        # Causal queries\n        if any(word in query_lower for word in [\"why\", \"cause\", \"reason\", \"leads to\", \"results in\", \"because\"]):\n            return ReasoningStrategy.CAUSAL\n\n        # Temporal queries\n        if any(\n            word in query_lower\n            for word in [\n                \"when\",\n                \"timeline\",\n                \"progression\",\n                \"stages\",\n                \"before\",\n                \"after\",\n                \"during\",\n            ]\n        ):\n            return ReasoningStrategy.TEMPORAL\n\n        # Complex multi-part queries\n        if len(query.split()) > 15 or \"and\" in query_lower or \",\" in query or query.count(\"?\") > 1:\n            return ReasoningStrategy.MULTI_HOP\n\n        # Simple direct queries\n        return ReasoningStrategy.DIRECT\n\n    async def reason(\n        self,\n        query: str,\n        max_hops: int = 3,\n        context: Optional[str] = None,\n        strategy: Optional[ReasoningStrategy] = None,\n    ) -> ReasoningResult:\n        \"\"\"\n        Perform multi-hop reasoning on a complex query.\n\n        Process:\n        1. Detect or use provided reasoning strategy\n        2. Decompose query into sub-questions\n        3. Answer each sub-question with retrieval\n        4. Synthesize final answer from reasoning chain\n\n        Args:\n            query: Complex query to answer\n            max_hops: Maximum reasoning steps\n            context: Optional initial context\n            strategy: Optional strategy override\n\n        Returns:\n            ReasoningResult with full reasoning chain\n        \"\"\"\n        # Detect strategy if not provided\n        if strategy is None:\n            strategy = self._detect_strategy(query)\n\n        logger.info(\n            \"Starting multi-hop reasoning\",\n            extra={\n                \"query\": query[:100],\n                \"strategy\": strategy.value,\n                \"max_hops\": max_hops,\n            },\n        )\n\n        # For direct queries, use single-step retrieval\n        if strategy == ReasoningStrategy.DIRECT:\n            return await self._direct_reasoning(query, context)\n\n        # Decompose query into sub-questions\n        sub_questions = await self._decompose_query(query, strategy)\n\n        reasoning_chain: List[ReasoningStep] = []\n        accumulated_context = context or \"\"\n        all_sources: List[str] = []\n\n        # Answer sub-questions iteratively\n        for i, sub_q in enumerate(sub_questions[:max_hops]):\n            step = await self._answer_sub_question(\n                question=sub_q,\n                step_number=i + 1,\n                context=accumulated_context,\n            )\n\n            reasoning_chain.append(step)\n            all_sources.extend(step.sources)\n\n            # Update context for next hop\n            accumulated_context += f\"\\n\\nQ: {sub_q}\\nA: {step.answer}\"\n\n        # Synthesize final answer\n        final_answer = await self._synthesize_answer(\n            original_query=query,\n            reasoning_chain=reasoning_chain,\n            strategy=strategy,\n        )\n\n        # Calculate overall confidence\n        confidence = self._calculate_confidence(reasoning_chain)\n\n        # Deduplicate sources\n        unique_sources = list(dict.fromkeys(all_sources))\n\n        return ReasoningResult(\n            original_query=query,\n            strategy=strategy,\n            reasoning_chain=reasoning_chain,\n            final_answer=final_answer,\n            confidence=confidence,\n            sources=unique_sources,\n            metadata={\n                \"num_hops\": len(reasoning_chain),\n                \"sub_questions\": [s.question for s in reasoning_chain],\n            },\n        )\n\n    async def _direct_reasoning(self, query: str, context: Optional[str]) -> ReasoningResult:\n        \"\"\"\n        Single-step direct reasoning for simple queries.\n\n        Args:\n            query: Simple query\n            context: Optional context\n\n        Returns:\n            ReasoningResult\n        \"\"\"\n        # Search for relevant documents\n        search_results = await self._search_engine.search(query, top_k=5)\n\n        # Generate answer\n        step = await self._answer_sub_question(\n            question=query,\n            step_number=1,\n            context=context or \"\",\n            search_results=search_results,\n        )\n\n        return ReasoningResult(\n            original_query=query,\n            strategy=ReasoningStrategy.DIRECT,\n            reasoning_chain=[step],\n            final_answer=step.answer,\n            confidence=step.confidence,\n            sources=step.sources,\n            metadata={\"num_hops\": 1},\n        )\n\n    async def _decompose_query(self, query: str, strategy: ReasoningStrategy) -> List[str]:\n        \"\"\"\n        Decompose complex query into simpler sub-questions.\n\n        Args:\n            query: Complex query\n            strategy: Reasoning strategy\n\n        Returns:\n            List of sub-questions\n        \"\"\"\n        multi_hop_prompt = (\n            \"Break down this complex medical question into 2-4 simpler \"\n            \"sub-questions that can be answered independently.\\n\\n\"\n            \"Question: {query}\\n\\n\"\n            \"Generate sub-questions that together would help answer \"\n            \"the main question comprehensively.\\n\"\n            \"Format: One question per line, no numbering or bullets.\"\n        )\n        comparative_prompt = (\n            \"Break down this comparative medical question into sub-questions \"\n            \"that address each entity separately.\\n\\n\"\n            \"Question: {query}\\n\\n\"\n            \"Generate:\\n\"\n            \"1. A question about the first entity/option\\n\"\n            \"2. A question about the second entity/option\\n\"\n            \"3. A question about their key differences\\n\\n\"\n            \"Format: One question per line, no numbering or bullets.\"\n        )\n        causal_prompt = (\n            \"Break down this causal medical question into sub-questions \"\n            \"that explore the mechanism.\\n\\n\"\n            \"Question: {query}\\n\\n\"\n            \"Generate:\\n\"\n            \"1. A question about the initial condition/trigger\\n\"\n            \"2. A question about the mechanism/pathway\\n\"\n            \"3. A question about the outcome/effect\\n\\n\"\n            \"Format: One question per line, no numbering or bullets.\"\n        )\n        temporal_prompt = (\n            \"Break down this temporal medical question into sub-questions \"\n            \"about different time phases.\\n\\n\"\n            \"Question: {query}\\n\\n\"\n            \"Generate:\\n\"\n            \"1. A question about the initial phase\\n\"\n            \"2. A question about the progression\\n\"\n            \"3. A question about the outcome/timeline\\n\\n\"\n            \"Format: One question per line, no numbering or bullets.\"\n        )\n        strategy_prompts = {\n            ReasoningStrategy.MULTI_HOP: multi_hop_prompt,\n            ReasoningStrategy.COMPARATIVE: comparative_prompt,\n            ReasoningStrategy.CAUSAL: causal_prompt,\n            ReasoningStrategy.TEMPORAL: temporal_prompt,\n        }\n\n        prompt = strategy_prompts.get(strategy, strategy_prompts[ReasoningStrategy.MULTI_HOP]).format(query=query)\n\n        response = await self._get_llm_response(prompt, max_tokens=300)\n\n        # Parse sub-questions\n        sub_questions = [\n            q.strip().lstrip(\"0123456789.-) \") for q in response.split(\"\\n\") if q.strip() and len(q.strip()) > 10\n        ]\n\n        # Ensure at least the original query\n        if not sub_questions:\n            sub_questions = [query]\n\n        logger.debug(f\"Decomposed into {len(sub_questions)} sub-questions\")\n        return sub_questions\n\n    async def _answer_sub_question(\n        self,\n        question: str,\n        step_number: int,\n        context: str,\n        search_results: Optional[List[SearchResult]] = None,\n    ) -> ReasoningStep:\n        \"\"\"\n        Answer a single sub-question with retrieval.\n\n        Args:\n            question: Sub-question to answer\n            step_number: Current step in reasoning chain\n            context: Accumulated context from previous steps\n            search_results: Optional pre-fetched search results\n\n        Returns:\n            ReasoningStep with answer and metadata\n        \"\"\"\n        # Search for relevant documents if not provided\n        if search_results is None:\n            search_results = await self._search_engine.search(question, top_k=5)\n\n        # Build document context\n        doc_context = \"\"\n        sources = []\n        doc_ids = []\n\n        for result in search_results:\n            source = result.metadata.get(\"source\", \"Unknown\")\n            doc_context += f\"\\nSource: {source}\\n{result.content[:500]}\\n\"\n            sources.append(source)\n            doc_ids.append(result.doc_id)\n\n        # Generate answer\n        prompt = f\"\"\"Based on the following context, answer the question concisely and accurately.\n\nPrevious Context:\n{context[:1000] if context else \"None\"}\n\nRetrieved Information:\n{doc_context if doc_context else \"No documents found.\"}\n\nQuestion: {question}\n\nAnswer (be concise, cite sources when possible):\"\"\"\n\n        answer = await self._get_llm_response(prompt, max_tokens=300)\n\n        # Calculate confidence based on search results\n        confidence = self._calculate_step_confidence(search_results, answer)\n\n        return ReasoningStep(\n            step_number=step_number,\n            question=question,\n            retrieved_docs=doc_ids,\n            answer=answer,\n            confidence=confidence,\n            sources=list(dict.fromkeys(sources)),  # Dedupe\n        )\n\n    async def _synthesize_answer(\n        self,\n        original_query: str,\n        reasoning_chain: List[ReasoningStep],\n        strategy: ReasoningStrategy,\n    ) -> str:\n        \"\"\"\n        Synthesize final answer from reasoning chain.\n\n        Args:\n            original_query: Original complex query\n            reasoning_chain: List of reasoning steps\n            strategy: Reasoning strategy used\n\n        Returns:\n            Synthesized comprehensive answer\n        \"\"\"\n        # Build chain summary\n        chain_text = \"\\n\".join(\n            [\n                f\"Step {step.step_number}: {step.question}\\n\"\n                f\"Answer: {step.answer}\\n\"\n                f\"Sources: {', '.join(step.sources[:3])}\"\n                for step in reasoning_chain\n            ]\n        )\n\n        synth_comparative = (\n            \"Based on the reasoning chain below, provide a clear comparison \"\n            \"answering the original question.\\n\\n\"\n            \"Original Question: {query}\\n\\n\"\n            \"Reasoning Chain:\\n{chain}\\n\\n\"\n            \"Provide a comprehensive comparison that:\\n\"\n            \"1. Summarizes key points about each option\\n\"\n            \"2. Highlights important differences\\n\"\n            \"3. Provides a clear conclusion\\n\\n\"\n            \"Synthesized Answer:\"\n        )\n        synth_causal = (\n            \"Based on the reasoning chain below, explain the causal \"\n            \"relationship answering the original question.\\n\\n\"\n            \"Original Question: {query}\\n\\n\"\n            \"Reasoning Chain:\\n{chain}\\n\\n\"\n            \"Provide a comprehensive explanation that:\\n\"\n            \"1. Describes the initial trigger/condition\\n\"\n            \"2. Explains the mechanism step by step\\n\"\n            \"3. Describes the final outcome\\n\\n\"\n            \"Synthesized Answer:\"\n        )\n        synth_temporal = (\n            \"Based on the reasoning chain below, provide a timeline-based \"\n            \"answer to the original question.\\n\\n\"\n            \"Original Question: {query}\\n\\n\"\n            \"Reasoning Chain:\\n{chain}\\n\\n\"\n            \"Provide a comprehensive timeline that:\\n\"\n            \"1. Describes the initial phase\\n\"\n            \"2. Explains the progression\\n\"\n            \"3. Describes the outcome and typical timeframes\\n\\n\"\n            \"Synthesized Answer:\"\n        )\n        synthesis_prompts = {\n            ReasoningStrategy.COMPARATIVE: synth_comparative,\n            ReasoningStrategy.CAUSAL: synth_causal,\n            ReasoningStrategy.TEMPORAL: synth_temporal,\n        }\n\n        default_prompt = (\n            \"Based on the reasoning chain below, provide a comprehensive \"\n            \"answer to the original question.\\n\\n\"\n            \"Original Question: {query}\\n\\n\"\n            \"Reasoning Chain:\\n{chain}\\n\\n\"\n            \"Synthesized Answer (comprehensive, well-structured, \"\n            \"with source attribution):\"\n        )\n\n        prompt = synthesis_prompts.get(strategy, default_prompt).format(query=original_query, chain=chain_text)\n\n        return await self._get_llm_response(prompt, max_tokens=500)\n\n    def _calculate_step_confidence(self, search_results: List[SearchResult], answer: str) -> float:\n        \"\"\"\n        Calculate confidence for a single reasoning step.\n\n        Args:\n            search_results: Retrieved documents\n            answer: Generated answer\n\n        Returns:\n            Confidence score 0-1\n        \"\"\"\n        if not search_results:\n            return 0.3  # Low confidence without sources\n\n        # Base confidence from search scores\n        if search_results:\n            avg_score = sum(r.score for r in search_results) / len(search_results)\n            base_confidence = min(0.9, avg_score)\n        else:\n            base_confidence = 0.3\n\n        # Adjust based on answer quality indicators\n        if len(answer) < 20:\n            base_confidence *= 0.5  # Very short answers\n        elif \"I don't know\" in answer.lower() or \"unclear\" in answer.lower():\n            base_confidence *= 0.6  # Uncertainty expressed\n\n        return round(base_confidence, 2)\n\n    def _calculate_confidence(self, reasoning_chain: List[ReasoningStep]) -> float:\n        \"\"\"\n        Calculate overall confidence for the reasoning.\n\n        Args:\n            reasoning_chain: List of reasoning steps\n\n        Returns:\n            Overall confidence score 0-1\n        \"\"\"\n        if not reasoning_chain:\n            return 0.0\n\n        # Average confidence across steps\n        avg_confidence = sum(s.confidence for s in reasoning_chain) / len(reasoning_chain)\n\n        # Bonus for complete chain\n        completeness_bonus = min(0.1, len(reasoning_chain) * 0.03)\n\n        return min(1.0, avg_confidence + completeness_bonus)\n\n    def to_dict(self, result: ReasoningResult) -> Dict[str, Any]:\n        \"\"\"\n        Convert ReasoningResult to dictionary for API response.\n\n        Args:\n            result: ReasoningResult to convert\n\n        Returns:\n            Dictionary representation\n        \"\"\"\n        return {\n            \"original_query\": result.original_query,\n            \"strategy\": result.strategy.value,\n            \"reasoning_chain\": [\n                {\n                    \"step\": step.step_number,\n                    \"question\": step.question,\n                    \"answer\": step.answer,\n                    \"confidence\": step.confidence,\n                    \"sources\": step.sources,\n                    \"doc_ids\": step.retrieved_docs,\n                }\n                for step in result.reasoning_chain\n            ],\n            \"final_answer\": result.final_answer,\n            \"confidence\": result.confidence,\n            \"sources\": result.sources,\n            \"metadata\": result.metadata,\n        }\n\n\n# Global service instances (lazy loaded)\nhybrid_search_engine = HybridSearchEngine(lazy_load=True)\nmulti_hop_reasoner = MultiHopReasoner(search_engine=hybrid_search_engine)\n"
}
