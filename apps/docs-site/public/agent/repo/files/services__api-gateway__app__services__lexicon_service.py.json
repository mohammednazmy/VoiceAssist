{
  "path": "services/api-gateway/app/services/lexicon_service.py",
  "language": "python",
  "size": 19135,
  "last_modified": "2025-12-05T03:07:13.134Z",
  "lines": 529,
  "content": "\"\"\"\nLexicon Service for Medical Pronunciation\nManages pronunciation lexicons with G2P fallback for medical terminology.\n\nPart of Voice Mode Enhancement Plan v4.1\nReference: /home/asimo/.claude/plans/noble-bubbling-trinket.md#lexicon--language-coverage\n\"\"\"\n\nimport json\nimport logging\nimport os\nfrom dataclasses import dataclass, field\nfrom pathlib import Path\nfrom typing import Dict, List, Optional, Set\n\nlogger = logging.getLogger(__name__)\n\n\ndef _resolve_data_dir() -> Path:\n    \"\"\"\n    Resolve the data directory path using multiple strategies.\n\n    Resolution order:\n    1. VOICEASSIST_DATA_DIR environment variable (absolute path)\n    2. Relative to repository root (walks up from this file)\n    3. Fallback to current working directory + data/\n\n    Returns:\n        Path to the data directory\n    \"\"\"\n    # 1. Check environment variable\n    env_data_dir = os.environ.get(\"VOICEASSIST_DATA_DIR\")\n    if env_data_dir:\n        data_path = Path(env_data_dir)\n        if data_path.exists():\n            logger.debug(f\"Using data dir from env: {data_path}\")\n            return data_path\n        else:\n            logger.warning(f\"VOICEASSIST_DATA_DIR={env_data_dir} does not exist\")\n\n    # 2. Walk up from this file to find repo root (contains data/ directory)\n    current = Path(__file__).resolve()\n    for _ in range(10):  # Max 10 levels up\n        current = current.parent\n        data_candidate = current / \"data\"\n        if data_candidate.exists() and (data_candidate / \"lexicons\").exists():\n            logger.debug(f\"Found data dir relative to repo: {data_candidate}\")\n            return data_candidate\n\n    # 3. Try relative to cwd\n    cwd_data = Path.cwd() / \"data\"\n    if cwd_data.exists():\n        logger.debug(f\"Using data dir from cwd: {cwd_data}\")\n        return cwd_data\n\n    # 4. Final fallback - may not exist, but let caller handle it\n    fallback = Path(__file__).resolve().parent.parent.parent.parent.parent / \"data\"\n    logger.warning(f\"Using fallback data dir: {fallback}\")\n    return fallback\n\n\n@dataclass\nclass PronunciationResult:\n    \"\"\"Result of a pronunciation lookup.\"\"\"\n\n    term: str\n    phoneme: str\n    language: str\n    source: str  # \"lexicon\", \"shared_drugs\", \"g2p\", \"g2p_fallback\"\n    confidence: float\n    alphabet: str = \"ipa\"  # IPA, SAMPA, etc.\n\n\n@dataclass\nclass LexiconReport:\n    \"\"\"Report on lexicon coverage for a language.\"\"\"\n\n    language: str\n    status: str  # \"complete\", \"partial\", \"placeholder\"\n    term_count: int\n    coverage_pct: float\n    missing_categories: List[str] = field(default_factory=list)\n    version: Optional[str] = None\n    last_updated: Optional[str] = None\n\n\nclass G2PService:\n    \"\"\"\n    Grapheme-to-Phoneme service for generating pronunciations.\n\n    Falls back to espeak-ng for languages without dedicated G2P models.\n    \"\"\"\n\n    # Language-specific G2P configurations\n    G2P_CONFIGS = {\n        \"en\": {\"engine\": \"espeak-ng\", \"voice\": \"en-us\"},\n        \"es\": {\"engine\": \"espeak-ng\", \"voice\": \"es\"},\n        \"fr\": {\"engine\": \"espeak-ng\", \"voice\": \"fr\"},\n        \"de\": {\"engine\": \"espeak-ng\", \"voice\": \"de\"},\n        \"it\": {\"engine\": \"espeak-ng\", \"voice\": \"it\"},\n        \"pt\": {\"engine\": \"espeak-ng\", \"voice\": \"pt\"},\n        \"ar\": {\"engine\": \"mishkal\", \"voice\": \"ar\"},\n        \"zh\": {\"engine\": \"pypinyin\", \"voice\": \"zh\"},\n        \"hi\": {\"engine\": \"espeak-ng\", \"voice\": \"hi\"},\n        \"ur\": {\"engine\": \"espeak-ng\", \"voice\": \"ur\"},\n    }\n\n    async def generate(self, term: str, language: str) -> str:\n        \"\"\"\n        Generate phoneme representation for a term.\n\n        Args:\n            term: The word/phrase to convert\n            language: ISO 639-1 language code\n\n        Returns:\n            IPA phoneme string\n\n        Raises:\n            G2PError: If generation fails\n        \"\"\"\n        config = self.G2P_CONFIGS.get(language, self.G2P_CONFIGS[\"en\"])\n\n        try:\n            if config[\"engine\"] == \"espeak-ng\":\n                return await self._generate_espeak(term, config[\"voice\"])\n            elif config[\"engine\"] == \"pypinyin\":\n                return await self._generate_pypinyin(term)\n            elif config[\"engine\"] == \"mishkal\":\n                return await self._generate_mishkal(term)\n            else:\n                return await self._generate_espeak(term, \"en-us\")\n        except Exception as e:\n            logger.error(f\"G2P generation failed for '{term}' ({language}): {e}\")\n            raise G2PError(f\"G2P generation failed: {e}\")\n\n    async def _generate_espeak(self, term: str, voice: str) -> str:\n        \"\"\"Generate phonemes using espeak-ng.\"\"\"\n        import asyncio\n\n        try:\n            proc = await asyncio.create_subprocess_exec(\n                \"espeak-ng\",\n                \"-v\",\n                voice,\n                \"-q\",  # Quiet (no audio)\n                \"--ipa\",  # Output IPA\n                term,\n                stdout=asyncio.subprocess.PIPE,\n                stderr=asyncio.subprocess.PIPE,\n            )\n            stdout, stderr = await asyncio.wait_for(proc.communicate(), timeout=5.0)\n\n            if proc.returncode == 0:\n                return stdout.decode().strip()\n            else:\n                logger.warning(f\"espeak-ng failed: {stderr.decode()}\")\n                # Return a simple approximation\n                return f\"/{term}/\"\n        except FileNotFoundError:\n            logger.warning(\"espeak-ng not installed, returning term as phoneme\")\n            return f\"/{term}/\"\n        except asyncio.TimeoutError:\n            logger.warning(\"espeak-ng timed out\")\n            return f\"/{term}/\"\n\n    async def _generate_pypinyin(self, term: str) -> str:\n        \"\"\"Generate pinyin for Chinese text.\"\"\"\n        try:\n            from pypinyin import Style, lazy_pinyin\n\n            pinyin = lazy_pinyin(term, style=Style.TONE)\n            return \" \".join(pinyin)\n        except ImportError:\n            logger.warning(\"pypinyin not installed\")\n            return term\n        except Exception as e:\n            logger.error(f\"pypinyin failed: {e}\")\n            return term\n\n    async def _generate_mishkal(self, term: str) -> str:\n        \"\"\"Generate diacritized Arabic text.\"\"\"\n        # Mishkal integration would go here\n        # For now, return the term as-is\n        return term\n\n\nclass G2PError(Exception):\n    \"\"\"Exception raised when G2P generation fails.\"\"\"\n\n    pass\n\n\nclass LexiconService:\n    \"\"\"\n    Pronunciation lexicon service with G2P fallback.\n\n    Manages per-language medical pronunciation lexicons and provides\n    phoneme lookups with multiple fallback strategies:\n    1. Language-specific lexicon\n    2. Shared drug names lexicon\n    3. G2P generation\n    4. English G2P fallback\n\n    Features:\n    - Lazy loading of lexicons\n    - Coverage validation\n    - User custom pronunciations\n    - Caching for performance\n    \"\"\"\n\n    # Paths to lexicon files (relative to data directory)\n    # Core supported languages\n    LEXICON_PATHS = {\n        \"en\": \"lexicons/en/medical_phonemes.json\",\n        \"es\": \"lexicons/es/medical_phonemes.json\",\n        \"fr\": \"lexicons/fr/medical_phonemes.json\",\n        \"de\": \"lexicons/de/medical_phonemes.json\",\n        \"it\": \"lexicons/it/medical_phonemes.json\",\n        \"pt\": \"lexicons/pt/medical_phonemes.json\",\n        \"ar\": \"lexicons/ar/medical_phonemes.json\",\n        \"zh\": \"lexicons/zh/medical_phonemes.json\",\n        \"hi\": \"lexicons/hi/medical_phonemes.json\",\n        \"ur\": \"lexicons/ur/medical_phonemes.json\",\n        # Placeholder languages (future expansion)\n        \"ja\": \"lexicons/ja/medical_phonemes.json\",\n        \"ko\": \"lexicons/ko/medical_phonemes.json\",\n        \"ru\": \"lexicons/ru/medical_phonemes.json\",\n        \"pl\": \"lexicons/pl/medical_phonemes.json\",\n        \"tr\": \"lexicons/tr/medical_phonemes.json\",\n    }\n\n    # Additional domain-specific lexicons\n    DOMAIN_LEXICON_PATHS = {\n        \"ar\": [\"lexicons/ar/quranic_phonemes.json\"],  # Quranic Arabic vocabulary\n        \"en\": [\"lexicons/en/quranic_phonemes.json\"],  # Transliterated Quranic terms\n    }\n\n    SHARED_LEXICON_PATH = \"lexicons/shared/drug_names.json\"\n\n    # Languages with placeholder lexicons (minimal coverage, require expansion)\n    PLACEHOLDER_LANGUAGES = [\"ja\", \"ko\", \"ru\", \"pl\", \"tr\"]\n\n    # Required term categories for complete coverage\n    REQUIRED_CATEGORIES = [\n        \"drug_names\",\n        \"conditions\",\n        \"procedures\",\n        \"anatomy\",\n        \"lab_tests\",\n        \"specialties\",\n    ]\n\n    def __init__(self, data_dir: Optional[Path] = None, g2p_service: Optional[G2PService] = None):\n        self.data_dir = data_dir or _resolve_data_dir()\n        self.g2p_service = g2p_service or G2PService()\n        logger.info(f\"LexiconService initialized with data_dir: {self.data_dir}\")\n\n        # Lazy-loaded lexicons\n        self.lexicons: Dict[str, Dict[str, str]] = {}\n        self.shared_drug_lexicon: Dict[str, str] = {}\n        self.user_custom_lexicon: Dict[str, Dict[str, str]] = {}\n\n        # Cache for G2P results\n        self._g2p_cache: Dict[str, str] = {}\n\n        self._loaded_languages: Set[str] = set()\n        self._shared_loaded = False\n\n    def _ensure_lexicon_loaded(self, language: str) -> None:\n        \"\"\"Load lexicon for a language if not already loaded.\"\"\"\n        if language in self._loaded_languages:\n            return\n\n        self.lexicons[language] = {}\n\n        # Load main lexicon\n        lexicon_path = self.data_dir / self.LEXICON_PATHS.get(language, \"\")\n        if lexicon_path.exists():\n            try:\n                with open(lexicon_path, \"r\", encoding=\"utf-8\") as f:\n                    data = json.load(f)\n                    # Remove metadata key if present\n                    self.lexicons[language] = {k: v for k, v in data.items() if not k.startswith(\"_\")}\n                logger.info(f\"Loaded {len(self.lexicons[language])} terms for {language}\")\n            except Exception as e:\n                logger.error(f\"Failed to load lexicon for {language}: {e}\")\n        else:\n            logger.warning(f\"Lexicon file not found: {lexicon_path}\")\n\n        # Load domain-specific lexicons (e.g., Quranic vocabulary for Arabic)\n        domain_paths = self.DOMAIN_LEXICON_PATHS.get(language, [])\n        for domain_path_str in domain_paths:\n            domain_path = self.data_dir / domain_path_str\n            if domain_path.exists():\n                try:\n                    with open(domain_path, \"r\", encoding=\"utf-8\") as f:\n                        data = json.load(f)\n                        domain_terms = {k: v for k, v in data.items() if not k.startswith(\"_\")}\n                        self.lexicons[language].update(domain_terms)\n                    logger.info(f\"Loaded {len(domain_terms)} domain terms from {domain_path_str}\")\n                except Exception as e:\n                    logger.error(f\"Failed to load domain lexicon {domain_path_str}: {e}\")\n\n        self._loaded_languages.add(language)\n\n    def _ensure_shared_loaded(self) -> None:\n        \"\"\"Load shared drug names lexicon if not already loaded.\"\"\"\n        if self._shared_loaded:\n            return\n\n        shared_path = self.data_dir / self.SHARED_LEXICON_PATH\n        if shared_path.exists():\n            try:\n                with open(shared_path, \"r\", encoding=\"utf-8\") as f:\n                    data = json.load(f)\n                    self.shared_drug_lexicon = {k: v for k, v in data.items() if not k.startswith(\"_\")}\n                logger.info(f\"Loaded {len(self.shared_drug_lexicon)} shared drug pronunciations\")\n            except Exception as e:\n                logger.error(f\"Failed to load shared lexicon: {e}\")\n\n        self._shared_loaded = True\n\n    async def get_phoneme(self, term: str, language: str) -> PronunciationResult:\n        \"\"\"\n        Get phoneme representation for a term.\n\n        Lookup order:\n        1. User custom pronunciations\n        2. Language-specific lexicon\n        3. Shared drug names lexicon\n        4. G2P generation\n        5. English G2P fallback\n\n        Args:\n            term: The word/phrase to look up\n            language: ISO 639-1 language code\n\n        Returns:\n            PronunciationResult with phoneme and metadata\n        \"\"\"\n        term_lower = term.lower().strip()\n\n        # 1. Check user custom pronunciations\n        if language in self.user_custom_lexicon:\n            if phoneme := self.user_custom_lexicon[language].get(term_lower):\n                return PronunciationResult(\n                    term=term, phoneme=phoneme, language=language, source=\"user_custom\", confidence=1.0\n                )\n\n        # 2. Check language-specific lexicon\n        self._ensure_lexicon_loaded(language)\n        if language in self.lexicons and term_lower in self.lexicons[language]:\n            return PronunciationResult(\n                term=term,\n                phoneme=self.lexicons[language][term_lower],\n                language=language,\n                source=\"lexicon\",\n                confidence=1.0,\n            )\n\n        # 3. Check shared drug names lexicon\n        self._ensure_shared_loaded()\n        if term_lower in self.shared_drug_lexicon:\n            return PronunciationResult(\n                term=term,\n                phoneme=self.shared_drug_lexicon[term_lower],\n                language=language,\n                source=\"shared_drugs\",\n                confidence=0.95,\n            )\n\n        # 4. G2P generation with caching\n        cache_key = f\"{language}:{term_lower}\"\n        if cache_key in self._g2p_cache:\n            return PronunciationResult(\n                term=term, phoneme=self._g2p_cache[cache_key], language=language, source=\"g2p_cached\", confidence=0.7\n            )\n\n        try:\n            phoneme = await self.g2p_service.generate(term, language)\n            self._g2p_cache[cache_key] = phoneme\n            return PronunciationResult(term=term, phoneme=phoneme, language=language, source=\"g2p\", confidence=0.7)\n        except G2PError:\n            # 5. Fall back to English G2P\n            try:\n                phoneme = await self.g2p_service.generate(term, \"en\")\n                return PronunciationResult(\n                    term=term, phoneme=phoneme, language=language, source=\"g2p_fallback\", confidence=0.5\n                )\n            except G2PError:\n                # Last resort: return term wrapped as phoneme\n                return PronunciationResult(\n                    term=term, phoneme=f\"/{term}/\", language=language, source=\"unknown\", confidence=0.1\n                )\n\n    async def get_phonemes_batch(self, terms: List[str], language: str) -> List[PronunciationResult]:\n        \"\"\"\n        Get phonemes for multiple terms.\n\n        Args:\n            terms: List of terms to look up\n            language: ISO 639-1 language code\n\n        Returns:\n            List of PronunciationResults in same order as input\n        \"\"\"\n        import asyncio\n\n        tasks = [self.get_phoneme(term, language) for term in terms]\n        return await asyncio.gather(*tasks)\n\n    def add_user_pronunciation(self, term: str, phoneme: str, language: str) -> None:\n        \"\"\"\n        Add a user-defined custom pronunciation.\n\n        Args:\n            term: The word/phrase\n            phoneme: IPA pronunciation\n            language: ISO 639-1 language code\n        \"\"\"\n        if language not in self.user_custom_lexicon:\n            self.user_custom_lexicon[language] = {}\n        self.user_custom_lexicon[language][term.lower()] = phoneme\n        logger.info(f\"Added user pronunciation: {term} -> {phoneme} ({language})\")\n\n    async def validate_lexicon_coverage(self, language: str) -> LexiconReport:\n        \"\"\"\n        Validate lexicon coverage for a language.\n\n        Checks that all required categories have adequate coverage.\n\n        Args:\n            language: ISO 639-1 language code\n\n        Returns:\n            LexiconReport with coverage statistics\n        \"\"\"\n        if language in self.PLACEHOLDER_LANGUAGES:\n            return LexiconReport(\n                language=language,\n                status=\"placeholder\",\n                term_count=0,\n                coverage_pct=0.0,\n                missing_categories=self.REQUIRED_CATEGORIES,\n            )\n\n        self._ensure_lexicon_loaded(language)\n        lexicon = self.lexicons.get(language, {})\n\n        # Check lexicon metadata\n        lexicon_path = self.data_dir / self.LEXICON_PATHS.get(language, \"\")\n        version = None\n        last_updated = None\n\n        if lexicon_path.exists():\n            try:\n                with open(lexicon_path, \"r\", encoding=\"utf-8\") as f:\n                    data = json.load(f)\n                    if \"_meta\" in data:\n                        version = data[\"_meta\"].get(\"version\")\n                        last_updated = data[\"_meta\"].get(\"last_updated\")\n            except Exception:\n                pass\n\n        # Calculate coverage (simplified - would need category annotations in real impl)\n        term_count = len(lexicon)\n        coverage_pct = min(100.0, term_count / 25.0)  # Assume 2500 terms = 100%\n\n        # Determine status\n        if term_count >= 2000:\n            status = \"complete\"\n            missing = []\n        elif term_count >= 500:\n            status = \"partial\"\n            missing = [\"conditions\", \"procedures\"]  # Placeholder\n        else:\n            status = \"minimal\"\n            missing = self.REQUIRED_CATEGORIES\n\n        return LexiconReport(\n            language=language,\n            status=status,\n            term_count=term_count,\n            coverage_pct=coverage_pct,\n            missing_categories=missing,\n            version=version,\n            last_updated=last_updated,\n        )\n\n    async def validate_all_lexicons(self) -> Dict[str, LexiconReport]:\n        \"\"\"\n        Validate coverage for all configured languages.\n\n        Returns:\n            Dict mapping language codes to LexiconReports\n        \"\"\"\n        reports = {}\n        for language in self.LEXICON_PATHS.keys():\n            reports[language] = await self.validate_lexicon_coverage(language)\n        return reports\n\n    def get_supported_languages(self) -> List[Dict[str, str]]:\n        \"\"\"Get list of languages with lexicon support.\"\"\"\n        return [\n            {\"code\": \"en\", \"name\": \"English\", \"status\": \"complete\"},\n            {\"code\": \"es\", \"name\": \"Spanish\", \"status\": \"complete\"},\n            {\"code\": \"fr\", \"name\": \"French\", \"status\": \"complete\"},\n            {\"code\": \"de\", \"name\": \"German\", \"status\": \"complete\"},\n            {\"code\": \"it\", \"name\": \"Italian\", \"status\": \"complete\"},\n            {\"code\": \"pt\", \"name\": \"Portuguese\", \"status\": \"complete\"},\n            {\"code\": \"ar\", \"name\": \"Arabic\", \"status\": \"in_progress\"},\n            {\"code\": \"zh\", \"name\": \"Chinese\", \"status\": \"in_progress\"},\n            {\"code\": \"hi\", \"name\": \"Hindi\", \"status\": \"in_progress\"},\n            {\"code\": \"ur\", \"name\": \"Urdu\", \"status\": \"in_progress\"},\n        ]\n\n\n# Singleton instance\n_lexicon_service: Optional[LexiconService] = None\n\n\ndef get_lexicon_service() -> LexiconService:\n    \"\"\"Get or create lexicon service instance.\"\"\"\n    global _lexicon_service\n    if _lexicon_service is None:\n        _lexicon_service = LexiconService()\n    return _lexicon_service\n"
}
