{
  "path": "services/api-gateway/app/services/voice_pipeline_service.py",
  "language": "python",
  "size": 73787,
  "last_modified": "2025-12-05T03:07:13.136Z",
  "lines": 1900,
  "content": "\"\"\"\nVoice Pipeline Service - Thinker/Talker Orchestrator\n\nOrchestrates the complete voice interaction flow:\n    Audio Input → STT → Thinker (LLM) → Talker (TTS) → Audio Output\n\nFeatures:\n- Streaming at every stage for low latency\n- Unified conversation context across voice and chat\n- Barge-in support (interrupt AI response)\n- State machine for session management\n- Metrics collection\n\nPhase: Thinker/Talker Voice Pipeline Migration\n\"\"\"\n\nimport asyncio\nimport base64\nimport random\nimport time\nimport uuid\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nfrom typing import Any, Awaitable, Callable, Dict, List, Optional\n\nfrom app.core.logging import get_logger\nfrom app.services.backchannel_service import (\n    BackchannelAudio,\n    BackchannelService,\n    BackchannelSession,\n    backchannel_service,\n)\nfrom app.services.dictation_phi_monitor import DictationPHIMonitor, PatientPHIContext, dictation_phi_monitor\nfrom app.services.dictation_service import (\n    DictationEvent,\n    DictationSession,\n    DictationSessionConfig,\n    NoteType,\n    dictation_service,\n)\nfrom app.services.emotion_detection_service import (\n    EmotionDetectionService,\n    EmotionDetectionSession,\n    EmotionResult,\n    emotion_detection_service,\n)\nfrom app.services.feedback_service import FeedbackService, feedback_service\nfrom app.services.medical_vocabulary_service import MedicalSpecialty\nfrom app.services.memory_context_service import ConversationMemoryManager, MemoryType, memory_context_service\nfrom app.services.note_formatter_service import FormattingConfig, FormattingLevel, note_formatter_service\nfrom app.services.patient_context_service import DictationContext, PatientContextService, patient_context_service\nfrom app.services.prosody_analysis_service import ProsodySession, ProsodySnapshot, prosody_service\nfrom app.services.session_analytics_service import (\n    InteractionType,\n    SessionAnalytics,\n    SessionAnalyticsService,\n    session_analytics_service,\n)\nfrom app.services.streaming_stt_service import (\n    DeepgramStreamingSession,\n    StreamingSTTService,\n    STTSessionConfig,\n    streaming_stt_service,\n)\nfrom app.services.talker_service import AudioChunk, TalkerService, TalkerSession, VoiceConfig, talker_service\nfrom app.services.thinker_service import ThinkerService, ThinkerSession, ToolCallEvent, ToolResultEvent, thinker_service\nfrom app.services.voice_command_service import voice_command_service\n\nlogger = get_logger(__name__)\n\n\n# ==============================================================================\n# Data Classes and Enums\n# ==============================================================================\n\n\nclass PipelineState(str, Enum):\n    \"\"\"State of the voice pipeline.\"\"\"\n\n    IDLE = \"idle\"\n    LISTENING = \"listening\"\n    PROCESSING = \"processing\"\n    SPEAKING = \"speaking\"\n    CANCELLED = \"cancelled\"\n    ERROR = \"error\"\n\n\nclass PipelineMode(str, Enum):\n    \"\"\"\n    Phase 8: Mode of the voice pipeline.\n\n    - CONVERSATION: Normal conversational voice mode with Thinker/Talker\n    - DICTATION: Medical dictation mode with transcription + formatting\n    \"\"\"\n\n    CONVERSATION = \"conversation\"\n    DICTATION = \"dictation\"\n\n\nclass QueryType(str, Enum):\n    \"\"\"\n    Phase 6: Classification of user query types for response timing.\n\n    Different query types warrant different response delays to feel natural:\n    - URGENT: Medical emergencies, immediate answers needed\n    - SIMPLE: Yes/no questions, confirmations, short factual answers\n    - COMPLEX: Multi-part questions, explanations, comparisons\n    - CLARIFICATION: Requests for clarification after misunderstanding\n    \"\"\"\n\n    URGENT = \"urgent\"\n    SIMPLE = \"simple\"\n    COMPLEX = \"complex\"\n    CLARIFICATION = \"clarification\"\n    UNKNOWN = \"unknown\"\n\n\n@dataclass\nclass ResponseTimingConfig:\n    \"\"\"\n    Phase 6: Configuration for human-like response timing.\n\n    Different query types warrant different response patterns:\n    - delay_ms: How long to wait before starting to respond\n    - use_filler: Whether to use a thinking filler (\"Hmm, let me think...\")\n    - filler_phrases: Available filler phrases for this query type\n    \"\"\"\n\n    delay_ms: int\n    use_filler: bool\n    filler_phrases: list = field(default_factory=list)\n\n\n# Phase 6: Response timing configuration by query type\nRESPONSE_TIMING: Dict[QueryType, ResponseTimingConfig] = {\n    QueryType.URGENT: ResponseTimingConfig(\n        delay_ms=0,\n        use_filler=False,\n        filler_phrases=[],\n    ),\n    QueryType.SIMPLE: ResponseTimingConfig(\n        delay_ms=200,\n        use_filler=False,\n        filler_phrases=[],\n    ),\n    QueryType.COMPLEX: ResponseTimingConfig(\n        delay_ms=600,\n        use_filler=True,\n        filler_phrases=[\n            \"Hmm, let me think about that...\",\n            \"That's a good question...\",\n            \"Let me consider this...\",\n        ],\n    ),\n    QueryType.CLARIFICATION: ResponseTimingConfig(\n        delay_ms=0,\n        use_filler=False,\n        filler_phrases=[],\n    ),\n    QueryType.UNKNOWN: ResponseTimingConfig(\n        delay_ms=300,\n        use_filler=False,\n        filler_phrases=[],\n    ),\n}\n\n\ndef classify_query_type(transcript: str) -> QueryType:\n    \"\"\"\n    Phase 6: Classify user query to determine appropriate response timing.\n\n    Uses keyword matching and pattern detection to categorize queries:\n    - URGENT: Emergency keywords, medical distress\n    - SIMPLE: Yes/no questions, confirmations, short queries\n    - COMPLEX: Multiple questions, comparisons, explanations\n    - CLARIFICATION: \"what did you mean\", \"I said\", etc.\n\n    Args:\n        transcript: User's spoken text\n\n    Returns:\n        QueryType classification\n    \"\"\"\n    text = transcript.lower().strip()\n    words = text.split()\n    word_count = len(words)\n\n    # Urgent keywords (medical emergencies, distress)\n    urgent_keywords = {\n        \"emergency\",\n        \"help\",\n        \"urgent\",\n        \"911\",\n        \"ambulance\",\n        \"bleeding\",\n        \"chest pain\",\n        \"can't breathe\",\n        \"heart attack\",\n        \"stroke\",\n        \"overdose\",\n        \"unconscious\",\n        \"dying\",\n        \"severe pain\",\n    }\n    if any(keyword in text for keyword in urgent_keywords):\n        return QueryType.URGENT\n\n    # Clarification patterns (user correcting or asking for clarification)\n    clarification_patterns = [\n        \"what did you mean\",\n        \"i said\",\n        \"no i meant\",\n        \"that's not what\",\n        \"i was asking\",\n        \"let me clarify\",\n        \"to clarify\",\n        \"what i meant\",\n        \"i didn't say\",\n        \"i actually said\",\n        \"sorry i meant\",\n    ]\n    if any(pattern in text for pattern in clarification_patterns):\n        return QueryType.CLARIFICATION\n\n    # Simple queries (yes/no, confirmations, short questions)\n    simple_starters = [\n        \"is \",\n        \"are \",\n        \"do \",\n        \"does \",\n        \"did \",\n        \"can \",\n        \"will \",\n        \"should \",\n    ]\n    simple_endings = [\n        \"yes\",\n        \"no\",\n        \"ok\",\n        \"okay\",\n        \"sure\",\n        \"thanks\",\n        \"thank you\",\n        \"got it\",\n    ]\n    if word_count <= 5 and (\n        any(text.startswith(s) for s in simple_starters)\n        or text in simple_endings\n        or text.endswith(\"?\")\n        and word_count <= 8\n    ):\n        return QueryType.SIMPLE\n\n    # Complex queries (multiple questions, comparisons, explanations)\n    complex_indicators = [\n        \"explain\",\n        \"describe\",\n        \"compare\",\n        \"difference between\",\n        \"how does\",\n        \"why does\",\n        \"what are the\",\n        \"tell me about\",\n        \"can you walk me through\",\n        \"i need to understand\",\n        \"what's the relationship\",\n        \"pros and cons\",\n    ]\n    if any(indicator in text for indicator in complex_indicators):\n        return QueryType.COMPLEX\n\n    # Multiple questions or very long queries\n    question_count = text.count(\"?\")\n    if question_count >= 2 or word_count >= 25:\n        return QueryType.COMPLEX\n\n    # Longer explanatory questions\n    if word_count >= 15 and (\"how\" in text or \"why\" in text or \"what\" in text):\n        return QueryType.COMPLEX\n\n    return QueryType.UNKNOWN\n\n\n@dataclass\nclass PipelineConfig:\n    \"\"\"Configuration for the voice pipeline.\"\"\"\n\n    # STT settings\n    stt_language: str = \"en\"\n    stt_sample_rate: int = 16000\n    # Endpointing: 800ms allows natural speech pauses (was 200ms - too aggressive)\n    stt_endpointing_ms: int = 800\n    # Utterance end: 1500ms wait after speech stops before finalizing\n    stt_utterance_end_ms: int = 1500\n\n    # LLM settings\n    max_response_tokens: int = 1024\n    temperature: float = 0.7\n\n    # TTS settings\n    voice_id: str = \"TxGEqnHWrfWFTfGW9XjX\"  # Josh (premium male voice)\n    tts_model: str = \"eleven_flash_v2_5\"  # Better quality + low latency\n    tts_output_format: str = \"pcm_24000\"  # Raw PCM for low-latency streaming\n\n    # Voice quality parameters\n    stability: float = 0.65\n    similarity_boost: float = 0.80\n    style: float = 0.15\n\n    # Barge-in settings\n    barge_in_enabled: bool = True\n    # VAD sensitivity: 0-100 scale (higher = more sensitive, triggers on quieter speech)\n    # Used as confidence threshold for barge-in: lower values require higher confidence\n    # 0 = max threshold (0.95 confidence required), 100 = min threshold (0.5 confidence required)\n    vad_sensitivity: int = 50\n\n    # Phase 8: Pipeline mode and dictation settings\n    mode: PipelineMode = PipelineMode.CONVERSATION\n    dictation_note_type: NoteType = NoteType.SOAP\n    dictation_specialty: Optional[MedicalSpecialty] = None\n    dictation_auto_format: bool = True\n    dictation_enable_commands: bool = True\n\n    # Phase 9: Patient context integration\n    patient_id: Optional[str] = None  # Patient ID for context-aware dictation\n    enable_phi_monitoring: bool = True  # Enable real-time PHI detection\n    enable_patient_context: bool = True  # Enable patient context retrieval\n\n\n@dataclass\nclass PipelineMessage:\n    \"\"\"A message to send to the client via WebSocket.\"\"\"\n\n    type: str\n    data: Dict[str, Any] = field(default_factory=dict)\n\n\n@dataclass\nclass PipelineMetrics:\n    \"\"\"Metrics for a pipeline session.\"\"\"\n\n    session_id: str = \"\"\n    start_time: float = 0.0\n    end_time: float = 0.0\n\n    # Latency breakdown\n    stt_latency_ms: int = 0\n    first_token_latency_ms: int = 0\n    tts_latency_ms: int = 0\n    total_latency_ms: int = 0\n\n    # Counts\n    audio_chunks_received: int = 0\n    audio_chunks_sent: int = 0\n    tokens_generated: int = 0\n    tool_calls_count: int = 0\n\n    # State\n    cancelled: bool = False\n    error: Optional[str] = None\n\n\n# ==============================================================================\n# Voice Pipeline Session\n# ==============================================================================\n\n\nclass VoicePipelineSession:\n    \"\"\"\n    A single voice interaction session.\n\n    Manages the complete flow:\n    1. Receive audio chunks from client\n    2. Stream to STT for transcription\n    3. On speech end, send to Thinker for response\n    4. Stream Thinker tokens to Talker for TTS\n    5. Stream TTS audio back to client\n    \"\"\"\n\n    def __init__(\n        self,\n        session_id: str,\n        conversation_id: str,\n        config: PipelineConfig,\n        stt_service: StreamingSTTService,\n        thinker_service: ThinkerService,\n        talker_service: TalkerService,\n        on_message: Callable[[PipelineMessage], Awaitable[None]],\n        user_id: Optional[str] = None,\n        emotion_service: Optional[EmotionDetectionService] = None,\n        backchannel_svc: Optional[BackchannelService] = None,\n    ):\n        self.session_id = session_id\n        self.conversation_id = conversation_id\n        self.config = config\n        self.user_id = user_id  # User ID for tool authentication\n        self._stt_service = stt_service\n        self._thinker_service = thinker_service\n        self._talker_service = talker_service\n        self._on_message = on_message\n        self._emotion_service = emotion_service or emotion_detection_service\n        self._backchannel_service = backchannel_svc or backchannel_service\n        self._prosody_service = prosody_service\n\n        # Session components\n        self._stt_session: Optional[DeepgramStreamingSession] = None\n        self._thinker_session: Optional[ThinkerSession] = None\n        self._talker_session: Optional[TalkerSession] = None\n        self._emotion_session: Optional[EmotionDetectionSession] = None\n        self._backchannel_session: Optional[BackchannelSession] = None\n        self._prosody_session: Optional[ProsodySession] = None\n\n        # Phase 4: Memory context manager for conversation continuity\n        self._memory_manager: Optional[ConversationMemoryManager] = None\n\n        # Phase 8: Dictation session for medical documentation\n        self._dictation_session: Optional[DictationSession] = None\n\n        # Phase 9: Patient context and PHI monitoring\n        self._patient_context: Optional[DictationContext] = None\n        self._phi_monitor: DictationPHIMonitor = dictation_phi_monitor\n        self._patient_context_service: PatientContextService = patient_context_service\n\n        # Phase 10: Analytics and feedback\n        self._analytics: Optional[SessionAnalytics] = None\n        self._analytics_service: SessionAnalyticsService = session_analytics_service\n        self._feedback_service: FeedbackService = feedback_service\n\n        # State\n        self._state = PipelineState.IDLE\n        self._cancelled = False\n\n        # Transcript accumulation\n        self._partial_transcript = \"\"\n        self._final_transcript = \"\"\n        self._transcript_confidence = 1.0  # Phase 7: Track STT confidence for repair strategies\n\n        # Current emotion state (for response adaptation)\n        self._current_emotion: Optional[EmotionResult] = None\n\n        # Current prosody analysis (for response timing)\n        self._current_prosody: Optional[ProsodySnapshot] = None\n\n        # Speech timing for backchannels\n        self._speech_start_time: Optional[float] = None\n        self._last_audio_time: float = 0.0\n        self._last_transcript_time: float = 0.0  # For pause detection\n\n        # Metrics\n        self._metrics = PipelineMetrics(session_id=session_id)\n\n        # Locks for thread safety\n        self._state_lock = asyncio.Lock()\n\n    @property\n    def state(self) -> PipelineState:\n        \"\"\"Get current pipeline state.\"\"\"\n        return self._state\n\n    def is_cancelled(self) -> bool:\n        \"\"\"Check if session was cancelled.\"\"\"\n        return self._cancelled\n\n    async def start(self) -> bool:\n        \"\"\"\n        Start the voice pipeline session.\n\n        Returns:\n            True if started successfully\n        \"\"\"\n        async with self._state_lock:\n            if self._state != PipelineState.IDLE:\n                logger.warning(f\"Cannot start pipeline in state {self._state}\")\n                return False\n\n            self._metrics.start_time = time.time()\n\n            try:\n                # Create STT session with lenient endpointing for natural speech\n                self._stt_session = await self._stt_service.create_session(\n                    on_partial=self._handle_partial_transcript,\n                    on_final=self._handle_final_transcript,\n                    on_endpoint=self._handle_speech_end,\n                    on_speech_start=self._handle_speech_start,\n                    on_words=self._handle_word_data,\n                    config=STTSessionConfig(\n                        language=self.config.stt_language,\n                        sample_rate=self.config.stt_sample_rate,\n                        endpointing_ms=self.config.stt_endpointing_ms,\n                        utterance_end_ms=self.config.stt_utterance_end_ms,\n                    ),\n                )\n\n                # Start STT\n                if not await self._stt_session.start():\n                    raise RuntimeError(\"Failed to start STT session\")\n\n                # Create emotion detection session (parallel to STT)\n                if self._emotion_service and self._emotion_service.is_enabled():\n                    self._emotion_session = await self._emotion_service.create_session(\n                        session_id=self.session_id,\n                        on_emotion=self._handle_emotion_result,\n                    )\n                    logger.info(f\"Emotion detection enabled for session: {self.session_id}\")\n\n                # Create backchannel session for natural verbal cues\n                if self._backchannel_service and self._backchannel_service.is_enabled():\n                    self._backchannel_session = await self._backchannel_service.create_session(\n                        session_id=self.session_id,\n                        voice_id=self.config.voice_id,\n                        language=self.config.stt_language,\n                        on_backchannel=self._handle_backchannel,\n                    )\n                    logger.info(f\"Backchanneling enabled for session: {self.session_id}\")\n\n                # Create prosody analysis session for speech pattern tracking\n                if self._prosody_service:\n                    self._prosody_session = await self._prosody_service.create_session(\n                        session_id=self.session_id,\n                        user_id=self.user_id,\n                    )\n                    logger.info(f\"Prosody analysis enabled for session: {self.session_id}\")\n\n                # Phase 4: Initialize memory context manager\n                if self.user_id:\n                    try:\n                        session_uuid = (\n                            uuid.UUID(self.session_id) if isinstance(self.session_id, str) else self.session_id\n                        )\n                        user_uuid = uuid.UUID(self.user_id) if isinstance(self.user_id, str) else self.user_id\n                        self._memory_manager = memory_context_service.get_conversation_memory(\n                            session_id=session_uuid,\n                            user_id=user_uuid,\n                        )\n                        logger.info(f\"Memory context enabled for session: {self.session_id}\")\n                    except Exception as e:\n                        logger.warning(f\"Failed to initialize memory context: {e}\")\n\n                # Phase 8: Initialize dictation session if in dictation mode\n                if self.config.mode == PipelineMode.DICTATION:\n                    dictation_config = DictationSessionConfig(\n                        note_type=self.config.dictation_note_type,\n                        language=self.config.stt_language,\n                        specialty=(self.config.dictation_specialty.value if self.config.dictation_specialty else None),\n                        auto_punctuate=True,\n                        auto_format=self.config.dictation_auto_format,\n                        enable_commands=self.config.dictation_enable_commands,\n                    )\n                    self._dictation_session = await dictation_service.create_session(\n                        user_id=self.user_id or \"anonymous\",\n                        config=dictation_config,\n                        on_event=self._handle_dictation_event,\n                    )\n                    await self._dictation_session.start()\n                    logger.info(\n                        f\"Dictation mode enabled for session: {self.session_id}, \"\n                        f\"type={self.config.dictation_note_type.value}\"\n                    )\n\n                    # Phase 9: Load patient context if patient_id is provided\n                    if self.config.patient_id and self.config.enable_patient_context:\n                        try:\n                            self._patient_context = await self._patient_context_service.get_context_for_dictation(\n                                user_id=self.user_id or \"anonymous\",\n                                patient_id=self.config.patient_id,\n                            )\n                            logger.info(\n                                f\"Patient context loaded for {self.config.patient_id}: \"\n                                f\"{len(self._patient_context.medications)} meds, \"\n                                f\"{len(self._patient_context.allergies)} allergies, \"\n                                f\"{len(self._patient_context.conditions)} conditions\"\n                            )\n\n                            # Set up PHI monitor with patient context\n                            if self.config.enable_phi_monitoring:\n                                phi_context = PatientPHIContext(\n                                    patient_id=self.config.patient_id,\n                                    known_mrn=self._patient_context.demographics.mrn,\n                                )\n                                # Add known names if available\n                                if self._patient_context.demographics.name:\n                                    phi_context.known_names.add(self._patient_context.demographics.name)\n                                self._phi_monitor.set_patient_context(phi_context)\n                                logger.info(f\"PHI monitor configured for patient {self.config.patient_id}\")\n\n                            # Generate and send context prompts to frontend\n                            prompts = self._patient_context_service.generate_context_prompts(self._patient_context)\n                            if prompts:\n                                await self._on_message(\n                                    PipelineMessage(\n                                        type=\"patient.context_loaded\",\n                                        data={\n                                            \"patient_id\": self.config.patient_id,\n                                            \"prompts\": [\n                                                {\n                                                    \"type\": p.prompt_type,\n                                                    \"category\": p.category.value,\n                                                    \"message\": p.message,\n                                                    \"priority\": p.priority,\n                                                }\n                                                for p in prompts\n                                            ],\n                                            \"summaries\": {\n                                                \"medications\": self._patient_context.medication_summary,\n                                                \"allergies\": self._patient_context.allergy_summary,\n                                                \"conditions\": self._patient_context.condition_summary,\n                                            },\n                                        },\n                                    )\n                                )\n                        except Exception as e:\n                            logger.warning(f\"Failed to load patient context: {e}\")\n\n                # Create Thinker session with user_id for tool authentication\n                self._thinker_session = self._thinker_service.create_session(\n                    conversation_id=self.conversation_id,\n                    on_token=self._handle_llm_token,\n                    on_tool_call=self._handle_tool_call,\n                    on_tool_result=self._handle_tool_result,\n                    user_id=self.user_id,\n                )\n\n                # Phase 10: Initialize analytics session\n                pipeline_mode = \"dictation\" if self.config.mode == PipelineMode.DICTATION else \"conversation\"\n                self._analytics = self._analytics_service.create_session(\n                    session_id=self.session_id,\n                    user_id=self.user_id,\n                    mode=pipeline_mode,\n                    on_analytics_update=self._send_analytics_update,\n                )\n                self._analytics_service.set_session_active(self.session_id)\n                logger.info(f\"Analytics session created: {self.session_id}, mode={pipeline_mode}\")\n\n                self._state = PipelineState.LISTENING\n\n                # Notify client\n                await self._send_state_update()\n\n                logger.info(f\"Voice pipeline started: {self.session_id}\")\n                return True\n\n            except Exception as e:\n                logger.error(f\"Failed to start pipeline: {e}\")\n                self._state = PipelineState.ERROR\n                self._metrics.error = str(e)\n                return False\n\n    async def send_audio(self, audio_data: bytes) -> None:\n        \"\"\"\n        Send audio data to the pipeline.\n\n        Args:\n            audio_data: Raw PCM16 audio bytes\n        \"\"\"\n        if self._cancelled or self._state not in (\n            PipelineState.LISTENING,\n            PipelineState.IDLE,\n        ):\n            if self._metrics.audio_chunks_received == 0:\n                logger.warning(f\"Dropping audio - state: {self._state}, cancelled: {self._cancelled}\")\n            return\n\n        self._metrics.audio_chunks_received += 1\n\n        # Log every 100 chunks to confirm audio flow\n        if self._metrics.audio_chunks_received % 100 == 0:\n            logger.debug(f\"Audio chunk #{self._metrics.audio_chunks_received}, {len(audio_data)} bytes\")\n\n        # Send to STT\n        if self._stt_session:\n            await self._stt_session.send_audio(audio_data)\n\n        # Send to emotion detection (parallel, non-blocking)\n        if self._emotion_session:\n            await self._emotion_session.add_audio(audio_data, sample_rate=self.config.stt_sample_rate)\n\n        # Track speech duration for backchanneling\n        current_time = time.time()\n        if self._backchannel_session and self._speech_start_time:\n            speech_duration_ms = int((current_time - self._speech_start_time) * 1000)\n            await self._backchannel_session.on_speech_continue(speech_duration_ms)\n\n            # Pause detection: if we haven't received a transcript in 150-400ms,\n            # it might be a natural pause suitable for backchanneling\n            if self._last_transcript_time > 0:\n                pause_ms = int((current_time - self._last_transcript_time) * 1000)\n                # Only check pause window if we're in the right range (150-400ms)\n                # and the pipeline is in listening state (user speaking)\n                if 150 <= pause_ms <= 400 and self._state == PipelineState.LISTENING:\n                    await self._backchannel_session.on_pause_detected(pause_ms)\n\n        self._last_audio_time = current_time\n\n    async def send_audio_base64(self, audio_b64: str) -> None:\n        \"\"\"\n        Send base64-encoded audio to the pipeline.\n\n        Args:\n            audio_b64: Base64-encoded PCM16 audio\n        \"\"\"\n        try:\n            audio_data = base64.b64decode(audio_b64)\n            # Debug: log every 50th chunk at base64 level\n            if not hasattr(self, \"_b64_chunk_count\"):\n                self._b64_chunk_count = 0\n            self._b64_chunk_count += 1\n            if self._b64_chunk_count % 50 == 0:\n                logger.debug(\n                    f\"[Pipeline] B64 chunk #{self._b64_chunk_count}, \"\n                    f\"decoded {len(audio_data)} bytes, state={self._state}\"\n                )\n            await self.send_audio(audio_data)\n        except Exception as e:\n            logger.error(f\"Failed to decode audio: {e}\")\n\n    async def commit_audio(self) -> None:\n        \"\"\"\n        Signal that the current audio input is complete.\n\n        This triggers immediate processing without waiting for VAD.\n        \"\"\"\n        if self._stt_session and self._state == PipelineState.LISTENING:\n            # Stop STT and get final transcript\n            final = await self._stt_session.stop()\n            if final:\n                self._final_transcript = final\n            await self._process_transcript()\n\n    async def barge_in(self) -> None:\n        \"\"\"\n        Handle barge-in (user interrupts AI).\n\n        Cancels current TTS and prepares for new input.\n        \"\"\"\n        if not self.config.barge_in_enabled:\n            return\n\n        logger.info(f\"Barge-in triggered: {self.session_id}\")\n\n        async with self._state_lock:\n            self._metrics.cancelled = True\n\n            # Cancel Talker if speaking\n            if self._talker_session:\n                await self._talker_session.cancel()\n\n            # Cancel Thinker if processing\n            if self._thinker_session:\n                await self._thinker_session.cancel()\n\n            # Reset to listening state\n            self._state = PipelineState.LISTENING\n            self._partial_transcript = \"\"\n            self._final_transcript = \"\"\n            self._transcript_confidence = 1.0  # Phase 7: Reset confidence\n\n            # Restart STT for new input\n            if self._stt_session:\n                await self._stt_session.stop()\n\n            self._stt_session = await self._stt_service.create_session(\n                on_partial=self._handle_partial_transcript,\n                on_final=self._handle_final_transcript,\n                on_endpoint=self._handle_speech_end,\n                on_speech_start=self._handle_speech_start,\n            )\n            await self._stt_session.start()\n\n            await self._send_state_update()\n\n    async def stop(self) -> PipelineMetrics:\n        \"\"\"\n        Stop the pipeline session.\n\n        Returns:\n            PipelineMetrics with session statistics\n        \"\"\"\n        self._cancelled = True\n        self._metrics.end_time = time.time()\n        self._metrics.total_latency_ms = int((self._metrics.end_time - self._metrics.start_time) * 1000)\n\n        # Stop all components\n        if self._stt_session:\n            await self._stt_session.stop()\n\n        if self._talker_session:\n            await self._talker_session.cancel()\n\n        if self._thinker_session:\n            await self._thinker_session.cancel()\n\n        # Stop emotion detection session\n        if self._emotion_session:\n            await self._emotion_session.stop()\n            self._emotion_session = None\n\n        # Stop backchannel session\n        if self._backchannel_session:\n            await self._backchannel_session.stop()\n            self._backchannel_session = None\n\n        # Stop prosody session and update user profile\n        if self._prosody_session:\n            await self._prosody_service.remove_session(self.session_id)\n            self._prosody_session = None\n\n        # Phase 4: Clean up memory context session\n        if self._memory_manager:\n            try:\n                session_uuid = uuid.UUID(self.session_id) if isinstance(self.session_id, str) else self.session_id\n                memory_context_service.end_session(session_uuid)\n                self._memory_manager = None\n                logger.debug(f\"Memory context cleaned up for session: {self.session_id}\")\n            except Exception as e:\n                logger.warning(f\"Failed to clean up memory context: {e}\")\n\n        # Phase 9: Clean up patient context and PHI monitor\n        if self._patient_context:\n            self._patient_context = None\n            logger.debug(f\"Patient context cleaned up for session: {self.session_id}\")\n\n        if self._phi_monitor:\n            self._phi_monitor.clear_patient_context()\n            logger.debug(f\"PHI monitor context cleared for session: {self.session_id}\")\n\n        # Phase 8: Clean up dictation session\n        if self._dictation_session:\n            self._dictation_session = None\n            logger.debug(f\"Dictation session cleaned up for session: {self.session_id}\")\n\n        # Phase 10: End analytics session and send feedback prompts\n        if self._analytics:\n            # Get feedback prompts based on session\n            prompts = self._feedback_service.get_feedback_prompts(\n                session_id=self.session_id,\n                session_duration_ms=self._analytics.duration_ms,\n                interaction_count=self._analytics.interactions.user_utterance_count,\n                has_errors=self._analytics.error_count > 0,\n            )\n            if prompts:\n                await self._on_message(\n                    PipelineMessage(\n                        type=\"feedback.prompts\",\n                        data={\n                            \"prompts\": [p.to_dict() for p in prompts],\n                        },\n                    )\n                )\n\n            # End the analytics session and send final summary\n            final_analytics = self._analytics_service.end_session(self.session_id)\n            if final_analytics:\n                await self._on_message(\n                    PipelineMessage(\n                        type=\"analytics.session_ended\",\n                        data=final_analytics,\n                    )\n                )\n\n            self._analytics = None\n            logger.debug(f\"Analytics session ended for session: {self.session_id}\")\n\n        self._state = PipelineState.IDLE\n\n        logger.info(\n            f\"Voice pipeline stopped: {self.session_id}\",\n            extra={\n                \"total_latency_ms\": self._metrics.total_latency_ms,\n                \"audio_chunks\": self._metrics.audio_chunks_received,\n            },\n        )\n\n        return self._metrics\n\n    # ==========================================================================\n    # Internal Handlers\n    # ==========================================================================\n\n    async def _handle_emotion_result(self, emotion: EmotionResult) -> None:\n        \"\"\"\n        Handle emotion detection result from Hume AI.\n\n        Updates internal state and sends to frontend for visualization.\n        \"\"\"\n        self._current_emotion = emotion\n\n        # Log emotion detection\n        logger.info(\n            f\"[Pipeline] Emotion detected: {emotion.primary_emotion.value} \"\n            f\"(conf={emotion.primary_confidence:.2f}, valence={emotion.valence:.2f}, arousal={emotion.arousal:.2f})\"\n        )\n\n        # Send emotion update to frontend\n        await self._on_message(\n            PipelineMessage(\n                type=\"emotion.detected\",\n                data=emotion.to_dict(),\n            )\n        )\n\n    async def _handle_backchannel(self, audio: BackchannelAudio) -> None:\n        \"\"\"\n        Handle backchannel audio from the backchannel service.\n\n        Sends the pre-generated audio clip to the frontend for playback.\n        \"\"\"\n        logger.info(f\"[Pipeline] Emitting backchannel: '{audio.phrase}' ({audio.duration_ms}ms)\")\n\n        # Send backchannel audio to frontend\n        await self._on_message(\n            PipelineMessage(\n                type=\"backchannel.trigger\",\n                data={\n                    \"phrase\": audio.phrase,\n                    \"audio\": (base64.b64encode(audio.audio_data).decode() if audio.audio_data else \"\"),\n                    \"format\": audio.format,\n                    \"duration_ms\": audio.duration_ms,\n                },\n            )\n        )\n\n    async def _handle_dictation_event(self, event: DictationEvent) -> None:\n        \"\"\"\n        Phase 8: Handle dictation events from the dictation service.\n\n        Forwards dictation state changes and section updates to the frontend.\n        \"\"\"\n        logger.info(f\"[Pipeline] Dictation event: {event.event_type} - {event.data}\")\n\n        if event.event_type == \"state_change\":\n            await self._on_message(\n                PipelineMessage(\n                    type=\"dictation.state\",\n                    data={\n                        \"state\": event.data.get(\"state\"),\n                        \"note_type\": event.data.get(\"note_type\"),\n                        \"current_section\": event.data.get(\"current_section\"),\n                    },\n                )\n            )\n        elif event.event_type == \"section_update\":\n            await self._on_message(\n                PipelineMessage(\n                    type=\"dictation.section_update\",\n                    data={\n                        \"section\": event.data.get(\"section\"),\n                        \"content\": event.data.get(\"content\"),\n                        \"partial_text\": event.data.get(\"partial_text\"),\n                        \"word_count\": event.data.get(\"word_count\"),\n                        \"is_final\": event.data.get(\"is_final\", False),\n                    },\n                )\n            )\n        elif event.event_type == \"section_change\":\n            await self._on_message(\n                PipelineMessage(\n                    type=\"dictation.section_change\",\n                    data={\n                        \"previous_section\": event.data.get(\"previous_section\"),\n                        \"current_section\": event.data.get(\"current_section\"),\n                    },\n                )\n            )\n\n    async def _handle_word_data(self, words: List[Dict]) -> None:\n        \"\"\"\n        Handle word-level data from Deepgram for prosody analysis.\n\n        Updates the prosody session with word timing data for:\n        - Speech rate calculation\n        - Pause pattern detection\n        - Turn-taking signal analysis\n        \"\"\"\n        if not self._prosody_session:\n            return\n\n        # Feed words to prosody analyzer\n        self._prosody_session.add_words(words)\n\n        # Get current analysis for potential backchannel triggers\n        snapshot = self._prosody_session.get_current_analysis()\n        self._current_prosody = snapshot\n\n        # Log significant prosody events\n        if snapshot.word_count > 0 and snapshot.word_count % 20 == 0:\n            logger.debug(\n                f\"[Pipeline] Prosody update: WPM={snapshot.words_per_minute:.0f}, \"\n                f\"pace={snapshot.pace.value}, pauses={snapshot.pause_count}\"\n            )\n\n        # Check if prosody suggests backchannel timing\n        if self._backchannel_session and self._prosody_session.should_backchannel():\n            # Trigger a backchannel check during natural pause\n            await self._backchannel_session.on_pause_detected(\n                int(snapshot.avg_pause_ms) if snapshot.avg_pause_ms > 0 else 300\n            )\n\n    def _is_substantial_transcript(self, text: str) -> bool:\n        \"\"\"\n        Check if a transcript is substantial enough to trigger barge-in.\n\n        Filters out noise like sighs, \"um\", \"uh\", single short sounds that\n        Deepgram might transcribe from background noise.\n\n        Returns True if the transcript looks like intentional speech.\n        \"\"\"\n        if not text:\n            return False\n\n        cleaned = text.strip().lower()\n\n        # Filter out common filler sounds and very short utterances\n        # These are often background noise or non-intentional sounds\n        noise_patterns = {\n            \"um\",\n            \"uh\",\n            \"hmm\",\n            \"hm\",\n            \"mm\",\n            \"ah\",\n            \"oh\",\n            \"er\",\n            \"erm\",\n            \"mhm\",\n            \"uhm\",\n            \"ehm\",\n            \"ahem\",\n            \"huh\",\n            \"eh\",\n        }\n\n        # Single word that's a noise pattern\n        if cleaned in noise_patterns:\n            logger.debug(f\"[Pipeline] Ignoring noise transcript: '{text}'\")\n            return False\n\n        # Too short (less than 3 characters after stripping) - likely noise\n        if len(cleaned) < 3:\n            logger.debug(f\"[Pipeline] Ignoring too-short transcript: '{text}'\")\n            return False\n\n        # Require at least 2 words OR a clear command word for barge-in\n        # This reduces false positives from sighs/noise\n        words = cleaned.split()\n        command_words = {\n            \"stop\",\n            \"wait\",\n            \"hold\",\n            \"pause\",\n            \"no\",\n            \"actually\",\n            \"but\",\n            \"hey\",\n            \"okay\",\n            \"ok\",\n        }\n\n        if len(words) >= 2:\n            return True\n\n        if len(words) == 1 and words[0] in command_words:\n            return True\n\n        # Single word that's not a command - might be noise, don't barge-in\n        logger.debug(f\"[Pipeline] Single non-command word, not triggering barge-in: '{text}'\")\n        return False\n\n    def _get_barge_in_confidence_threshold(self) -> float:\n        \"\"\"\n        Calculate the confidence threshold for barge-in based on VAD sensitivity.\n\n        VAD sensitivity 0-100 maps to confidence threshold:\n        - 0 (least sensitive) = 0.95 confidence required (very high bar)\n        - 50 (default) = 0.75 confidence required\n        - 100 (most sensitive) = 0.50 confidence required (low bar)\n        \"\"\"\n        # Linear interpolation: sensitivity 0->0.95, 100->0.50\n        # threshold = 0.95 - (sensitivity/100) * 0.45\n        threshold = 0.95 - (self.config.vad_sensitivity / 100) * 0.45\n        return max(0.50, min(0.95, threshold))\n\n    async def _handle_partial_transcript(self, text: str, confidence: float) -> None:\n        \"\"\"Handle partial transcript from STT.\"\"\"\n        logger.info(f\"[Pipeline] Partial transcript received: '{text}' (conf={confidence:.2f})\")\n        self._partial_transcript = text\n        self._transcript_confidence = confidence  # Phase 7: Track for repair strategies\n        self._last_transcript_time = time.time()  # Track for pause detection\n\n        # Trigger barge-in if AI is speaking and we got substantial speech\n        # Requires multiple words or a command word to avoid false positives from noise\n        # Also requires confidence above threshold based on VAD sensitivity\n        if self._state == PipelineState.SPEAKING and self._is_substantial_transcript(text):\n            threshold = self._get_barge_in_confidence_threshold()\n            if confidence >= threshold:\n                logger.info(\n                    f\"[Pipeline] Triggering barge-in (substantial transcript while AI speaking): \"\n                    f\"'{text}' (conf={confidence:.2f} >= threshold={threshold:.2f}, vad_sensitivity={self.config.vad_sensitivity})\"\n                )\n                await self.barge_in()\n                return  # Don't send transcript delta after barge-in\n            else:\n                logger.info(\n                    f\"[Pipeline] Skipping barge-in (confidence too low): '{text}' \"\n                    f\"(conf={confidence:.2f} < threshold={threshold:.2f}, vad_sensitivity={self.config.vad_sensitivity})\"\n                )\n\n        await self._on_message(\n            PipelineMessage(\n                type=\"transcript.delta\",\n                data={\n                    \"text\": text,\n                    \"is_final\": False,\n                    \"confidence\": confidence,\n                },\n            )\n        )\n\n    async def _handle_final_transcript(self, text: str) -> None:\n        \"\"\"Handle final transcript segment from STT.\"\"\"\n        logger.info(f\"[Pipeline] Final transcript received: '{text}'\")\n\n        # Trigger barge-in if AI is speaking and we got substantial speech\n        if self._state == PipelineState.SPEAKING and self._is_substantial_transcript(text):\n            logger.info(f\"[Pipeline] Triggering barge-in (substantial final transcript while AI speaking): '{text}'\")\n            await self.barge_in()\n            # Store the transcript for the next turn\n            self._final_transcript = text\n            return\n\n        if self._final_transcript:\n            self._final_transcript += \" \" + text\n        else:\n            self._final_transcript = text\n\n        # NOTE: Don't emit transcript.delta with is_final=True here.\n        # The transcript.complete message in _process_transcript() is the\n        # authoritative final transcript. Emitting both causes duplicate\n        # messages in the chat UI.\n\n    async def _handle_speech_start(self) -> None:\n        \"\"\"Handle speech start detection from STT (for barge-in).\n\n        NOTE: We do NOT auto-trigger barge-in here because Deepgram's SpeechStarted\n        event can fire on background noise or TTS echo. Instead, barge-in is only\n        triggered when we receive actual transcript text (partial or final) while\n        the AI is speaking. See _handle_partial_transcript() and _handle_final_transcript().\n        \"\"\"\n        logger.info(f\"[Pipeline] Speech start detected: {self.session_id}, current_state={self._state}\")\n\n        # Track speech timing for backchannels\n        self._speech_start_time = time.time()\n\n        # Notify backchannel session\n        if self._backchannel_session:\n            await self._backchannel_session.on_speech_start()\n\n        # Notify frontend of speech start (for UI feedback only)\n        await self._on_message(\n            PipelineMessage(\n                type=\"input_audio_buffer.speech_started\",\n                data={\n                    \"timestamp\": time.time(),\n                    \"vad_confidence\": 0.9,  # Deepgram VAD is high confidence\n                },\n            )\n        )\n\n        # NOTE: DO NOT auto-trigger barge-in here! SpeechStarted fires on ANY\n        # audio that looks like speech (including noise, echo from TTS, etc.)\n        # Barge-in is now triggered in _handle_partial_transcript() when we\n        # get actual words, which is more reliable.\n\n    async def _handle_speech_end(self) -> None:\n        \"\"\"Handle speech endpoint detection from STT.\"\"\"\n        logger.info(f\"[Pipeline] Speech end detected: {self.session_id}, accumulated='{self._final_transcript}'\")\n\n        # Notify backchannel session of speech end\n        if self._backchannel_session:\n            await self._backchannel_session.on_speech_end()\n        self._speech_start_time = None\n\n        # Finalize prosody analysis for this utterance\n        if self._prosody_session:\n            prosody_snapshot = self._prosody_session.finalize_utterance()\n            self._current_prosody = prosody_snapshot\n            logger.info(\n                f\"[Pipeline] Prosody analysis: WPM={prosody_snapshot.words_per_minute:.0f}, \"\n                f\"pace={prosody_snapshot.pace.value}, finished={prosody_snapshot.likely_finished}\"\n            )\n\n        # Get final transcript from STT\n        if self._stt_session:\n            final = await self._stt_session.stop()\n            if final and final != self._final_transcript:\n                self._final_transcript = final\n\n        await self._process_transcript()\n\n    async def _process_transcript(self) -> None:\n        \"\"\"Process the completed transcript through Thinker and Talker.\"\"\"\n        if not self._final_transcript or not self._final_transcript.strip():\n            logger.debug(\"Empty transcript, skipping processing\")\n            self._state = PipelineState.LISTENING\n            return\n\n        async with self._state_lock:\n            self._state = PipelineState.PROCESSING\n            await self._send_state_update()\n\n        transcript = self._final_transcript.strip()\n        self._metrics.stt_latency_ms = int((time.time() - self._metrics.start_time) * 1000)\n\n        # Send final transcript\n        message_id = str(uuid.uuid4())\n        await self._on_message(\n            PipelineMessage(\n                type=\"transcript.complete\",\n                data={\n                    \"text\": transcript,\n                    \"message_id\": message_id,\n                },\n            )\n        )\n\n        # Reset for next utterance\n        self._partial_transcript = \"\"\n        self._final_transcript = \"\"\n\n        # Phase 8: Handle dictation mode separately\n        if self.config.mode == PipelineMode.DICTATION and self._dictation_session:\n            await self._process_dictation_transcript(transcript, message_id)\n            return\n\n        # Create Talker session for TTS\n        voice_config = VoiceConfig(\n            voice_id=self.config.voice_id,\n            model_id=self.config.tts_model,\n            output_format=self.config.tts_output_format,\n        )\n\n        self._talker_session = await self._talker_service.start_session(\n            on_audio_chunk=self._handle_audio_chunk,\n            voice_config=voice_config,\n        )\n\n        # Build emotion context for response adaptation\n        emotion_context = None\n        if self._current_emotion and self._emotion_service:\n            emotion_context = {\n                \"emotion\": self._current_emotion,\n                \"trend\": (self._emotion_session.get_trend() if self._emotion_session else None),\n                \"prompt_addition\": self._emotion_service.build_emotion_context_prompt(\n                    self._current_emotion,\n                    (self._emotion_session.get_trend() if self._emotion_session else None),\n                ),\n            }\n\n        # Phase 4: Track conversation context in memory\n        memory_context = None\n        if self._memory_manager:\n            try:\n                # Remember user's utterance as context\n                await self._memory_manager.remember(\n                    memory_type=MemoryType.CONTEXT,\n                    key=\"last_user_input\",\n                    value=transcript,\n                )\n\n                # Track emotion if detected\n                if self._current_emotion:\n                    await self._memory_manager.remember(\n                        memory_type=MemoryType.EMOTION,\n                        key=\"current_emotion\",\n                        value=self._current_emotion.primary_emotion,\n                        metadata={\n                            \"confidence\": self._current_emotion.primary_confidence,\n                            \"valence\": self._current_emotion.valence,\n                        },\n                    )\n\n                # Get conversation context summary for LLM\n                memory_context = await self._memory_manager.get_context_summary()\n                logger.debug(f\"Memory context for LLM: {memory_context}\")\n            except Exception as e:\n                logger.warning(f\"Failed to track memory context: {e}\")\n\n        # Phase 6: Classify query type for response timing\n        query_type = classify_query_type(transcript)\n        timing_config = RESPONSE_TIMING.get(query_type, RESPONSE_TIMING[QueryType.UNKNOWN])\n        logger.debug(\n            f\"Query type: {query_type.value}, timing: delay={timing_config.delay_ms}ms, filler={timing_config.use_filler}\"\n        )\n\n        # Phase 3, 5 & 6: Apply combined response delay\n        total_delay_ms = 0\n\n        # First, apply prosody-based delay with turn-taking awareness\n        if self._prosody_session:\n            try:\n                # Phase 5: Check turn-taking prediction\n                turn_prediction = self._prosody_session.get_turn_prediction()\n                if turn_prediction.confidence > 0.6:\n                    logger.debug(\n                        f\"Turn prediction: {turn_prediction.state.value} \" f\"(conf={turn_prediction.confidence:.2f})\"\n                    )\n\n                    # Send turn state to frontend for UI feedback\n                    await self._on_message(\n                        PipelineMessage(\n                            type=\"turn.state\",\n                            data={\n                                \"state\": turn_prediction.state.value,\n                                \"confidence\": turn_prediction.confidence,\n                                \"recommended_wait_ms\": turn_prediction.recommended_wait_ms,\n                                \"signals\": {\n                                    \"falling_intonation\": turn_prediction.has_falling_intonation,\n                                    \"trailing_off\": turn_prediction.has_trailing_off,\n                                    \"thinking_aloud\": turn_prediction.is_thinking_aloud,\n                                    \"continuation_cue\": turn_prediction.has_continuation_cue,\n                                },\n                            },\n                        )\n                    )\n\n                    # If user is likely continuing or thinking, wait longer\n                    if self._prosody_session.should_wait_for_continuation():\n                        logger.info(\n                            f\"Waiting for user continuation: \"\n                            f\"state={turn_prediction.state.value}, \"\n                            f\"wait={turn_prediction.recommended_wait_ms}ms\"\n                        )\n\n                # Get prosody-recommended delay\n                prosody_delay_ms = self._prosody_session.get_recommended_response_delay_ms()\n                total_delay_ms = max(prosody_delay_ms, timing_config.delay_ms)\n            except Exception as e:\n                logger.warning(f\"Failed to get prosody delay: {e}\")\n                total_delay_ms = timing_config.delay_ms\n        else:\n            total_delay_ms = timing_config.delay_ms\n\n        # Phase 6: Apply thinking filler for complex queries\n        if timing_config.use_filler and timing_config.filler_phrases and self._talker_session:\n            try:\n                # Select a random filler phrase (not security-sensitive, just for UI variety)\n                filler = random.choice(timing_config.filler_phrases)  # nosec B311\n                logger.info(f\"Sending thinking filler: '{filler}'\")\n\n                # Send filler to TTS immediately (before the main response)\n                await self._talker_session.add_text(filler + \" \")\n\n                # Notify frontend about the filler\n                await self._on_message(\n                    PipelineMessage(\n                        type=\"response.filler\",\n                        data={\n                            \"text\": filler,\n                            \"query_type\": query_type.value,\n                        },\n                    )\n                )\n\n                # Reduce delay since we're already responding with filler\n                total_delay_ms = max(0, total_delay_ms - 400)\n            except Exception as e:\n                logger.warning(f\"Failed to send thinking filler: {e}\")\n\n        # Apply remaining delay\n        if total_delay_ms > 0:\n            logger.debug(f\"Applying response delay: {total_delay_ms}ms\")\n            await asyncio.sleep(total_delay_ms / 1000.0)\n\n        # Process through Thinker\n        try:\n            if self._thinker_session:\n                response = await self._thinker_session.think(\n                    user_input=transcript,\n                    source_mode=\"voice\",\n                    emotion_context=emotion_context,\n                    memory_context=memory_context,  # Phase 4: Pass memory context\n                    transcript_confidence=self._transcript_confidence,  # Phase 7: For repair strategies\n                    session_id=self.session_id,  # Phase 7: For repair strategy tracking\n                )\n\n                self._metrics.tokens_generated = response.tokens_used\n                self._metrics.tool_calls_count = len(response.tool_calls_made)\n\n                # Phase 7: Send repair status if a repair strategy was applied\n                if response.repair_applied:\n                    await self._on_message(\n                        PipelineMessage(\n                            type=\"response.repair\",\n                            data={\n                                \"confidence\": response.confidence,\n                                \"needs_clarification\": response.needs_clarification,\n                                \"repair_applied\": response.repair_applied,\n                            },\n                        )\n                    )\n                    logger.info(\n                        f\"Repair strategy applied: confidence={response.confidence:.2f}, \"\n                        f\"needs_clarification={response.needs_clarification}\"\n                    )\n\n                # Send response complete\n                await self._on_message(\n                    PipelineMessage(\n                        type=\"response.complete\",\n                        data={\n                            \"text\": response.text,\n                            \"message_id\": message_id,\n                            \"citations\": response.citations,\n                            \"confidence\": response.confidence,  # Phase 7: Include confidence\n                            \"needs_clarification\": response.needs_clarification,  # Phase 7\n                        },\n                    )\n                )\n\n        except Exception as e:\n            logger.error(f\"Thinker error: {e}\")\n            await self._on_message(\n                PipelineMessage(\n                    type=\"error\",\n                    data={\n                        \"code\": \"thinker_error\",\n                        \"message\": str(e),\n                        \"recoverable\": True,\n                    },\n                )\n            )\n\n        # Finish TTS\n        if self._talker_session:\n            await self._talker_session.finish()\n\n        # Wait for TTS audio to finish playing on frontend and echo to settle\n        # This prevents the AI's own voice from being transcribed as user speech\n        # 800ms accounts for: audio buffer playback (~300ms) + room reverb (~200ms) + safety margin\n        logger.info(\"[Pipeline] Waiting for TTS echo to settle before restarting STT...\")\n        await asyncio.sleep(0.8)\n\n        # Return to listening\n        async with self._state_lock:\n            self._state = PipelineState.LISTENING\n\n            # Restart STT for next input, preserving on_words callback for prosody/backchannel\n            self._stt_session = await self._stt_service.create_session(\n                on_partial=self._handle_partial_transcript,\n                on_final=self._handle_final_transcript,\n                on_endpoint=self._handle_speech_end,\n                on_speech_start=self._handle_speech_start,\n                on_words=self._handle_word_data,\n            )\n            await self._stt_session.start()\n\n            await self._send_state_update()\n\n    async def _handle_llm_token(self, token: str) -> None:\n        \"\"\"Handle token from Thinker, feed to Talker.\"\"\"\n        if self._cancelled:\n            return\n\n        # Track first token latency\n        if self._metrics.first_token_latency_ms == 0:\n            self._metrics.first_token_latency_ms = int((time.time() - self._metrics.start_time) * 1000)\n            logger.info(f\"First token latency: {self._metrics.first_token_latency_ms}ms\")\n\n        # Send token to client for display\n        await self._on_message(\n            PipelineMessage(\n                type=\"response.delta\",\n                data={\"text\": token},\n            )\n        )\n\n        # Feed to Talker for TTS\n        if self._talker_session and not self._talker_session.is_cancelled():\n            await self._talker_session.add_token(token)\n\n    async def _process_dictation_transcript(\n        self,\n        transcript: str,\n        message_id: str,\n    ) -> None:\n        \"\"\"\n        Phase 8: Process transcript in dictation mode.\n\n        In dictation mode:\n        1. Check for voice commands first\n        2. If command found, execute it\n        3. If not a command, add text to current section\n        4. Apply formatting if configured\n        5. Don't send to Thinker (no AI response needed for regular dictation)\n        \"\"\"\n        logger.info(f\"[Pipeline] Processing dictation transcript: '{transcript}'\")\n\n        # Check for voice commands\n        if self.config.dictation_enable_commands:\n            parsed_command = voice_command_service.parse_command(transcript)\n            if parsed_command:\n                logger.info(f\"[Pipeline] Voice command detected: {parsed_command.command_type.value}\")\n\n                # Execute the command\n                result = await voice_command_service.execute_command(\n                    parsed_command,\n                    self._dictation_session,\n                )\n\n                # Send command result to frontend\n                await self._on_message(\n                    PipelineMessage(\n                        type=\"dictation.command\",\n                        data={\n                            \"command\": parsed_command.command_type.value,\n                            \"category\": parsed_command.category.value,\n                            \"executed\": result.success,\n                            \"message\": result.message,\n                            \"data\": result.data,\n                        },\n                    )\n                )\n\n                # If there's remaining text after the command, add it to dictation\n                if parsed_command.remaining_text:\n                    await self._add_dictation_text(parsed_command.remaining_text)\n\n                # If \"read back\" was requested, speak the content\n                if result.data.get(\"speak\") and result.message:\n                    await self._speak_dictation_feedback(result.message)\n\n                # Return to listening state\n                async with self._state_lock:\n                    self._state = PipelineState.LISTENING\n                    await self._send_state_update()\n                return\n\n        # Not a command - add text to current section\n        await self._add_dictation_text(transcript)\n\n        # Return to listening state\n        async with self._state_lock:\n            self._state = PipelineState.LISTENING\n            await self._send_state_update()\n\n    async def _add_dictation_text(self, text: str) -> None:\n        \"\"\"Add text to the current dictation section with optional formatting.\"\"\"\n        if not self._dictation_session:\n            return\n\n        # Phase 9: Scan for PHI before adding to dictation\n        if self.config.enable_phi_monitoring:\n            phi_result = self._phi_monitor.scan_text(text)\n            if phi_result.matches:\n                logger.info(\n                    f\"[Pipeline] PHI detected in dictation: {len(phi_result.matches)} matches, \"\n                    f\"critical={phi_result.has_critical_phi}\"\n                )\n\n                # Send PHI alerts to frontend\n                for alert in phi_result.alerts:\n                    await self._on_message(\n                        PipelineMessage(\n                            type=\"phi.alert\",\n                            data={\n                                \"alert_level\": alert.alert_level.value,\n                                \"phi_type\": alert.phi_type.value,\n                                \"message\": alert.message,\n                                \"recommended_action\": alert.recommended_action.value,\n                            },\n                        )\n                    )\n\n                # If critical PHI detected outside patient context, use sanitized text\n                if phi_result.has_critical_phi:\n                    logger.warning(f\"[Pipeline] Critical PHI detected - using sanitized text\")\n                    text = phi_result.sanitized_text\n\n        # Apply formatting if configured\n        if self.config.dictation_auto_format:\n            format_result = note_formatter_service.format_text(\n                text,\n                FormattingConfig(level=FormattingLevel.STANDARD),\n            )\n            formatted_text = format_result.formatted\n            logger.debug(\n                f\"[Pipeline] Formatted dictation: '{text}' -> '{formatted_text}', \"\n                f\"changes: {format_result.changes_made}\"\n            )\n        else:\n            formatted_text = text\n\n        # Add to dictation session\n        await self._dictation_session.add_transcript(\n            formatted_text,\n            is_final=True,\n            confidence=self._transcript_confidence,\n        )\n\n    async def _speak_dictation_feedback(self, text: str) -> None:\n        \"\"\"Speak feedback during dictation (e.g., read back content).\"\"\"\n        voice_config = VoiceConfig(\n            voice_id=self.config.voice_id,\n            model_id=self.config.tts_model,\n            output_format=self.config.tts_output_format,\n        )\n\n        self._talker_session = await self._talker_service.start_session(\n            on_audio_chunk=self._handle_audio_chunk,\n            voice_config=voice_config,\n        )\n\n        async with self._state_lock:\n            self._state = PipelineState.SPEAKING\n            await self._send_state_update()\n\n        # Send the text to TTS\n        if self._talker_session:\n            await self._talker_session.add_token(text)\n            await self._talker_session.finish()\n\n    async def _handle_audio_chunk(self, chunk: AudioChunk) -> None:\n        \"\"\"Handle audio chunk from Talker.\"\"\"\n        if self._cancelled:\n            return\n\n        self._metrics.audio_chunks_sent += 1\n\n        # Track TTS latency (first audio)\n        if self._metrics.tts_latency_ms == 0 and chunk.data:\n            self._metrics.tts_latency_ms = int((time.time() - self._metrics.start_time) * 1000)\n\n        # Update state to speaking\n        if self._state != PipelineState.SPEAKING and chunk.data:\n            async with self._state_lock:\n                self._state = PipelineState.SPEAKING\n                await self._send_state_update()\n\n        # Send audio to client\n        await self._on_message(\n            PipelineMessage(\n                type=\"audio.output\",\n                data={\n                    \"audio\": (base64.b64encode(chunk.data).decode() if chunk.data else \"\"),\n                    \"format\": chunk.format,\n                    \"is_final\": chunk.is_final,\n                },\n            )\n        )\n\n    async def _handle_tool_call(self, event: ToolCallEvent) -> None:\n        \"\"\"Handle tool call from Thinker.\"\"\"\n        await self._on_message(\n            PipelineMessage(\n                type=\"tool.call\",\n                data={\n                    \"tool_id\": event.tool_id,\n                    \"tool_name\": event.tool_name,\n                    \"arguments\": event.arguments,\n                },\n            )\n        )\n\n    async def _handle_tool_result(self, event: ToolResultEvent) -> None:\n        \"\"\"Handle tool result from Thinker.\"\"\"\n        await self._on_message(\n            PipelineMessage(\n                type=\"tool.result\",\n                data={\n                    \"tool_id\": event.tool_id,\n                    \"tool_name\": event.tool_name,\n                    \"result\": event.result,\n                    \"citations\": event.citations,\n                },\n            )\n        )\n\n    async def _send_state_update(self) -> None:\n        \"\"\"Send state update to client.\"\"\"\n        await self._on_message(\n            PipelineMessage(\n                type=\"voice.state\",\n                data={\"state\": self._state.value},\n            )\n        )\n\n    # ==========================================================================\n    # Phase 10: Analytics Methods\n    # ==========================================================================\n\n    async def _send_analytics_update(self, analytics_data: Dict[str, Any]) -> None:\n        \"\"\"\n        Send analytics update to frontend.\n\n        Called periodically by the analytics service.\n        \"\"\"\n        await self._on_message(\n            PipelineMessage(\n                type=\"analytics.update\",\n                data=analytics_data,\n            )\n        )\n\n    def _track_stt_latency(self, latency_ms: float) -> None:\n        \"\"\"Track STT latency in analytics.\"\"\"\n        if self._analytics:\n            self._analytics_service.record_latency(self.session_id, \"stt\", latency_ms)\n\n    def _track_llm_latency(self, latency_ms: float) -> None:\n        \"\"\"Track LLM latency in analytics.\"\"\"\n        if self._analytics:\n            self._analytics_service.record_latency(self.session_id, \"llm\", latency_ms)\n\n    def _track_tts_latency(self, latency_ms: float) -> None:\n        \"\"\"Track TTS latency in analytics.\"\"\"\n        if self._analytics:\n            self._analytics_service.record_latency(self.session_id, \"tts\", latency_ms)\n\n    def _track_e2e_latency(self, latency_ms: float) -> None:\n        \"\"\"Track end-to-end latency in analytics.\"\"\"\n        if self._analytics:\n            self._analytics_service.record_latency(self.session_id, \"e2e\", latency_ms)\n\n    def _track_user_utterance(self, word_count: int, duration_ms: float) -> None:\n        \"\"\"Track user utterance in analytics.\"\"\"\n        if self._analytics:\n            self._analytics_service.record_interaction(\n                self.session_id,\n                InteractionType.USER_UTTERANCE,\n                word_count=word_count,\n                duration_ms=duration_ms,\n            )\n\n    def _track_ai_response(self, word_count: int, duration_ms: float) -> None:\n        \"\"\"Track AI response in analytics.\"\"\"\n        if self._analytics:\n            self._analytics_service.record_interaction(\n                self.session_id,\n                InteractionType.AI_RESPONSE,\n                word_count=word_count,\n                duration_ms=duration_ms,\n            )\n\n    def _track_tool_call(self) -> None:\n        \"\"\"Track tool call in analytics.\"\"\"\n        if self._analytics:\n            self._analytics_service.record_interaction(\n                self.session_id,\n                InteractionType.TOOL_CALL,\n            )\n\n    def _track_barge_in(self) -> None:\n        \"\"\"Track barge-in in analytics.\"\"\"\n        if self._analytics:\n            self._analytics_service.record_interaction(\n                self.session_id,\n                InteractionType.BARGE_IN,\n            )\n\n    def _track_emotion(self, emotion: str, valence: float, arousal: float) -> None:\n        \"\"\"Track detected emotion in analytics.\"\"\"\n        if self._analytics:\n            self._analytics_service.record_emotion(self.session_id, emotion, valence, arousal)\n\n    def _track_repair(self) -> None:\n        \"\"\"Track repair/clarification in analytics.\"\"\"\n        if self._analytics:\n            self._analytics_service.record_repair(self.session_id)\n\n    def _track_dictation_event(self, event_type: str, data: Optional[Dict] = None) -> None:\n        \"\"\"Track dictation event in analytics.\"\"\"\n        if self._analytics:\n            self._analytics_service.record_dictation_event(self.session_id, event_type, data)\n\n    def _track_error(self, error_type: str, message: str, recoverable: bool = True) -> None:\n        \"\"\"Track error in analytics.\"\"\"\n        if self._analytics:\n            self._analytics_service.record_error(self.session_id, error_type, message, recoverable)\n\n    async def record_feedback(\n        self,\n        thumbs_up: bool = True,\n        message_id: Optional[str] = None,\n    ) -> None:\n        \"\"\"\n        Record quick feedback from user.\n\n        Args:\n            thumbs_up: True for positive feedback\n            message_id: Optional ID of message being rated\n        \"\"\"\n        self._feedback_service.record_quick_feedback(\n            session_id=self.session_id,\n            user_id=self.user_id,\n            thumbs_up=thumbs_up,\n            message_id=message_id,\n        )\n\n        # Send confirmation to frontend\n        await self._on_message(\n            PipelineMessage(\n                type=\"feedback.recorded\",\n                data={\n                    \"thumbs_up\": thumbs_up,\n                    \"message_id\": message_id,\n                },\n            )\n        )\n\n\n# ==============================================================================\n# Voice Pipeline Service\n# ==============================================================================\n\n\nclass VoicePipelineService:\n    \"\"\"\n    Factory for creating voice pipeline sessions.\n\n    Usage:\n        service = VoicePipelineService()\n\n        session = await service.create_session(\n            conversation_id=\"conv-123\",\n            on_message=handle_message,\n        )\n\n        await session.start()\n        await session.send_audio(audio_chunk)\n        await session.stop()\n    \"\"\"\n\n    def __init__(self):\n        self._stt_service = streaming_stt_service\n        self._thinker_service = thinker_service\n        self._talker_service = talker_service\n\n        # Active sessions\n        self._sessions: Dict[str, VoicePipelineSession] = {}\n\n    def is_available(self) -> bool:\n        \"\"\"Check if the pipeline is available.\"\"\"\n        return self._stt_service.is_streaming_available() and self._talker_service.is_enabled()\n\n    async def create_session(\n        self,\n        conversation_id: str,\n        on_message: Callable[[PipelineMessage], Awaitable[None]],\n        config: Optional[PipelineConfig] = None,\n        user_id: Optional[str] = None,\n    ) -> VoicePipelineSession:\n        \"\"\"\n        Create a new voice pipeline session.\n\n        Args:\n            conversation_id: Conversation identifier\n            on_message: Callback for pipeline messages\n            config: Optional pipeline configuration\n            user_id: User ID for tool authentication (required for calendar, etc.)\n\n        Returns:\n            VoicePipelineSession instance\n        \"\"\"\n        session_id = str(uuid.uuid4())\n        config = config or PipelineConfig()\n\n        session = VoicePipelineSession(\n            session_id=session_id,\n            conversation_id=conversation_id,\n            config=config,\n            stt_service=self._stt_service,\n            thinker_service=self._thinker_service,\n            talker_service=self._talker_service,\n            on_message=on_message,\n            user_id=user_id,\n        )\n\n        self._sessions[session_id] = session\n\n        logger.info(f\"Created voice pipeline session: {session_id}\")\n        return session\n\n    def get_session(self, session_id: str) -> Optional[VoicePipelineSession]:\n        \"\"\"Get an active session by ID.\"\"\"\n        return self._sessions.get(session_id)\n\n    async def remove_session(self, session_id: str) -> None:\n        \"\"\"Remove a session.\"\"\"\n        session = self._sessions.pop(session_id, None)\n        if session:\n            await session.stop()\n            logger.info(f\"Removed voice pipeline session: {session_id}\")\n\n    def get_active_sessions(self) -> List[str]:\n        \"\"\"Get list of active session IDs.\"\"\"\n        return list(self._sessions.keys())\n\n\n# Global service instance\nvoice_pipeline_service = VoicePipelineService()\n"
}
