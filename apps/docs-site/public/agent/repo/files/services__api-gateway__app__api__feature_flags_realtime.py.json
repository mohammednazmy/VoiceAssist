{
  "path": "services/api-gateway/app/api/feature_flags_realtime.py",
  "language": "python",
  "size": 42683,
  "last_modified": "2025-12-04T12:32:40.264Z",
  "lines": 1180,
  "content": "\"\"\"Feature Flags Real-time API (Phase 3 - Real-time Propagation).\n\nProvides Server-Sent Events (SSE) for real-time feature flag updates.\nClients can subscribe to flag changes and receive push notifications\nwhen flags are updated, avoiding polling overhead.\n\nUsage:\n    # Subscribe to all flag changes\n    curl -N http://localhost:5000/api/flags/stream\n\n    # Subscribe to specific flags\n    curl -N \"http://localhost:5000/api/flags/stream?flags=ui.dark_mode,backend.rag_strategy\"\n\nEvents:\n    - connected: Initial connection with current flag state and version\n    - flag_update: Single flag was updated\n    - flags_bulk_update: Multiple flags changed (e.g., import)\n    - heartbeat: Keep-alive ping every 30 seconds\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport json\nimport time\nfrom datetime import datetime, timezone\nfrom typing import Any, Dict, List, Optional, Set, Tuple\n\nfrom app.core.api_envelope import success_response\nfrom app.core.database import get_db, redis_client\nfrom app.core.dependencies import get_optional_current_user\nfrom app.core.logging import get_logger\nfrom app.core.metrics import (\n    sse_clients_notified,\n    sse_connection_duration_seconds,\n    sse_connections_active,\n    sse_connections_total,\n    sse_event_delivery_latency_seconds,\n    sse_events_dropped_total,\n    sse_events_replayed_total,\n    sse_flag_update_rate,\n    sse_flag_updates_broadcast_total,\n    sse_history_incomplete_total,\n    sse_reconnects_total,\n    sse_version_lag,\n)\nfrom app.models.user import User\nfrom app.services.feature_flags import feature_flag_service\nfrom fastapi import APIRouter, Depends, Query, Request\nfrom fastapi.responses import JSONResponse, StreamingResponse\nfrom sqlalchemy.orm import Session\n\nrouter = APIRouter(prefix=\"/api/flags\", tags=[\"feature-flags\", \"realtime\"])\nlogger = get_logger(__name__)\n\n# Redis keys for pub/sub\nFLAG_UPDATE_CHANNEL = \"feature_flags:updates\"\nFLAG_VERSION_KEY = \"feature_flags:version\"\nFLAG_EVENT_HISTORY_KEY = \"feature_flags:events\"  # Sorted set for global event history\nFLAG_EVENT_HISTORY_PER_FLAG_PREFIX = \"feature_flags:events:\"  # Per-flag event history\nCLIENT_LAST_EVENT_KEY = \"feature_flags:client_last_event:\"  # Per-client last event\n\n# SSE heartbeat interval (seconds)\nHEARTBEAT_INTERVAL = 30\n\n# Event history settings (configurable)\nEVENT_HISTORY_MAX_SIZE = 1000  # Maximum events to keep in global history\nEVENT_HISTORY_TTL = 3600  # 1 hour TTL for event history (configurable)\nEVENT_HISTORY_PER_FLAG_MAX_SIZE = 100  # Max events per flag\nEVENT_HISTORY_PRUNE_THRESHOLD = 1500  # Prune when this many events accumulated\n\n\nclass FlagSubscriptionManager:\n    \"\"\"Manages SSE subscriptions for feature flag updates.\n\n    Handles:\n    - Client connection tracking\n    - Flag-specific subscriptions\n    - Broadcasting updates to connected clients\n    - Redis pub/sub for cross-instance coordination\n    - Connection duration and latency tracking\n    \"\"\"\n\n    def __init__(self):\n        self._connections: Dict[str, asyncio.Queue] = {}\n        self._subscriptions: Dict[str, Set[str]] = {}  # connection_id -> set of flag names\n        self._connection_times: Dict[str, float] = {}  # connection_id -> connect timestamp\n        self._client_versions: Dict[str, int] = {}  # connection_id -> last known version\n        self._version = 0\n        self._lock = asyncio.Lock()\n        self.logger = get_logger(__name__)\n\n    async def connect(self, client_id: str, flag_filter: Optional[List[str]] = None) -> asyncio.Queue:\n        \"\"\"Register a new SSE client connection.\n\n        Args:\n            client_id: Unique client identifier\n            flag_filter: Optional list of flag names to subscribe to (None = all flags)\n\n        Returns:\n            Queue for sending events to this client\n        \"\"\"\n        async with self._lock:\n            queue: asyncio.Queue = asyncio.Queue()\n            self._connections[client_id] = queue\n            self._subscriptions[client_id] = set(flag_filter) if flag_filter else set()\n            self._connection_times[client_id] = time.time()  # Track connection start\n            self._client_versions[client_id] = get_current_version()  # Initial version\n\n            # Update metrics\n            sse_connections_active.inc()\n            sse_connections_total.labels(action=\"connect\").inc()\n\n            self.logger.info(\n                f\"SSE client connected: {client_id}\",\n                extra={\n                    \"filter\": flag_filter,\n                    \"total_connections\": len(self._connections),\n                },\n            )\n            return queue\n\n    async def disconnect(self, client_id: str) -> None:\n        \"\"\"Remove a client connection.\"\"\"\n        async with self._lock:\n            if client_id in self._connections:\n                del self._connections[client_id]\n                # Update metrics\n                sse_connections_active.dec()\n                sse_connections_total.labels(action=\"disconnect\").inc()\n\n                # Track connection duration\n                if client_id in self._connection_times:\n                    duration = time.time() - self._connection_times[client_id]\n                    sse_connection_duration_seconds.observe(duration)\n                    del self._connection_times[client_id]\n\n            if client_id in self._subscriptions:\n                del self._subscriptions[client_id]\n            if client_id in self._client_versions:\n                del self._client_versions[client_id]\n\n            self.logger.info(\n                f\"SSE client disconnected: {client_id}\",\n                extra={\"total_connections\": len(self._connections)},\n            )\n\n    async def broadcast_flag_update(\n        self,\n        flag_name: str,\n        flag_data: Dict[str, Any],\n        version: int,\n    ) -> int:\n        \"\"\"Broadcast a flag update to subscribed clients.\n\n        Args:\n            flag_name: Name of the updated flag\n            flag_data: Complete flag data\n            version: New version number\n\n        Returns:\n            Number of clients notified\n        \"\"\"\n        broadcast_start = time.time()\n\n        event = {\n            \"event\": \"flag_update\",\n            \"data\": {\n                \"flag\": flag_name,\n                \"value\": flag_data,\n                \"version\": version,\n                \"timestamp\": datetime.now(timezone.utc).isoformat() + \"Z\",\n            },\n        }\n\n        notified = 0\n        dropped = 0\n        async with self._lock:\n            for client_id, queue in self._connections.items():\n                # Check if client is subscribed to this flag\n                subscriptions = self._subscriptions.get(client_id, set())\n                if not subscriptions or flag_name in subscriptions:\n                    try:\n                        await queue.put(event)\n                        notified += 1\n                        # Update client's known version\n                        self._client_versions[client_id] = version\n                    except Exception as e:\n                        dropped += 1\n                        sse_events_dropped_total.labels(reason=\"queue_error\").inc()\n                        self.logger.warning(f\"Failed to notify client {client_id}: {e}\")\n\n        # Calculate delivery latency\n        delivery_latency = time.time() - broadcast_start\n        sse_event_delivery_latency_seconds.labels(event_type=\"flag_update\").observe(delivery_latency)\n\n        # Track broadcast metrics\n        sse_flag_updates_broadcast_total.labels(event_type=\"flag_update\").inc()\n        sse_flag_update_rate.labels(flag_name=flag_name).inc()  # Per-flag update rate\n        sse_clients_notified.observe(notified)\n\n        self.logger.info(\n            f\"Broadcast flag update: {flag_name}\",\n            extra={\n                \"version\": version,\n                \"clients_notified\": notified,\n                \"dropped\": dropped,\n                \"latency_ms\": delivery_latency * 1000,\n            },\n        )\n        return notified\n\n    async def broadcast_bulk_update(\n        self,\n        flags: Dict[str, Dict[str, Any]],\n        version: int,\n    ) -> int:\n        \"\"\"Broadcast multiple flag updates at once.\n\n        Args:\n            flags: Dictionary of flag_name -> flag_data\n            version: New version number\n\n        Returns:\n            Number of clients notified\n        \"\"\"\n        broadcast_start = time.time()\n\n        event = {\n            \"event\": \"flags_bulk_update\",\n            \"data\": {\n                \"flags\": flags,\n                \"version\": version,\n                \"timestamp\": datetime.now(timezone.utc).isoformat() + \"Z\",\n            },\n        }\n\n        notified = 0\n        dropped = 0\n        async with self._lock:\n            for client_id, queue in self._connections.items():\n                subscriptions = self._subscriptions.get(client_id, set())\n                # Check if any subscribed flags were updated\n                if not subscriptions or any(f in subscriptions for f in flags.keys()):\n                    try:\n                        await queue.put(event)\n                        notified += 1\n                        # Update client's known version\n                        self._client_versions[client_id] = version\n                    except Exception as e:\n                        dropped += 1\n                        sse_events_dropped_total.labels(reason=\"queue_error\").inc()\n                        self.logger.warning(f\"Failed to notify client {client_id}: {e}\")\n\n        # Calculate delivery latency\n        delivery_latency = time.time() - broadcast_start\n        sse_event_delivery_latency_seconds.labels(event_type=\"flags_bulk_update\").observe(delivery_latency)\n\n        # Track broadcast metrics\n        sse_flag_updates_broadcast_total.labels(event_type=\"flags_bulk_update\").inc()\n        sse_clients_notified.observe(notified)\n\n        # Track per-flag update rate for bulk updates\n        for flag_name in flags.keys():\n            sse_flag_update_rate.labels(flag_name=flag_name).inc()\n\n        return notified\n\n    def get_connection_count(self) -> int:\n        \"\"\"Get current number of connected clients.\"\"\"\n        return len(self._connections)\n\n    def get_version_drift_stats(self) -> Dict[str, Any]:\n        \"\"\"Get version drift statistics across connected clients.\n\n        Returns:\n            Dictionary with min/max/avg drift from current version\n        \"\"\"\n        current_version = get_current_version()\n        if not self._client_versions:\n            return {\"min_drift\": 0, \"max_drift\": 0, \"avg_drift\": 0.0, \"clients\": 0}\n\n        drifts = [current_version - v for v in self._client_versions.values()]\n        return {\n            \"min_drift\": min(drifts) if drifts else 0,\n            \"max_drift\": max(drifts) if drifts else 0,\n            \"avg_drift\": sum(drifts) / len(drifts) if drifts else 0.0,\n            \"clients\": len(drifts),\n        }\n\n\n# Global subscription manager\nflag_subscription_manager = FlagSubscriptionManager()\n\n\n# ============================================================================\n# SSE Rate Limiter\n# ============================================================================\n\nSSE_RATE_LIMIT_WINDOW = 60  # seconds\nSSE_RATE_LIMIT_MAX_CONNECTIONS = 10  # max connections per IP per window\nSSE_RATE_LIMIT_KEY_PREFIX = \"sse_rate_limit:\"\n\n\nclass SSERateLimiter:\n    \"\"\"Rate limiter for SSE connections.\n\n    Limits the number of SSE connections from a single IP address\n    within a time window to prevent abuse.\n    \"\"\"\n\n    def __init__(\n        self,\n        window_seconds: int = SSE_RATE_LIMIT_WINDOW,\n        max_connections: int = SSE_RATE_LIMIT_MAX_CONNECTIONS,\n    ):\n        self.window_seconds = window_seconds\n        self.max_connections = max_connections\n        self.logger = get_logger(__name__)\n\n    def _get_key(self, client_ip: str) -> str:\n        \"\"\"Get Redis key for rate limiting.\"\"\"\n        return f\"{SSE_RATE_LIMIT_KEY_PREFIX}{client_ip}\"\n\n    def check_rate_limit(self, client_ip: str) -> Tuple[bool, int]:\n        \"\"\"Check if a client IP is within rate limits.\n\n        Args:\n            client_ip: Client IP address\n\n        Returns:\n            Tuple of (is_allowed, current_count)\n        \"\"\"\n        if not client_ip:\n            return True, 0\n\n        try:\n            key = self._get_key(client_ip)\n\n            # Get current count\n            current = redis_client.get(key)\n            count = int(current) if current else 0\n\n            if count >= self.max_connections:\n                self.logger.warning(\n                    f\"SSE rate limit exceeded for IP: {client_ip}\",\n                    extra={\"count\": count, \"limit\": self.max_connections},\n                )\n                return False, count\n\n            # Increment counter\n            pipe = redis_client.pipeline()\n            pipe.incr(key)\n            pipe.expire(key, self.window_seconds)\n            pipe.execute()\n\n            return True, count + 1\n\n        except Exception as e:\n            self.logger.warning(f\"Rate limit check failed: {e}\")\n            # Allow on error to avoid blocking legitimate users\n            return True, 0\n\n    def release(self, client_ip: str) -> None:\n        \"\"\"Release a rate limit slot when connection closes.\n\n        Args:\n            client_ip: Client IP address\n        \"\"\"\n        if not client_ip:\n            return\n\n        try:\n            key = self._get_key(client_ip)\n            # Decrement counter (but don't go below 0)\n            current = redis_client.get(key)\n            if current and int(current) > 0:\n                redis_client.decr(key)\n        except Exception as e:\n            self.logger.warning(f\"Rate limit release failed: {e}\")\n\n\n# Global rate limiter\nsse_rate_limiter = SSERateLimiter()\n\n\n# ============================================================================\n# RBAC Helpers for SSE\n# ============================================================================\n\n# Flag visibility levels\nFLAG_VISIBILITY_PUBLIC = \"public\"  # Anyone can see\nFLAG_VISIBILITY_AUTHENTICATED = \"authenticated\"  # Any authenticated user\nFLAG_VISIBILITY_ADMIN = \"admin\"  # Admin only\nFLAG_VISIBILITY_INTERNAL = \"internal\"  # Internal flags, not exposed via SSE\n\n# Default visibility for flags without explicit setting\nDEFAULT_FLAG_VISIBILITY = FLAG_VISIBILITY_PUBLIC\n\n\ndef get_flag_visibility(flag_metadata: Optional[Dict[str, Any]]) -> str:\n    \"\"\"Get the visibility level for a flag from its metadata.\n\n    Args:\n        flag_metadata: Flag metadata dictionary\n\n    Returns:\n        Visibility level string\n    \"\"\"\n    if not flag_metadata:\n        return DEFAULT_FLAG_VISIBILITY\n    return flag_metadata.get(\"visibility\", DEFAULT_FLAG_VISIBILITY)\n\n\ndef user_can_access_flag(\n    user: Optional[User],\n    flag_metadata: Optional[Dict[str, Any]],\n) -> bool:\n    \"\"\"Check if a user can access a flag based on visibility settings.\n\n    Args:\n        user: User object (None if unauthenticated)\n        flag_metadata: Flag metadata dictionary\n\n    Returns:\n        True if user can access the flag\n    \"\"\"\n    visibility = get_flag_visibility(flag_metadata)\n\n    # Public flags are accessible to everyone\n    if visibility == FLAG_VISIBILITY_PUBLIC:\n        return True\n\n    # Internal flags are never exposed via SSE\n    if visibility == FLAG_VISIBILITY_INTERNAL:\n        return False\n\n    # Authenticated flags require a user\n    if visibility == FLAG_VISIBILITY_AUTHENTICATED:\n        return user is not None\n\n    # Admin flags require admin role\n    if visibility == FLAG_VISIBILITY_ADMIN:\n        if user is None:\n            return False\n        return user.admin_role in {\"admin\", \"viewer\"}\n\n    # Unknown visibility, default to public\n    return True\n\n\nasync def filter_flags_for_user(\n    flags: List[Any],\n    user: Optional[User],\n) -> List[Any]:\n    \"\"\"Filter a list of flags based on user permissions.\n\n    Args:\n        flags: List of flag objects\n        user: User object (None if unauthenticated)\n\n    Returns:\n        Filtered list of flags the user can access\n    \"\"\"\n    return [f for f in flags if user_can_access_flag(user, getattr(f, \"flag_metadata\", None))]\n\n\ndef get_client_ip(request: Request) -> Optional[str]:\n    \"\"\"Extract client IP from request, handling proxies.\n\n    Args:\n        request: FastAPI request object\n\n    Returns:\n        Client IP address or None\n    \"\"\"\n    # Check X-Forwarded-For first (set by reverse proxy)\n    forwarded = request.headers.get(\"X-Forwarded-For\")\n    if forwarded:\n        return forwarded.split(\",\")[0].strip()\n    # Fall back to direct connection IP\n    if request.client:\n        return request.client.host\n    return None\n\n\ndef get_current_version() -> int:\n    \"\"\"Get the current global flag version from Redis.\"\"\"\n    try:\n        version = redis_client.get(FLAG_VERSION_KEY)\n        return int(version) if version else 0\n    except Exception:\n        return 0\n\n\ndef increment_version() -> int:\n    \"\"\"Increment and return the new global flag version.\"\"\"\n    try:\n        return redis_client.incr(FLAG_VERSION_KEY)\n    except Exception:\n        return int(time.time())\n\n\ndef store_event_in_history(\n    event_id: int,\n    event_data: Dict[str, Any],\n    flag_name: Optional[str] = None,\n) -> None:\n    \"\"\"Store an event in Redis sorted set for Last-Event-ID support.\n\n    Stores events in both global history and per-flag history for efficient replay.\n\n    Args:\n        event_id: Monotonic event ID (version number)\n        event_data: Event data to store\n        flag_name: Optional flag name for per-flag storage\n    \"\"\"\n    try:\n        event_json = json.dumps(event_data)\n\n        # Store in global history\n        redis_client.zadd(FLAG_EVENT_HISTORY_KEY, {event_json: event_id})\n\n        # Prune global history if threshold reached (check periodically)\n        current_count = redis_client.zcard(FLAG_EVENT_HISTORY_KEY)\n        if current_count and current_count > EVENT_HISTORY_PRUNE_THRESHOLD:\n            # Keep only the most recent events\n            redis_client.zremrangebyrank(FLAG_EVENT_HISTORY_KEY, 0, -EVENT_HISTORY_MAX_SIZE - 1)\n            logger.debug(f\"Pruned global event history from {current_count} to {EVENT_HISTORY_MAX_SIZE}\")\n\n        # Set TTL on global key\n        redis_client.expire(FLAG_EVENT_HISTORY_KEY, EVENT_HISTORY_TTL)\n\n        # Store in per-flag history if flag_name provided\n        if flag_name:\n            per_flag_key = f\"{FLAG_EVENT_HISTORY_PER_FLAG_PREFIX}{flag_name}\"\n            redis_client.zadd(per_flag_key, {event_json: event_id})\n\n            # Prune per-flag history\n            per_flag_count = redis_client.zcard(per_flag_key)\n            if per_flag_count and per_flag_count > EVENT_HISTORY_PER_FLAG_MAX_SIZE + 50:\n                redis_client.zremrangebyrank(per_flag_key, 0, -EVENT_HISTORY_PER_FLAG_MAX_SIZE - 1)\n\n            # Set TTL on per-flag key\n            redis_client.expire(per_flag_key, EVENT_HISTORY_TTL)\n\n    except Exception as e:\n        logger.warning(f\"Failed to store event in history: {e}\")\n\n\ndef get_events_since(\n    last_event_id: int,\n    flag_filter: Optional[List[str]] = None,\n) -> Tuple[List[Dict[str, Any]], bool]:\n    \"\"\"Get all events since a given event ID.\n\n    Used for Last-Event-ID reconnection support. Detects incomplete history\n    and returns a flag indicating whether a bulk refresh is needed.\n\n    Args:\n        last_event_id: The last event ID the client received\n        flag_filter: Optional list of flag names to filter to\n\n    Returns:\n        Tuple of (events, history_complete):\n        - events: List of events since that ID (exclusive)\n        - history_complete: True if all events could be retrieved, False if\n          history was pruned and client should do bulk refresh\n    \"\"\"\n    try:\n        history_complete = True\n        result = []\n\n        # If filtering to specific flags, try per-flag history first\n        if flag_filter and len(flag_filter) <= 5:\n            # For small filters, use per-flag history for efficiency\n            for flag_name in flag_filter:\n                per_flag_key = f\"{FLAG_EVENT_HISTORY_PER_FLAG_PREFIX}{flag_name}\"\n                events = redis_client.zrangebyscore(\n                    per_flag_key,\n                    f\"({last_event_id}\",\n                    \"+inf\",\n                    withscores=True,\n                )\n                for event_json, score in events:\n                    try:\n                        event_data = json.loads(event_json)\n                        event_data[\"_event_id\"] = int(score)\n                        result.append(event_data)\n                    except json.JSONDecodeError:\n                        continue\n        else:\n            # Use global history\n            events = redis_client.zrangebyscore(\n                FLAG_EVENT_HISTORY_KEY,\n                f\"({last_event_id}\",\n                \"+inf\",\n                withscores=True,\n            )\n\n            for event_json, score in events:\n                try:\n                    event_data = json.loads(event_json)\n                    event_data[\"_event_id\"] = int(score)\n\n                    # Apply filter if specified\n                    if flag_filter:\n                        event_flag = event_data.get(\"flag\")\n                        if event_flag and event_flag not in flag_filter:\n                            continue\n\n                    result.append(event_data)\n                except json.JSONDecodeError:\n                    continue\n\n        # Detect incomplete history: check if there's a gap\n        if result:\n            # Get the oldest event in history to detect gaps\n            oldest_events = redis_client.zrangebyscore(\n                FLAG_EVENT_HISTORY_KEY,\n                \"-inf\",\n                \"+inf\",\n                start=0,\n                num=1,\n                withscores=True,\n            )\n            if oldest_events:\n                oldest_event_id = int(oldest_events[0][1])\n                # If client's last event is older than our oldest, history is incomplete\n                if last_event_id < oldest_event_id - 1:\n                    history_complete = False\n                    logger.warning(\n                        f\"Incomplete event history: client at {last_event_id}, \"\n                        f\"oldest available is {oldest_event_id}\"\n                    )\n\n        # Sort by event_id to ensure ordering\n        result.sort(key=lambda e: e.get(\"_event_id\", 0))\n\n        return result, history_complete\n    except Exception as e:\n        logger.warning(f\"Failed to get events since {last_event_id}: {e}\")\n        return [], False\n\n\ndef get_history_stats() -> Dict[str, Any]:\n    \"\"\"Get statistics about the event history.\n\n    Returns:\n        Dictionary with history size, oldest/newest event IDs, TTL remaining\n    \"\"\"\n    try:\n        # Global history stats\n        global_count = redis_client.zcard(FLAG_EVENT_HISTORY_KEY) or 0\n        global_ttl = redis_client.ttl(FLAG_EVENT_HISTORY_KEY)\n\n        oldest_id = None\n        newest_id = None\n\n        if global_count > 0:\n            oldest = redis_client.zrange(FLAG_EVENT_HISTORY_KEY, 0, 0, withscores=True)\n            newest = redis_client.zrange(FLAG_EVENT_HISTORY_KEY, -1, -1, withscores=True)\n            if oldest:\n                oldest_id = int(oldest[0][1])\n            if newest:\n                newest_id = int(newest[0][1])\n\n        return {\n            \"global_event_count\": global_count,\n            \"global_ttl_seconds\": global_ttl,\n            \"oldest_event_id\": oldest_id,\n            \"newest_event_id\": newest_id,\n            \"max_size\": EVENT_HISTORY_MAX_SIZE,\n            \"per_flag_max_size\": EVENT_HISTORY_PER_FLAG_MAX_SIZE,\n        }\n    except Exception as e:\n        logger.warning(f\"Failed to get history stats: {e}\")\n        return {\"error\": str(e)}\n\n\ndef update_client_last_event(client_id: str, event_id: int) -> None:\n    \"\"\"Update the last event ID sent to a client.\n\n    Args:\n        client_id: Client identifier\n        event_id: Last event ID sent\n    \"\"\"\n    try:\n        key = f\"{CLIENT_LAST_EVENT_KEY}{client_id}\"\n        redis_client.setex(key, EVENT_HISTORY_TTL, str(event_id))\n    except Exception as e:\n        logger.warning(f\"Failed to update client last event: {e}\")\n\n\nasync def publish_flag_update(flag_name: str, flag_data: Dict[str, Any]) -> None:\n    \"\"\"Publish a flag update to Redis pub/sub and local subscribers.\n\n    Called by the FeatureFlagService when a flag is updated.\n    Stores the event in both global and per-flag history for Last-Event-ID support.\n    \"\"\"\n    version = increment_version()\n\n    # Create event data\n    event_data = {\n        \"type\": \"flag_update\",\n        \"flag\": flag_name,\n        \"data\": flag_data,\n        \"version\": version,\n        \"timestamp\": datetime.now(timezone.utc).isoformat() + \"Z\",\n    }\n\n    # Store event in history for Last-Event-ID support (both global and per-flag)\n    store_event_in_history(version, event_data, flag_name=flag_name)\n\n    # Broadcast to local SSE connections\n    await flag_subscription_manager.broadcast_flag_update(flag_name, flag_data, version)\n\n    # Publish to Redis for cross-instance coordination\n    try:\n        redis_client.publish(FLAG_UPDATE_CHANNEL, json.dumps(event_data))\n    except Exception as e:\n        logger.warning(f\"Failed to publish flag update to Redis: {e}\")\n\n\nasync def event_generator(\n    client_id: str,\n    queue: asyncio.Queue,\n    flag_filter: Optional[List[str]],\n    db: Session,\n    last_event_id: Optional[int] = None,\n    current_user: Optional[User] = None,\n):\n    \"\"\"Generate SSE events for a connected client.\n\n    Supports Last-Event-ID for reconnection: if provided, sends missed events\n    before switching to live updates. Respects RBAC visibility for flags.\n\n    Args:\n        client_id: Unique client identifier\n        queue: Event queue for this client\n        flag_filter: Optional list of flags to filter to\n        db: Database session\n        last_event_id: Last event ID received by client (for reconnection)\n        current_user: Authenticated user (for RBAC filtering)\n\n    Yields:\n        SSE-formatted event strings\n    \"\"\"\n    try:\n        version = get_current_version()\n\n        # Handle reconnection with Last-Event-ID\n        if last_event_id is not None:\n            logger.info(\n                f\"Client {client_id} reconnecting with Last-Event-ID: {last_event_id}\",\n                extra={\"current_version\": version},\n            )\n\n            # Track reconnection metrics\n            sse_reconnects_total.labels(with_last_event_id=\"true\").inc()\n            version_lag = version - last_event_id\n            if version_lag > 0:\n                sse_version_lag.observe(version_lag)\n\n            # Get missed events (now returns tuple with history_complete flag)\n            missed_events, history_complete = get_events_since(last_event_id, flag_filter)\n\n            if not history_complete:\n                # History is incomplete - fall back to bulk refresh with warning\n                logger.warning(f\"Client {client_id} event history incomplete, falling back to bulk refresh\")\n                sse_history_incomplete_total.inc()\n\n                # Send all current flags as bulk update (with RBAC filtering)\n                flags = await feature_flag_service.list_flags(db)\n                flags = await filter_flags_for_user(flags, current_user)\n                if flag_filter:\n                    flags_data = {f.name: f.to_dict() for f in flags if f.name in flag_filter}\n                else:\n                    flags_data = {f.name: f.to_dict() for f in flags}\n\n                # Send warning event before bulk update\n                warning_event = {\n                    \"event\": \"history_incomplete\",\n                    \"data\": {\n                        \"client_id\": client_id,\n                        \"message\": \"Event history was pruned. Sending bulk refresh.\",\n                        \"last_event_id\": last_event_id,\n                        \"current_version\": version,\n                        \"timestamp\": datetime.now(timezone.utc).isoformat() + \"Z\",\n                    },\n                }\n                yield format_sse_event(warning_event, version)\n\n                # Send bulk update\n                bulk_event = {\n                    \"event\": \"flags_bulk_update\",\n                    \"data\": {\n                        \"flags\": flags_data,\n                        \"version\": version,\n                        \"timestamp\": datetime.now(timezone.utc).isoformat() + \"Z\",\n                        \"reason\": \"history_incomplete\",\n                    },\n                }\n                yield format_sse_event(bulk_event, version)\n                update_client_last_event(client_id, version)\n\n            elif missed_events:\n                # Track replayed events\n                sse_events_replayed_total.inc(len(missed_events))\n\n                # Send missed events first\n                for event_data in missed_events:\n                    event_id = event_data.pop(\"_event_id\", None)\n                    flag_name = event_data.get(\"flag\")\n\n                    # Apply filter if set (already filtered for per-flag history)\n                    if flag_filter and flag_name and flag_name not in flag_filter:\n                        continue\n\n                    missed_event = {\n                        \"event\": \"flag_update\",\n                        \"data\": {\n                            \"flag\": flag_name,\n                            \"value\": event_data.get(\"data\"),\n                            \"version\": event_data.get(\"version\"),\n                            \"timestamp\": event_data.get(\"timestamp\"),\n                            \"replayed\": True,  # Mark as replayed event\n                        },\n                    }\n                    yield format_sse_event(missed_event, event_id)\n                    if event_id:\n                        update_client_last_event(client_id, event_id)\n\n                # Send reconnected event after replaying missed events\n                reconnected_event = {\n                    \"event\": \"reconnected\",\n                    \"data\": {\n                        \"client_id\": client_id,\n                        \"version\": version,\n                        \"events_replayed\": len(missed_events),\n                        \"history_complete\": True,\n                        \"timestamp\": datetime.now(timezone.utc).isoformat() + \"Z\",\n                    },\n                }\n                yield format_sse_event(reconnected_event, version)\n            else:\n                # No missed events, just send reconnected\n                reconnected_event = {\n                    \"event\": \"reconnected\",\n                    \"data\": {\n                        \"client_id\": client_id,\n                        \"version\": version,\n                        \"events_replayed\": 0,\n                        \"history_complete\": True,\n                        \"timestamp\": datetime.now(timezone.utc).isoformat() + \"Z\",\n                    },\n                }\n                yield format_sse_event(reconnected_event, version)\n        else:\n            # Fresh connection - send all current flags (with RBAC filtering)\n            flags = await feature_flag_service.list_flags(db)\n            flags = await filter_flags_for_user(flags, current_user)\n\n            # Filter flags if specific ones requested\n            if flag_filter:\n                flags_data = {f.name: f.to_dict() for f in flags if f.name in flag_filter}\n            else:\n                flags_data = {f.name: f.to_dict() for f in flags}\n\n            connected_event = {\n                \"event\": \"connected\",\n                \"data\": {\n                    \"client_id\": client_id,\n                    \"version\": version,\n                    \"flags\": flags_data,\n                    \"timestamp\": datetime.now(timezone.utc).isoformat() + \"Z\",\n                },\n            }\n            yield format_sse_event(connected_event, version)\n\n        # Update client's last event\n        update_client_last_event(client_id, version)\n\n        while True:\n            try:\n                # Wait for events with timeout for heartbeat\n                try:\n                    event = await asyncio.wait_for(queue.get(), timeout=HEARTBEAT_INTERVAL)\n                    event_version = event.get(\"data\", {}).get(\"version\", version)\n                    yield format_sse_event(event, event_version)\n                    update_client_last_event(client_id, event_version)\n                except asyncio.TimeoutError:\n                    # Send heartbeat with current version as ID\n                    current_version = get_current_version()\n                    heartbeat = {\n                        \"event\": \"heartbeat\",\n                        \"data\": {\n                            \"timestamp\": datetime.now(timezone.utc).isoformat() + \"Z\",\n                            \"version\": current_version,\n                        },\n                    }\n                    yield format_sse_event(heartbeat, current_version)\n\n            except asyncio.CancelledError:\n                break\n\n    except Exception as e:\n        logger.error(f\"SSE generator error for client {client_id}: {e}\")\n        error_event = {\n            \"event\": \"error\",\n            \"data\": {\n                \"message\": str(e),\n                \"timestamp\": datetime.now(timezone.utc).isoformat() + \"Z\",\n            },\n        }\n        yield format_sse_event(error_event)\n    finally:\n        await flag_subscription_manager.disconnect(client_id)\n\n\ndef format_sse_event(event: Dict[str, Any], event_id: Optional[int] = None) -> str:\n    \"\"\"Format an event dictionary as an SSE message.\n\n    Args:\n        event: Dictionary with 'event' and 'data' keys\n        event_id: Optional event ID for Last-Event-ID support\n\n    Returns:\n        SSE-formatted string with optional id field\n    \"\"\"\n    event_type = event.get(\"event\", \"message\")\n    data = json.dumps(event.get(\"data\", {}))\n\n    # Include event ID if provided (for Last-Event-ID support)\n    if event_id is not None:\n        return f\"id: {event_id}\\nevent: {event_type}\\ndata: {data}\\n\\n\"\n    else:\n        return f\"event: {event_type}\\ndata: {data}\\n\\n\"\n\n\n@router.get(\"/stream\")\nasync def stream_flag_updates(\n    request: Request,\n    flags: Optional[str] = Query(None, description=\"Comma-separated list of flag names to subscribe to\"),\n    db: Session = Depends(get_db),\n    current_user: Optional[User] = Depends(get_optional_current_user),\n):\n    \"\"\"Subscribe to real-time feature flag updates via Server-Sent Events.\n\n    Returns a stream of SSE events for flag changes. Clients can optionally\n    filter to specific flags using the `flags` query parameter.\n\n    Supports reconnection via Last-Event-ID header: when the client reconnects\n    with this header, the server will replay any missed events before switching\n    to live updates.\n\n    RBAC: Flags are filtered based on user permissions. Unauthenticated users\n    only receive public flags. Admin flags require admin role.\n\n    Rate Limiting: Max 10 connections per IP per minute.\n\n    Events:\n        - connected: Initial connection with current flag state\n        - reconnected: Reconnection after Last-Event-ID (includes replayed count)\n        - history_incomplete: Event history was pruned, includes bulk refresh\n        - flag_update: A single flag was updated\n        - flags_bulk_update: Multiple flags changed at once\n        - heartbeat: Keep-alive every 30 seconds\n\n    Headers:\n        Last-Event-ID: (optional) Last received event ID for reconnection\n        Authorization: (optional) Bearer token for authenticated access\n\n    Query Parameters:\n        flags: Comma-separated list of flag names (e.g., \"ui.dark_mode,backend.rag\")\n               If not provided, subscribes to all accessible flags.\n\n    Example:\n        ```javascript\n        const eventSource = new EventSource('/api/flags/stream?flags=ui.dark_mode');\n\n        eventSource.addEventListener('connected', (e) => {\n            const data = JSON.parse(e.data);\n            console.log('Connected with version:', data.version);\n        });\n\n        eventSource.addEventListener('reconnected', (e) => {\n            const data = JSON.parse(e.data);\n            console.log('Reconnected, replayed events:', data.events_replayed);\n        });\n\n        eventSource.addEventListener('flag_update', (e) => {\n            const data = JSON.parse(e.data);\n            console.log('Flag updated:', data.flag, data.value);\n        });\n        ```\n    \"\"\"\n    import uuid\n\n    # Rate limiting check\n    client_ip = get_client_ip(request)\n    is_allowed, current_count = sse_rate_limiter.check_rate_limit(client_ip)\n    if not is_allowed:\n        logger.warning(\n            f\"SSE rate limit exceeded for {client_ip}\",\n            extra={\"count\": current_count, \"limit\": SSE_RATE_LIMIT_MAX_CONNECTIONS},\n        )\n        return JSONResponse(\n            status_code=429,\n            content={\n                \"error\": \"Rate limit exceeded\",\n                \"message\": f\"Too many SSE connections from this IP. Max {SSE_RATE_LIMIT_MAX_CONNECTIONS} per minute.\",\n                \"retry_after\": SSE_RATE_LIMIT_WINDOW,\n            },\n            headers={\"Retry-After\": str(SSE_RATE_LIMIT_WINDOW)},\n        )\n\n    client_id = str(uuid.uuid4())\n    flag_filter = [f.strip() for f in flags.split(\",\")] if flags else None\n\n    # Validate that user can access requested flags (RBAC)\n    if flag_filter:\n        # Get flag metadata to check permissions\n        all_flags = await feature_flag_service.list_flags(db)\n        flag_metadata_map = {f.name: getattr(f, \"flag_metadata\", None) for f in all_flags}\n\n        # Filter to only flags user can access\n        accessible_flags = []\n        for flag_name in flag_filter:\n            metadata = flag_metadata_map.get(flag_name)\n            if user_can_access_flag(current_user, metadata):\n                accessible_flags.append(flag_name)\n            else:\n                logger.warning(\n                    f\"User denied access to flag: {flag_name}\",\n                    extra={\n                        \"user_id\": current_user.id if current_user else None,\n                        \"visibility\": get_flag_visibility(metadata),\n                    },\n                )\n\n        if not accessible_flags and flag_filter:\n            # Release rate limit slot since we're rejecting\n            sse_rate_limiter.release(client_ip)\n            # User requested flags but has access to none\n            return JSONResponse(\n                status_code=403,\n                content={\n                    \"error\": \"Access denied\",\n                    \"message\": \"You don't have permission to access the requested flags.\",\n                },\n            )\n\n        flag_filter = accessible_flags if accessible_flags else None\n\n    # Extract Last-Event-ID header for reconnection support\n    last_event_id = None\n    last_event_id_header = request.headers.get(\"Last-Event-ID\")\n    if last_event_id_header:\n        try:\n            last_event_id = int(last_event_id_header)\n            logger.info(f\"SSE reconnection with Last-Event-ID: {last_event_id}\")\n        except ValueError:\n            logger.warning(f\"Invalid Last-Event-ID header: {last_event_id_header}\")\n\n    # Register client\n    queue = await flag_subscription_manager.connect(client_id, flag_filter)\n\n    # Create event generator with rate limit cleanup on disconnect\n    async def event_generator_with_cleanup():\n        try:\n            async for event in event_generator(client_id, queue, flag_filter, db, last_event_id, current_user):\n                yield event\n        finally:\n            # Release rate limit slot when connection closes\n            sse_rate_limiter.release(client_ip)\n\n    return StreamingResponse(\n        event_generator_with_cleanup(),\n        media_type=\"text/event-stream\",\n        headers={\n            \"Cache-Control\": \"no-cache\",\n            \"Connection\": \"keep-alive\",\n            \"X-Accel-Buffering\": \"no\",  # Disable nginx buffering\n        },\n    )\n\n\n@router.get(\"/version\")\nasync def get_flags_version():\n    \"\"\"Get the current global feature flags version.\n\n    Used by clients to check if their cached flags are stale.\n    Returns version number and timestamp.\n    \"\"\"\n    version = get_current_version()\n    return success_response(\n        data={\n            \"version\": version,\n            \"timestamp\": datetime.now(timezone.utc).isoformat() + \"Z\",\n        },\n        version=\"2.0.0\",\n    )\n\n\n@router.get(\"/changes\")\nasync def get_flag_changes(\n    since_version: int = Query(0, description=\"Return changes since this version\"),\n    db: Session = Depends(get_db),\n):\n    \"\"\"Get flag changes since a specific version.\n\n    Used by clients to efficiently sync their local cache after reconnecting.\n\n    Args:\n        since_version: Version number to get changes since (0 for all flags)\n\n    Returns:\n        Current version and list of changed flags\n    \"\"\"\n    current_version = get_current_version()\n    flags = await feature_flag_service.list_flags(db)\n\n    # For now, return all flags if version mismatch\n    # A more sophisticated implementation would track changes per version\n    flags_data = {f.name: f.to_dict() for f in flags}\n\n    return success_response(\n        data={\n            \"version\": current_version,\n            \"since_version\": since_version,\n            \"flags\": flags_data,\n            \"timestamp\": datetime.now(timezone.utc).isoformat() + \"Z\",\n        },\n        version=\"2.0.0\",\n    )\n\n\n@router.get(\"/stats\")\nasync def get_realtime_stats():\n    \"\"\"Get real-time connection statistics.\n\n    Returns current number of SSE connections, version info, event history stats,\n    and version drift across clients.\n    \"\"\"\n    history_stats = get_history_stats()\n    version_drift = flag_subscription_manager.get_version_drift_stats()\n\n    return success_response(\n        data={\n            \"connections\": flag_subscription_manager.get_connection_count(),\n            \"version\": get_current_version(),\n            \"event_history\": history_stats,\n            \"version_drift\": version_drift,\n            \"timestamp\": datetime.now(timezone.utc).isoformat() + \"Z\",\n        },\n        version=\"2.0.0\",\n    )\n\n\n@router.get(\"/history-stats\")\nasync def get_event_history_stats():\n    \"\"\"Get detailed event history statistics.\n\n    Returns information about the event history storage including:\n    - Number of events in global history\n    - Oldest and newest event IDs\n    - TTL remaining on history key\n    - Configuration limits\n    \"\"\"\n    stats = get_history_stats()\n\n    return success_response(\n        data={\n            **stats,\n            \"timestamp\": datetime.now(timezone.utc).isoformat() + \"Z\",\n        },\n        version=\"2.0.0\",\n    )\n"
}
