{
  "path": "services/api-gateway/app/api/admin_medical.py",
  "language": "python",
  "size": 18765,
  "last_modified": "2025-12-04T11:26:45.570Z",
  "lines": 575,
  "content": "\"\"\"Admin Medical AI API endpoints (Sprint 4 - Enhanced Analytics).\n\nProvides admin endpoints for:\n- AI model usage metrics and cost tracking\n- Search analytics and statistics\n- Model routing configuration\n- Embedding database stats\n\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nimport logging\nfrom datetime import datetime, timezone\nfrom typing import Any, Dict, List, Optional\n\nfrom app.core.api_envelope import success_response\nfrom app.core.database import get_db, redis_client\nfrom app.core.dependencies import ensure_admin_privileges, get_current_admin_or_viewer, get_current_admin_user\nfrom app.models.user import User\nfrom fastapi import APIRouter, Depends, HTTPException, Query, Request\nfrom pydantic import BaseModel\nfrom sqlalchemy.orm import Session\n\nlogger = logging.getLogger(__name__)\n\nrouter = APIRouter(prefix=\"/api/admin/medical\", tags=[\"admin\", \"medical\"])\n\n# Redis keys for metrics caching\nREDIS_MODEL_METRICS_KEY = \"voiceassist:admin:model_metrics\"\nREDIS_SEARCH_STATS_KEY = \"voiceassist:admin:search_stats\"\nMETRICS_CACHE_TTL = 300  # 5 minutes\n\n\n# ============================================================================\n# Pydantic Models\n# ============================================================================\n\n\nclass ModelInfo(BaseModel):\n    \"\"\"Model information.\"\"\"\n\n    id: str\n    name: str\n    provider: str  # \"openai\", \"anthropic\", \"local\"\n    type: str  # \"chat\", \"embedding\", \"tts\", \"stt\"\n    enabled: bool\n    is_primary: bool\n    supports_phi: bool  # Whether it can handle PHI (local models only)\n    context_window: int\n    cost_per_1k_input: float\n    cost_per_1k_output: float\n\n\nclass ModelUsageMetrics(BaseModel):\n    \"\"\"Model usage metrics.\"\"\"\n\n    total_requests_24h: int\n    total_tokens_input_24h: int\n    total_tokens_output_24h: int\n    estimated_cost_24h: float\n    avg_latency_ms: float\n    p95_latency_ms: float\n    error_rate: float\n    cloud_requests: int\n    local_requests: int\n    cloud_percentage: float\n\n\nclass ModelBreakdown(BaseModel):\n    \"\"\"Per-model breakdown.\"\"\"\n\n    model_id: str\n    model_name: str\n    provider: str\n    requests: int\n    tokens_input: int\n    tokens_output: int\n    estimated_cost: float\n    avg_latency_ms: float\n\n\nclass SearchStats(BaseModel):\n    \"\"\"Search analytics statistics.\"\"\"\n\n    total_searches_24h: int\n    avg_latency_ms: float\n    p95_latency_ms: float\n    cache_hit_rate: float\n    top_queries: List[Dict[str, Any]]\n    search_types: Dict[str, int]  # semantic, keyword, hybrid\n    no_results_rate: float\n\n\nclass EmbeddingStats(BaseModel):\n    \"\"\"Embedding database statistics.\"\"\"\n\n    total_documents: int\n    total_chunks: int\n    total_embeddings: int\n    embedding_dimensions: int\n    index_size_mb: float\n    last_indexed_at: Optional[str]\n\n\nclass RoutingConfig(BaseModel):\n    \"\"\"Model routing configuration.\"\"\"\n\n    phi_detection_enabled: bool\n    phi_route_to_local: bool\n    default_chat_model: str\n    default_embedding_model: str\n    fallback_enabled: bool\n    fallback_model: Optional[str]\n\n\nclass RoutingConfigUpdate(BaseModel):\n    \"\"\"Update model for routing configuration.\"\"\"\n\n    phi_detection_enabled: Optional[bool] = None\n    phi_route_to_local: Optional[bool] = None\n    default_chat_model: Optional[str] = None\n    default_embedding_model: Optional[str] = None\n    fallback_enabled: Optional[bool] = None\n    fallback_model: Optional[str] = None\n\n\n# ============================================================================\n# Helper Functions\n# ============================================================================\n\n\ndef _get_model_metrics_from_redis() -> Dict[str, Any]:\n    \"\"\"Fetch model usage metrics from Redis counters.\"\"\"\n    try:\n        # Get metrics from Redis - these would be populated by the actual LLM client\n        metrics_key = \"voiceassist:metrics:model_usage\"\n        cached = redis_client.get(metrics_key)\n        if cached:\n            return json.loads(cached)\n    except Exception as e:\n        logger.warning(f\"Failed to get model metrics from Redis: {e}\")\n\n    # Return mock/default metrics if Redis fails\n    return {\n        \"total_requests_24h\": 0,\n        \"total_tokens_input_24h\": 0,\n        \"total_tokens_output_24h\": 0,\n        \"cloud_requests\": 0,\n        \"local_requests\": 0,\n        \"latencies_ms\": [],\n        \"errors\": 0,\n        \"by_model\": {},\n    }\n\n\ndef _calculate_cost(tokens_input: int, tokens_output: int, model: str) -> float:\n    \"\"\"Calculate estimated cost based on token usage.\"\"\"\n    # Cost per 1K tokens (approximate pricing)\n    pricing = {\n        \"gpt-4\": {\"input\": 0.03, \"output\": 0.06},\n        \"gpt-4-turbo\": {\"input\": 0.01, \"output\": 0.03},\n        \"gpt-3.5-turbo\": {\"input\": 0.0005, \"output\": 0.0015},\n        \"claude-3-opus\": {\"input\": 0.015, \"output\": 0.075},\n        \"claude-3-sonnet\": {\"input\": 0.003, \"output\": 0.015},\n        \"claude-3-haiku\": {\"input\": 0.00025, \"output\": 0.00125},\n        \"text-embedding-3-small\": {\"input\": 0.00002, \"output\": 0},\n        \"text-embedding-3-large\": {\"input\": 0.00013, \"output\": 0},\n        \"local\": {\"input\": 0, \"output\": 0},  # Local models have no API cost\n    }\n\n    model_pricing = pricing.get(model, pricing.get(\"gpt-4-turbo\"))\n    input_cost = (tokens_input / 1000) * model_pricing[\"input\"]\n    output_cost = (tokens_output / 1000) * model_pricing[\"output\"]\n    return round(input_cost + output_cost, 4)\n\n\ndef _get_search_stats_from_redis() -> Dict[str, Any]:\n    \"\"\"Fetch search statistics from Redis.\"\"\"\n    try:\n        stats_key = \"voiceassist:metrics:search_stats\"\n        cached = redis_client.get(stats_key)\n        if cached:\n            return json.loads(cached)\n    except Exception as e:\n        logger.warning(f\"Failed to get search stats from Redis: {e}\")\n\n    return {\n        \"total_searches_24h\": 0,\n        \"latencies_ms\": [],\n        \"cache_hits\": 0,\n        \"cache_misses\": 0,\n        \"search_types\": {\"semantic\": 0, \"keyword\": 0, \"hybrid\": 0},\n        \"no_results\": 0,\n        \"top_queries\": [],\n    }\n\n\n# ============================================================================\n# Model Management Endpoints\n# ============================================================================\n\n\n@router.get(\"/models\")\nasync def list_available_models(\n    request: Request,\n    current_admin_user: User = Depends(get_current_admin_or_viewer),\n) -> Dict:\n    \"\"\"List all available AI models with their configuration.\"\"\"\n\n    # Define available models (in production, this would come from configuration)\n    models = [\n        ModelInfo(\n            id=\"gpt-4-turbo\",\n            name=\"GPT-4 Turbo\",\n            provider=\"openai\",\n            type=\"chat\",\n            enabled=True,\n            is_primary=True,\n            supports_phi=False,\n            context_window=128000,\n            cost_per_1k_input=0.01,\n            cost_per_1k_output=0.03,\n        ),\n        ModelInfo(\n            id=\"gpt-3.5-turbo\",\n            name=\"GPT-3.5 Turbo\",\n            provider=\"openai\",\n            type=\"chat\",\n            enabled=True,\n            is_primary=False,\n            supports_phi=False,\n            context_window=16385,\n            cost_per_1k_input=0.0005,\n            cost_per_1k_output=0.0015,\n        ),\n        ModelInfo(\n            id=\"claude-3-sonnet\",\n            name=\"Claude 3 Sonnet\",\n            provider=\"anthropic\",\n            type=\"chat\",\n            enabled=True,\n            is_primary=False,\n            supports_phi=False,\n            context_window=200000,\n            cost_per_1k_input=0.003,\n            cost_per_1k_output=0.015,\n        ),\n        ModelInfo(\n            id=\"text-embedding-3-large\",\n            name=\"Text Embedding 3 Large\",\n            provider=\"openai\",\n            type=\"embedding\",\n            enabled=True,\n            is_primary=True,\n            supports_phi=False,\n            context_window=8191,\n            cost_per_1k_input=0.00013,\n            cost_per_1k_output=0,\n        ),\n        ModelInfo(\n            id=\"local-llama\",\n            name=\"Local Llama 3.1\",\n            provider=\"local\",\n            type=\"chat\",\n            enabled=True,\n            is_primary=False,\n            supports_phi=True,\n            context_window=8192,\n            cost_per_1k_input=0,\n            cost_per_1k_output=0,\n        ),\n    ]\n\n    trace_id = getattr(request.state, \"trace_id\", None)\n    return success_response(\n        data={\n            \"models\": [m.model_dump() for m in models],\n            \"total\": len(models),\n            \"timestamp\": datetime.now(timezone.utc).isoformat() + \"Z\",\n        },\n        trace_id=trace_id,\n    )\n\n\n@router.get(\"/models/{model_id}\")\nasync def get_model_details(\n    request: Request,\n    model_id: str,\n    current_admin_user: User = Depends(get_current_admin_or_viewer),\n) -> Dict:\n    \"\"\"Get detailed information about a specific model.\"\"\"\n\n    # In production, fetch from configuration store\n    # For now, return a structured response\n    trace_id = getattr(request.state, \"trace_id\", None)\n\n    model_details = {\n        \"id\": model_id,\n        \"name\": model_id.replace(\"-\", \" \").title(),\n        \"provider\": (\"openai\" if \"gpt\" in model_id else \"anthropic\" if \"claude\" in model_id else \"local\"),\n        \"type\": \"embedding\" if \"embedding\" in model_id else \"chat\",\n        \"enabled\": True,\n        \"configuration\": {\n            \"temperature\": 0.7,\n            \"max_tokens\": 4096,\n            \"top_p\": 1.0,\n        },\n        \"usage_24h\": {\n            \"requests\": 1250,\n            \"tokens_input\": 450000,\n            \"tokens_output\": 125000,\n            \"estimated_cost\": _calculate_cost(450000, 125000, model_id),\n        },\n        \"timestamp\": datetime.now(timezone.utc).isoformat() + \"Z\",\n    }\n\n    return success_response(data=model_details, trace_id=trace_id)\n\n\n# ============================================================================\n# Metrics Endpoints\n# ============================================================================\n\n\n@router.get(\"/metrics\")\nasync def get_model_usage_metrics(\n    request: Request,\n    current_admin_user: User = Depends(get_current_admin_or_viewer),\n    days: int = Query(1, ge=1, le=30),\n) -> Dict:\n    \"\"\"Get AI model usage metrics and cost tracking.\"\"\"\n\n    # Try to get from cache first\n    cache_key = f\"{REDIS_MODEL_METRICS_KEY}:{days}\"\n    try:\n        cached = redis_client.get(cache_key)\n        if cached:\n            trace_id = getattr(request.state, \"trace_id\", None)\n            return success_response(data=json.loads(cached), trace_id=trace_id)\n    except Exception:\n        pass\n\n    # Get raw metrics from Redis counters\n    raw_metrics = _get_model_metrics_from_redis()\n\n    total_requests = raw_metrics.get(\"total_requests_24h\", 0)\n    total_tokens_in = raw_metrics.get(\"total_tokens_input_24h\", 0)\n    total_tokens_out = raw_metrics.get(\"total_tokens_output_24h\", 0)\n    cloud_requests = raw_metrics.get(\"cloud_requests\", 0)\n    local_requests = raw_metrics.get(\"local_requests\", 0)\n    latencies = raw_metrics.get(\"latencies_ms\", [])\n    errors = raw_metrics.get(\"errors\", 0)\n\n    # Calculate aggregate metrics\n    avg_latency = sum(latencies) / len(latencies) if latencies else 0\n    sorted_latencies = sorted(latencies)\n    p95_latency = sorted_latencies[int(len(sorted_latencies) * 0.95)] if latencies else 0\n    error_rate = (errors / total_requests * 100) if total_requests > 0 else 0\n    cloud_percentage = (cloud_requests / total_requests * 100) if total_requests > 0 else 0\n\n    # Calculate estimated cost\n    estimated_cost = _calculate_cost(total_tokens_in, total_tokens_out, \"gpt-4-turbo\")\n\n    metrics = ModelUsageMetrics(\n        total_requests_24h=total_requests,\n        total_tokens_input_24h=total_tokens_in,\n        total_tokens_output_24h=total_tokens_out,\n        estimated_cost_24h=estimated_cost,\n        avg_latency_ms=round(avg_latency, 2),\n        p95_latency_ms=round(p95_latency, 2),\n        error_rate=round(error_rate, 2),\n        cloud_requests=cloud_requests,\n        local_requests=local_requests,\n        cloud_percentage=round(cloud_percentage, 1),\n    )\n\n    # Build per-model breakdown\n    by_model = raw_metrics.get(\"by_model\", {})\n    model_breakdown = []\n    for model_id, model_data in by_model.items():\n        model_breakdown.append(\n            ModelBreakdown(\n                model_id=model_id,\n                model_name=model_id.replace(\"-\", \" \").title(),\n                provider=(\"openai\" if \"gpt\" in model_id else \"anthropic\" if \"claude\" in model_id else \"local\"),\n                requests=model_data.get(\"requests\", 0),\n                tokens_input=model_data.get(\"tokens_input\", 0),\n                tokens_output=model_data.get(\"tokens_output\", 0),\n                estimated_cost=_calculate_cost(\n                    model_data.get(\"tokens_input\", 0),\n                    model_data.get(\"tokens_output\", 0),\n                    model_id,\n                ),\n                avg_latency_ms=model_data.get(\"avg_latency_ms\", 0),\n            ).model_dump()\n        )\n\n    response_data = {\n        **metrics.model_dump(),\n        \"model_breakdown\": model_breakdown,\n        \"period_days\": days,\n        \"timestamp\": datetime.now(timezone.utc).isoformat() + \"Z\",\n    }\n\n    # Cache the response\n    try:\n        redis_client.setex(cache_key, METRICS_CACHE_TTL, json.dumps(response_data))\n    except Exception:\n        pass\n\n    trace_id = getattr(request.state, \"trace_id\", None)\n    return success_response(data=response_data, trace_id=trace_id)\n\n\n@router.get(\"/search/stats\")\nasync def get_search_statistics(\n    request: Request,\n    current_admin_user: User = Depends(get_current_admin_or_viewer),\n    days: int = Query(1, ge=1, le=30),\n) -> Dict:\n    \"\"\"Get search analytics and statistics.\"\"\"\n\n    # Try cache first\n    cache_key = f\"{REDIS_SEARCH_STATS_KEY}:{days}\"\n    try:\n        cached = redis_client.get(cache_key)\n        if cached:\n            trace_id = getattr(request.state, \"trace_id\", None)\n            return success_response(data=json.loads(cached), trace_id=trace_id)\n    except Exception:\n        pass\n\n    # Get raw stats\n    raw_stats = _get_search_stats_from_redis()\n\n    total_searches = raw_stats.get(\"total_searches_24h\", 0)\n    latencies = raw_stats.get(\"latencies_ms\", [])\n    cache_hits = raw_stats.get(\"cache_hits\", 0)\n    cache_misses = raw_stats.get(\"cache_misses\", 0)\n    no_results = raw_stats.get(\"no_results\", 0)\n\n    avg_latency = sum(latencies) / len(latencies) if latencies else 0\n    sorted_latencies = sorted(latencies)\n    p95_latency = sorted_latencies[int(len(sorted_latencies) * 0.95)] if latencies else 0\n    total_cache_requests = cache_hits + cache_misses\n    cache_hit_rate = (cache_hits / total_cache_requests * 100) if total_cache_requests > 0 else 0\n    no_results_rate = (no_results / total_searches * 100) if total_searches > 0 else 0\n\n    stats = SearchStats(\n        total_searches_24h=total_searches,\n        avg_latency_ms=round(avg_latency, 2),\n        p95_latency_ms=round(p95_latency, 2),\n        cache_hit_rate=round(cache_hit_rate, 1),\n        top_queries=raw_stats.get(\"top_queries\", [])[:10],\n        search_types=raw_stats.get(\"search_types\", {\"semantic\": 0, \"keyword\": 0, \"hybrid\": 0}),\n        no_results_rate=round(no_results_rate, 1),\n    )\n\n    response_data = {\n        **stats.model_dump(),\n        \"period_days\": days,\n        \"timestamp\": datetime.now(timezone.utc).isoformat() + \"Z\",\n    }\n\n    # Cache response\n    try:\n        redis_client.setex(cache_key, METRICS_CACHE_TTL, json.dumps(response_data))\n    except Exception:\n        pass\n\n    trace_id = getattr(request.state, \"trace_id\", None)\n    return success_response(data=response_data, trace_id=trace_id)\n\n\n@router.get(\"/embeddings/stats\")\nasync def get_embedding_statistics(\n    request: Request,\n    current_admin_user: User = Depends(get_current_admin_or_viewer),\n) -> Dict:\n    \"\"\"Get embedding database statistics.\"\"\"\n\n    # In production, this would query the actual vector database\n    # For now, return structured mock data\n    stats = EmbeddingStats(\n        total_documents=1250,\n        total_chunks=45000,\n        total_embeddings=45000,\n        embedding_dimensions=3072,  # text-embedding-3-large dimensions\n        index_size_mb=524.5,\n        last_indexed_at=datetime.now(timezone.utc).isoformat() + \"Z\",\n    )\n\n    trace_id = getattr(request.state, \"trace_id\", None)\n    return success_response(\n        data={\n            **stats.model_dump(),\n            \"timestamp\": datetime.now(timezone.utc).isoformat() + \"Z\",\n        },\n        trace_id=trace_id,\n    )\n\n\n# ============================================================================\n# Routing Configuration Endpoints\n# ============================================================================\n\n\n@router.get(\"/routing\")\nasync def get_routing_config(\n    request: Request,\n    current_admin_user: User = Depends(get_current_admin_or_viewer),\n) -> Dict:\n    \"\"\"Get current model routing configuration.\"\"\"\n\n    # In production, fetch from config store\n    config = RoutingConfig(\n        phi_detection_enabled=True,\n        phi_route_to_local=True,\n        default_chat_model=\"gpt-4-turbo\",\n        default_embedding_model=\"text-embedding-3-large\",\n        fallback_enabled=True,\n        fallback_model=\"gpt-3.5-turbo\",\n    )\n\n    trace_id = getattr(request.state, \"trace_id\", None)\n    return success_response(\n        data={\n            **config.model_dump(),\n            \"timestamp\": datetime.now(timezone.utc).isoformat() + \"Z\",\n        },\n        trace_id=trace_id,\n    )\n\n\n@router.patch(\"/routing\")\nasync def update_routing_config(\n    request: Request,\n    config_update: RoutingConfigUpdate,\n    db: Session = Depends(get_db),\n    current_admin_user: User = Depends(get_current_admin_user),\n) -> Dict:\n    \"\"\"Update model routing configuration.\"\"\"\n    ensure_admin_privileges(current_admin_user)\n\n    # Validate PHI detection cannot be disabled in production\n    # This is a HIPAA compliance requirement\n    if config_update.phi_detection_enabled is False:\n        raise HTTPException(\n            status_code=400,\n            detail=\"PHI detection cannot be disabled (HIPAA requirement)\",\n        )\n\n    # In production, persist to config store and emit audit log\n    # For now, return the updated config\n    update_data = config_update.model_dump(exclude_unset=True)\n\n    logger.info(\n        \"routing_config_updated\",\n        extra={\n            \"admin_user_id\": str(current_admin_user.id),\n            \"updates\": update_data,\n        },\n    )\n\n    trace_id = getattr(request.state, \"trace_id\", None)\n    return success_response(\n        data={\n            \"message\": \"Routing configuration updated successfully\",\n            \"updates\": update_data,\n            \"timestamp\": datetime.now(timezone.utc).isoformat() + \"Z\",\n        },\n        trace_id=trace_id,\n    )\n"
}
