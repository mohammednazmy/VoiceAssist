{
  "path": "services/api-gateway/app/services/medical_embeddings.py",
  "language": "python",
  "size": 18909,
  "last_modified": "2025-12-04T23:44:49.029Z",
  "lines": 592,
  "content": "\"\"\"\nMedical Embeddings Service (Phase 5 - Advanced RAG)\n\nProvides medical-specific embeddings using domain-trained models.\n\nSupported Models:\n1. PubMedBERT - Pre-trained on PubMed abstracts\n2. BioGPT - Generative model for biomedical text\n3. BioBERT - BERT pre-trained on biomedical literature\n4. SciBERT - BERT pre-trained on scientific text\n\nFeatures:\n- Multiple embedding model support\n- Hybrid embeddings (combining OpenAI + medical)\n- Query-type based model selection\n- Batch embedding generation\n- Model caching and lazy loading\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport logging\nfrom dataclasses import dataclass\nfrom enum import Enum\nfrom typing import Any, Dict, List, Optional, Union\n\nimport numpy as np\nfrom app.core.config import settings\nfrom app.services.cache_service import cache_service, generate_cache_key\n\nlogger = logging.getLogger(__name__)\n\n\nclass MedicalModelType(str, Enum):\n    \"\"\"Available medical embedding models.\"\"\"\n\n    OPENAI = \"openai\"  # OpenAI text-embedding-3-small/large\n    PUBMEDBERT = \"pubmedbert\"  # microsoft/BiomedNLP-PubMedBERT\n    BIOBERT = \"biobert\"  # dmis-lab/biobert-base-cased-v1.2\n    SCIBERT = \"scibert\"  # allenai/scibert_scivocab_uncased\n    BIOGPT = \"biogpt\"  # microsoft/biogpt\n    MEDCPT = \"medcpt\"  # ncbi/MedCPT-Query-Encoder\n\n\nclass QueryType(str, Enum):\n    \"\"\"Query type for embedding strategy selection.\"\"\"\n\n    GENERAL = \"general\"  # General medical questions\n    CLINICAL = \"clinical\"  # Clinical decision support\n    RESEARCH = \"research\"  # Research/literature queries\n    DRUG = \"drug\"  # Drug-related queries\n    DIAGNOSIS = \"diagnosis\"  # Diagnostic queries\n\n\n@dataclass\nclass EmbeddingConfig:\n    \"\"\"Configuration for embedding generation.\"\"\"\n\n    model_type: MedicalModelType = MedicalModelType.OPENAI\n    device: str = \"cpu\"  # cuda, cpu, mps\n    batch_size: int = 32\n    max_length: int = 512\n    normalize: bool = True\n    cache_ttl: int = 86400  # 24 hours\n\n\n@dataclass\nclass EmbeddingResult:\n    \"\"\"Result from embedding generation.\"\"\"\n\n    embedding: List[float]\n    model: str\n    dimensions: int\n    tokens_used: Optional[int] = None\n\n\nclass OpenAIEmbeddings:\n    \"\"\"\n    OpenAI embeddings for general text.\n\n    Models:\n    - text-embedding-3-small (1536 dimensions, cheaper)\n    - text-embedding-3-large (3072 dimensions, higher quality)\n    - text-embedding-ada-002 (1536 dimensions, legacy)\n    \"\"\"\n\n    def __init__(\n        self,\n        model: str = \"text-embedding-3-small\",\n        dimensions: Optional[int] = None,\n    ):\n        self.model = model\n        self.dimensions = dimensions\n\n    async def embed(\n        self,\n        texts: Union[str, List[str]],\n    ) -> List[List[float]]:\n        \"\"\"Generate embeddings using OpenAI API.\"\"\"\n        import openai\n\n        if isinstance(texts, str):\n            texts = [texts]\n\n        try:\n            kwargs = {\"model\": self.model, \"input\": texts}\n            if self.dimensions:\n                kwargs[\"dimensions\"] = self.dimensions\n\n            response = await openai.embeddings.create(**kwargs)\n            return [data.embedding for data in response.data]\n\n        except Exception as e:\n            logger.error(f\"OpenAI embedding error: {e}\")\n            raise\n\n\nclass PubMedBERTEmbeddings:\n    \"\"\"\n    PubMedBERT embeddings for biomedical text.\n\n    Model: microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\n\n    Pre-trained on:\n    - PubMed abstracts\n    - PubMed Central full-text articles\n    \"\"\"\n\n    # Pinned model revision for reproducibility\n    DEFAULT_REVISION = \"v1.1\"\n\n    def __init__(\n        self,\n        model_name: str = \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\",\n        model_revision: str = \"v1.1\",\n        device: str = \"cpu\",\n        max_length: int = 512,\n    ):\n        self.model_name = model_name\n        self.model_revision = model_revision\n        self.device = device\n        self.max_length = max_length\n        self._model = None\n        self._tokenizer = None\n\n    async def _load_model(self):\n        \"\"\"Load model lazily.\"\"\"\n        if self._model is None:\n            try:\n                from transformers import AutoModel, AutoTokenizer\n\n                # nosec B615 - revision pinned via self.model_revision\n                self._tokenizer = AutoTokenizer.from_pretrained(  # nosec B615\n                    self.model_name, revision=self.model_revision\n                )\n                self._model = AutoModel.from_pretrained(self.model_name, revision=self.model_revision)  # nosec B615\n\n                # Move to device\n                if self.device != \"cpu\":\n                    import torch\n\n                    if self.device == \"cuda\" and torch.cuda.is_available():\n                        self._model = self._model.to(\"cuda\")\n                    elif self.device == \"mps\" and torch.backends.mps.is_available():\n                        self._model = self._model.to(\"mps\")\n\n                self._model.eval()\n                logger.info(f\"Loaded PubMedBERT model on {self.device}\")\n\n            except ImportError:\n                logger.error(\"transformers package not installed. \" \"Install with: pip install transformers torch\")\n                raise\n\n    async def embed(\n        self,\n        texts: Union[str, List[str]],\n    ) -> List[List[float]]:\n        \"\"\"Generate embeddings using PubMedBERT.\"\"\"\n        import torch\n\n        await self._load_model()\n\n        if isinstance(texts, str):\n            texts = [texts]\n\n        # Tokenize\n        inputs = self._tokenizer(\n            texts,\n            padding=True,\n            truncation=True,\n            max_length=self.max_length,\n            return_tensors=\"pt\",\n        )\n\n        # Move to device\n        if self.device != \"cpu\":\n            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n\n        # Generate embeddings\n        with torch.no_grad():\n            outputs = self._model(**inputs)\n            # Mean pooling over tokens (excluding padding)\n            attention_mask = inputs[\"attention_mask\"]\n            embeddings = outputs.last_hidden_state\n\n            # Apply attention mask\n            mask_expanded = attention_mask.unsqueeze(-1).expand(embeddings.size()).float()\n            sum_embeddings = torch.sum(embeddings * mask_expanded, 1)\n            sum_mask = torch.clamp(mask_expanded.sum(1), min=1e-9)\n            mean_embeddings = sum_embeddings / sum_mask\n\n            # Normalize\n            mean_embeddings = torch.nn.functional.normalize(mean_embeddings, p=2, dim=1)\n\n        return mean_embeddings.cpu().numpy().tolist()\n\n\nclass BioBERTEmbeddings:\n    \"\"\"\n    BioBERT embeddings for biomedical text.\n\n    Model: dmis-lab/biobert-base-cased-v1.2\n\n    Pre-trained on:\n    - PubMed abstracts\n    - PMC full-text articles\n    \"\"\"\n\n    # Pinned model revision for reproducibility\n    DEFAULT_REVISION = \"main\"\n\n    def __init__(\n        self,\n        model_name: str = \"dmis-lab/biobert-base-cased-v1.2\",\n        model_revision: str = \"main\",\n        device: str = \"cpu\",\n        max_length: int = 512,\n    ):\n        self.model_name = model_name\n        self.model_revision = model_revision\n        self.device = device\n        self.max_length = max_length\n        self._model = None\n        self._tokenizer = None\n\n    async def _load_model(self):\n        \"\"\"Load model lazily.\"\"\"\n        if self._model is None:\n            try:\n                from transformers import AutoModel, AutoTokenizer\n\n                # nosec B615 - revision pinned via self.model_revision\n                self._tokenizer = AutoTokenizer.from_pretrained(  # nosec B615\n                    self.model_name, revision=self.model_revision\n                )\n                self._model = AutoModel.from_pretrained(self.model_name, revision=self.model_revision)  # nosec B615\n\n                if self.device != \"cpu\":\n                    import torch\n\n                    if self.device == \"cuda\" and torch.cuda.is_available():\n                        self._model = self._model.to(\"cuda\")\n\n                self._model.eval()\n                logger.info(f\"Loaded BioBERT model on {self.device}\")\n\n            except ImportError:\n                logger.error(\"transformers package not installed\")\n                raise\n\n    async def embed(\n        self,\n        texts: Union[str, List[str]],\n    ) -> List[List[float]]:\n        \"\"\"Generate embeddings using BioBERT.\"\"\"\n        import torch\n\n        await self._load_model()\n\n        if isinstance(texts, str):\n            texts = [texts]\n\n        inputs = self._tokenizer(\n            texts,\n            padding=True,\n            truncation=True,\n            max_length=self.max_length,\n            return_tensors=\"pt\",\n        )\n\n        if self.device != \"cpu\":\n            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n\n        with torch.no_grad():\n            outputs = self._model(**inputs)\n            # CLS token embedding\n            cls_embeddings = outputs.last_hidden_state[:, 0, :]\n            cls_embeddings = torch.nn.functional.normalize(cls_embeddings, p=2, dim=1)\n\n        return cls_embeddings.cpu().numpy().tolist()\n\n\nclass HuggingFaceInferenceEmbeddings:\n    \"\"\"\n    Embeddings using Hugging Face Inference API.\n\n    Useful when you don't want to host models locally.\n    Requires HF_API_KEY environment variable.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_id: str = \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\",\n        api_key: Optional[str] = None,\n    ):\n        self.model_id = model_id\n        self.api_key = api_key or getattr(settings, \"HF_API_KEY\", None)\n        self.api_url = f\"https://api-inference.huggingface.co/pipeline/feature-extraction/{model_id}\"\n\n    async def embed(\n        self,\n        texts: Union[str, List[str]],\n    ) -> List[List[float]]:\n        \"\"\"Generate embeddings using HF Inference API.\"\"\"\n        import httpx\n\n        if isinstance(texts, str):\n            texts = [texts]\n\n        if not self.api_key:\n            raise ValueError(\"HF_API_KEY not configured\")\n\n        headers = {\"Authorization\": f\"Bearer {self.api_key}\"}\n\n        async with httpx.AsyncClient() as client:\n            response = await client.post(\n                self.api_url,\n                headers=headers,\n                json={\"inputs\": texts, \"options\": {\"wait_for_model\": True}},\n                timeout=30,\n            )\n            response.raise_for_status()\n            embeddings = response.json()\n\n            # HF returns [batch, seq_len, hidden_dim] - need to mean pool\n            result = []\n            for emb in embeddings:\n                if isinstance(emb[0], list):\n                    # Mean pooling\n                    arr = np.array(emb)\n                    mean_emb = np.mean(arr, axis=0).tolist()\n                    result.append(mean_emb)\n                else:\n                    result.append(emb)\n\n            return result\n\n\nclass MedicalEmbeddingService:\n    \"\"\"\n    Main medical embedding service.\n\n    Provides unified interface for generating embeddings\n    using different medical models.\n    \"\"\"\n\n    def __init__(\n        self,\n        config: Optional[EmbeddingConfig] = None,\n    ):\n        self.config = config or EmbeddingConfig()\n        self._embedders: Dict[MedicalModelType, Any] = {}\n\n    async def _get_embedder(self, model_type: MedicalModelType):\n        \"\"\"Get or create embedder for model type.\"\"\"\n        if model_type not in self._embedders:\n            if model_type == MedicalModelType.OPENAI:\n                self._embedders[model_type] = OpenAIEmbeddings()\n            elif model_type == MedicalModelType.PUBMEDBERT:\n                self._embedders[model_type] = PubMedBERTEmbeddings(\n                    device=self.config.device,\n                    max_length=self.config.max_length,\n                )\n            elif model_type == MedicalModelType.BIOBERT:\n                self._embedders[model_type] = BioBERTEmbeddings(\n                    device=self.config.device,\n                    max_length=self.config.max_length,\n                )\n            elif model_type == MedicalModelType.SCIBERT:\n                self._embedders[model_type] = BioBERTEmbeddings(\n                    model_name=\"allenai/scibert_scivocab_uncased\",\n                    device=self.config.device,\n                    max_length=self.config.max_length,\n                )\n            else:\n                raise ValueError(f\"Unknown model type: {model_type}\")\n\n        return self._embedders[model_type]\n\n    async def embed(\n        self,\n        texts: Union[str, List[str]],\n        model_type: Optional[MedicalModelType] = None,\n    ) -> List[EmbeddingResult]:\n        \"\"\"\n        Generate embeddings for texts.\n\n        Args:\n            texts: Text(s) to embed\n            model_type: Model to use (defaults to config)\n\n        Returns:\n            List of embedding results\n        \"\"\"\n        model = model_type or self.config.model_type\n\n        if isinstance(texts, str):\n            texts = [texts]\n\n        # Check cache\n        results = []\n        uncached_texts = []\n        uncached_indices = []\n\n        for i, text in enumerate(texts):\n            cache_key = generate_cache_key(\"medical_emb\", text, model=model.value)\n            cached = await cache_service.get(cache_key)\n            if cached is not None:\n                results.append((i, EmbeddingResult(**cached)))\n            else:\n                uncached_texts.append(text)\n                uncached_indices.append(i)\n\n        if not uncached_texts:\n            # All cached\n            results.sort(key=lambda x: x[0])\n            return [r[1] for r in results]\n\n        # Generate embeddings for uncached texts\n        try:\n            embedder = await self._get_embedder(model)\n            embeddings = await embedder.embed(uncached_texts)\n\n            # Cache and collect results\n            for j, (idx, text) in enumerate(zip(uncached_indices, uncached_texts)):\n                emb_result = EmbeddingResult(\n                    embedding=embeddings[j],\n                    model=model.value,\n                    dimensions=len(embeddings[j]),\n                )\n\n                # Cache\n                cache_key = generate_cache_key(\"medical_emb\", text, model=model.value)\n                await cache_service.set(\n                    cache_key,\n                    {\n                        \"embedding\": emb_result.embedding,\n                        \"model\": emb_result.model,\n                        \"dimensions\": emb_result.dimensions,\n                    },\n                    ttl=self.config.cache_ttl,\n                )\n\n                results.append((idx, emb_result))\n\n            # Sort by original index\n            results.sort(key=lambda x: x[0])\n            return [r[1] for r in results]\n\n        except Exception as e:\n            logger.error(f\"Embedding generation failed: {e}\", exc_info=True)\n            raise\n\n    async def embed_query(\n        self,\n        query: str,\n        query_type: QueryType = QueryType.GENERAL,\n    ) -> EmbeddingResult:\n        \"\"\"\n        Embed a query with query-specific processing.\n\n        Adds query prefix based on query type for better retrieval.\n        \"\"\"\n        # Add query-specific prefix\n        prefixes = {\n            QueryType.GENERAL: \"medical query: \",\n            QueryType.CLINICAL: \"clinical question: \",\n            QueryType.RESEARCH: \"research query: \",\n            QueryType.DRUG: \"drug information query: \",\n            QueryType.DIAGNOSIS: \"diagnostic query: \",\n        }\n\n        prefix = prefixes.get(query_type, \"\")\n        processed_query = prefix + query\n\n        results = await self.embed(processed_query)\n        return results[0]\n\n\nclass HybridEmbeddingService:\n    \"\"\"\n    Hybrid embedding service combining multiple models.\n\n    Strategy:\n    1. Generate embeddings from multiple models\n    2. Weight and combine based on query type\n    3. Optionally project to common dimension\n    \"\"\"\n\n    def __init__(\n        self,\n        openai_weight: float = 0.4,\n        medical_weight: float = 0.6,\n        device: str = \"cpu\",\n    ):\n        self.openai_weight = openai_weight\n        self.medical_weight = medical_weight\n\n        self.openai_embeddings = OpenAIEmbeddings()\n        self.medical_service = MedicalEmbeddingService(\n            config=EmbeddingConfig(\n                model_type=MedicalModelType.PUBMEDBERT,\n                device=device,\n            )\n        )\n\n    async def embed(\n        self,\n        text: str,\n        query_type: QueryType = QueryType.GENERAL,\n    ) -> Dict[str, Any]:\n        \"\"\"\n        Generate hybrid embedding.\n\n        Returns both individual and combined embeddings.\n        \"\"\"\n        # Adjust weights based on query type\n        weights = self._get_weights(query_type)\n\n        # Generate embeddings in parallel\n        openai_task = self.openai_embeddings.embed(text)\n        medical_task = self.medical_service.embed(text)\n\n        openai_emb, medical_emb = await asyncio.gather(openai_task, medical_task, return_exceptions=True)\n\n        result = {\n            \"query_type\": query_type.value,\n            \"weights\": weights,\n        }\n\n        # Handle potential failures gracefully\n        if isinstance(openai_emb, Exception):\n            logger.error(f\"OpenAI embedding failed: {openai_emb}\")\n            openai_emb = None\n        else:\n            openai_emb = openai_emb[0]\n            result[\"openai_embedding\"] = openai_emb\n            result[\"openai_dimensions\"] = len(openai_emb)\n\n        if isinstance(medical_emb, Exception):\n            logger.error(f\"Medical embedding failed: {medical_emb}\")\n            medical_emb = None\n        else:\n            medical_emb = medical_emb[0].embedding\n            result[\"medical_embedding\"] = medical_emb\n            result[\"medical_dimensions\"] = len(medical_emb)\n\n        # Create combined embedding if both available\n        # Note: This requires projection for different dimensions\n        if openai_emb and medical_emb:\n            # For different dimensions, we'd need a projection layer\n            # For now, return both separately\n            result[\"combined_available\"] = True\n        else:\n            result[\"combined_available\"] = False\n            # Use whichever is available\n            result[\"fallback_embedding\"] = openai_emb or medical_emb\n\n        return result\n\n    def _get_weights(self, query_type: QueryType) -> Dict[str, float]:\n        \"\"\"Get embedding weights based on query type.\"\"\"\n        weight_configs = {\n            QueryType.GENERAL: {\"openai\": 0.6, \"medical\": 0.4},\n            QueryType.CLINICAL: {\"openai\": 0.3, \"medical\": 0.7},\n            QueryType.RESEARCH: {\"openai\": 0.2, \"medical\": 0.8},\n            QueryType.DRUG: {\"openai\": 0.4, \"medical\": 0.6},\n            QueryType.DIAGNOSIS: {\"openai\": 0.3, \"medical\": 0.7},\n        }\n        return weight_configs.get(query_type, {\"openai\": 0.5, \"medical\": 0.5})\n"
}
