{
  "path": "services/api-gateway/app/services/barge_in_classifier.py",
  "language": "python",
  "size": 32481,
  "last_modified": "2025-12-04T11:26:56.372Z",
  "lines": 859,
  "content": "\"\"\"\nBarge-In Classification Service\n\nServer-side classification for barge-in events. Used when the client defers\ncomplex classification decisions to the backend, or for analytics and ML training.\n\nFeatures:\n- Multilingual backchannel detection (12 languages)\n- Intent classification from transcript\n- Escalation detection for repeated backchannels\n- Confidence scoring with context awareness\n- Integration with voice pipeline\n\nPhase 3: Context-Aware Interruption Intelligence\n\"\"\"\n\nfrom __future__ import annotations\n\nimport time\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Literal, Optional, Tuple\n\nfrom app.core.logging import get_logger\n\nlogger = get_logger(__name__)\n\n\n# ============================================================================\n# Type Definitions\n# ============================================================================\n\nSupportedLanguage = Literal[\"en\", \"ar\", \"es\", \"fr\", \"de\", \"zh\", \"ja\", \"ko\", \"pt\", \"ru\", \"hi\", \"tr\"]\n\nBargeInClassificationType = Literal[\n    \"backchannel\",  # Non-interrupting acknowledgment\n    \"soft_barge\",  # Polite interruption\n    \"hard_barge\",  # Urgent interruption\n    \"command\",  # Direct command\n    \"correction\",  # Correcting something\n    \"clarification\",  # Asking for clarification\n    \"topic_change\",  # Changing subject\n    \"unknown\",  # Could not classify\n]\n\nUserIntent = Literal[\n    \"acknowledge\",  # Just acknowledging\n    \"continue\",  # Wants AI to continue\n    \"stop\",  # Wants AI to stop\n    \"pause\",  # Wants AI to pause\n    \"ask_question\",  # Has a question\n    \"provide_info\",  # Wants to provide info\n    \"correct\",  # Correcting the AI\n    \"change_topic\",  # Changing subject\n    \"clarify\",  # Asking for clarification\n    \"command\",  # Issuing a command\n    \"uncertain\",  # Intent unclear\n]\n\nInterruptionPriority = Literal[\"low\", \"medium\", \"high\", \"critical\"]\n\n\n# ============================================================================\n# Pattern Definitions\n# ============================================================================\n\n\n@dataclass\nclass BackchannelPattern:\n    \"\"\"Pattern for detecting backchannel utterances\"\"\"\n\n    phrases: List[str]\n    max_duration_ms: int\n    confidence: float = 1.0\n\n\n@dataclass\nclass SoftBargePattern:\n    \"\"\"Pattern for detecting soft barge-in phrases\"\"\"\n\n    phrases: List[str]\n    requires_follow_up: bool\n\n\n@dataclass\nclass HardBargePattern:\n    \"\"\"Pattern for detecting hard barge-in phrases\"\"\"\n\n    phrases: List[str]\n    intent: UserIntent\n\n\n@dataclass\nclass CommandPattern:\n    \"\"\"Pattern for detecting command phrases\"\"\"\n\n    phrases: List[str]\n    command_type: str\n    priority: InterruptionPriority\n\n\n# Multilingual backchannel patterns\nBACKCHANNEL_PATTERNS: Dict[SupportedLanguage, List[BackchannelPattern]] = {\n    \"en\": [\n        BackchannelPattern([\"uh huh\", \"uh-huh\", \"uhuh\", \"mm hmm\", \"mmhmm\", \"mhm\"], 600),\n        BackchannelPattern([\"yeah\", \"yep\", \"yes\", \"yea\", \"ya\"], 400),\n        BackchannelPattern([\"okay\", \"ok\", \"k\", \"kay\"], 400),\n        BackchannelPattern([\"right\", \"right right\"], 500),\n        BackchannelPattern([\"sure\", \"got it\", \"gotcha\"], 500),\n        BackchannelPattern([\"i see\", \"interesting\", \"cool\"], 600),\n    ],\n    \"ar\": [\n        BackchannelPattern([\"نعم\", \"اه\", \"اها\", \"ايوه\", \"ايه\"], 500),\n        BackchannelPattern([\"صح\", \"صحيح\", \"تمام\", \"ماشي\"], 500),\n        BackchannelPattern([\"طيب\", \"حسنا\", \"اوكي\"], 400),\n        BackchannelPattern([\"فاهم\", \"مفهوم\"], 600),\n    ],\n    \"es\": [\n        BackchannelPattern([\"sí\", \"si\", \"ajá\", \"aha\"], 400),\n        BackchannelPattern([\"vale\", \"ok\", \"bueno\"], 400),\n        BackchannelPattern([\"claro\", \"entiendo\", \"ya\"], 500),\n        BackchannelPattern([\"mmm\", \"mhm\"], 400),\n    ],\n    \"fr\": [\n        BackchannelPattern([\"oui\", \"ouais\", \"mouais\"], 400),\n        BackchannelPattern([\"d'accord\", \"ok\", \"entendu\"], 500),\n        BackchannelPattern([\"je vois\", \"ah bon\", \"mmm\"], 600),\n        BackchannelPattern([\"bien\", \"super\", \"parfait\"], 500),\n    ],\n    \"de\": [\n        BackchannelPattern([\"ja\", \"jap\", \"jo\"], 400),\n        BackchannelPattern([\"okay\", \"ok\", \"gut\"], 400),\n        BackchannelPattern([\"genau\", \"richtig\", \"stimmt\"], 500),\n        BackchannelPattern([\"verstehe\", \"aha\", \"mmm\"], 600),\n    ],\n    \"zh\": [\n        BackchannelPattern([\"嗯\", \"哦\", \"啊\"], 400),\n        BackchannelPattern([\"是\", \"对\", \"好\"], 400),\n        BackchannelPattern([\"明白\", \"了解\", \"知道\"], 600),\n        BackchannelPattern([\"没问题\", \"可以\"], 600),\n    ],\n    \"ja\": [\n        BackchannelPattern([\"はい\", \"うん\", \"ええ\"], 400),\n        BackchannelPattern([\"そうですね\", \"なるほど\"], 700),\n        BackchannelPattern([\"分かりました\", \"了解\"], 800),\n    ],\n    \"ko\": [\n        BackchannelPattern([\"네\", \"응\", \"예\"], 400),\n        BackchannelPattern([\"그래요\", \"맞아요\", \"알겠어요\"], 600),\n        BackchannelPattern([\"좋아요\", \"오케이\"], 500),\n    ],\n    \"pt\": [\n        BackchannelPattern([\"sim\", \"é\", \"ahã\"], 400),\n        BackchannelPattern([\"ok\", \"tá\", \"certo\"], 400),\n        BackchannelPattern([\"entendi\", \"compreendo\", \"sei\"], 600),\n    ],\n    \"ru\": [\n        BackchannelPattern([\"да\", \"ага\", \"угу\"], 400),\n        BackchannelPattern([\"понятно\", \"ясно\", \"хорошо\"], 600),\n        BackchannelPattern([\"ладно\", \"окей\", \"ок\"], 400),\n    ],\n    \"hi\": [\n        BackchannelPattern([\"हाँ\", \"जी\", \"अच्छा\"], 400),\n        BackchannelPattern([\"ठीक है\", \"समझ गया\", \"सही\"], 600),\n        BackchannelPattern([\"हम्म\", \"ओके\"], 400),\n    ],\n    \"tr\": [\n        BackchannelPattern([\"evet\", \"hı hı\", \"tamam\"], 400),\n        BackchannelPattern([\"anladım\", \"peki\", \"oldu\"], 600),\n        BackchannelPattern([\"doğru\", \"iyi\", \"güzel\"], 500),\n    ],\n}\n\n# Soft barge patterns\nSOFT_BARGE_PATTERNS: Dict[SupportedLanguage, List[SoftBargePattern]] = {\n    \"en\": [\n        SoftBargePattern([\"wait\", \"hold on\", \"hang on\", \"one moment\"], True),\n        SoftBargePattern([\"actually\", \"but\", \"well\", \"um\"], True),\n        SoftBargePattern([\"let me\", \"can i\", \"i want to\"], True),\n    ],\n    \"ar\": [\n        SoftBargePattern([\"انتظر\", \"لحظة\", \"ثانية\"], True),\n        SoftBargePattern([\"بس\", \"لكن\", \"في الحقيقة\"], True),\n    ],\n    \"es\": [\n        SoftBargePattern([\"espera\", \"un momento\", \"para\"], True),\n        SoftBargePattern([\"pero\", \"en realidad\", \"bueno\"], True),\n    ],\n    \"fr\": [\n        SoftBargePattern([\"attends\", \"un moment\", \"une seconde\"], True),\n        SoftBargePattern([\"mais\", \"en fait\", \"euh\"], True),\n    ],\n    \"de\": [\n        SoftBargePattern([\"warte\", \"moment\", \"einen augenblick\"], True),\n        SoftBargePattern([\"aber\", \"eigentlich\", \"also\"], True),\n    ],\n    \"zh\": [\n        SoftBargePattern([\"等一下\", \"等等\", \"稍等\"], True),\n        SoftBargePattern([\"但是\", \"其实\", \"不过\"], True),\n    ],\n    \"ja\": [\n        SoftBargePattern([\"ちょっと待って\", \"待って\", \"少々\"], True),\n        SoftBargePattern([\"でも\", \"実は\", \"あの\"], True),\n    ],\n    \"ko\": [\n        SoftBargePattern([\"잠깐만\", \"잠시만요\", \"기다려\"], True),\n        SoftBargePattern([\"그런데\", \"사실은\", \"근데\"], True),\n    ],\n    \"pt\": [\n        SoftBargePattern([\"espera\", \"um momento\", \"peraí\"], True),\n        SoftBargePattern([\"mas\", \"na verdade\", \"bom\"], True),\n    ],\n    \"ru\": [\n        SoftBargePattern([\"подожди\", \"секунду\", \"минутку\"], True),\n        SoftBargePattern([\"но\", \"на самом деле\", \"вообще-то\"], True),\n    ],\n    \"hi\": [\n        SoftBargePattern([\"रुको\", \"एक मिनट\", \"ज़रा\"], True),\n        SoftBargePattern([\"लेकिन\", \"असल में\", \"वैसे\"], True),\n    ],\n    \"tr\": [\n        SoftBargePattern([\"bekle\", \"bir dakika\", \"dur\"], True),\n        SoftBargePattern([\"ama\", \"aslında\", \"şey\"], True),\n    ],\n}\n\n# Hard barge patterns (urgent interruptions)\nHARD_BARGE_PATTERNS: Dict[SupportedLanguage, List[HardBargePattern]] = {\n    \"en\": [\n        HardBargePattern([\"stop\", \"stop it\", \"stop talking\"], \"stop\"),\n        HardBargePattern([\"enough\", \"that's enough\"], \"stop\"),\n        HardBargePattern([\"quiet\", \"be quiet\", \"shush\"], \"stop\"),\n        HardBargePattern([\"no no no\", \"no wait\", \"hold up\"], \"stop\"),\n        HardBargePattern([\"listen\", \"listen to me\", \"hear me out\"], \"provide_info\"),\n    ],\n    \"ar\": [\n        HardBargePattern([\"توقف\", \"كفى\", \"بس\"], \"stop\"),\n        HardBargePattern([\"اسمع\", \"استمع لي\"], \"provide_info\"),\n    ],\n    \"es\": [\n        HardBargePattern([\"para\", \"basta\", \"detente\"], \"stop\"),\n        HardBargePattern([\"escucha\", \"escúchame\"], \"provide_info\"),\n    ],\n    \"fr\": [\n        HardBargePattern([\"arrête\", \"stop\", \"suffit\"], \"stop\"),\n        HardBargePattern([\"écoute\", \"écoute-moi\"], \"provide_info\"),\n    ],\n    \"de\": [\n        HardBargePattern([\"stopp\", \"halt\", \"genug\"], \"stop\"),\n        HardBargePattern([\"hör zu\", \"hör mir zu\"], \"provide_info\"),\n    ],\n    \"zh\": [\n        HardBargePattern([\"停\", \"停下\", \"别说了\"], \"stop\"),\n        HardBargePattern([\"听我说\", \"你听我说\"], \"provide_info\"),\n    ],\n    \"ja\": [\n        HardBargePattern([\"止まって\", \"ストップ\", \"やめて\"], \"stop\"),\n        HardBargePattern([\"聞いて\", \"聞いてください\"], \"provide_info\"),\n    ],\n    \"ko\": [\n        HardBargePattern([\"멈춰\", \"그만\", \"스톱\"], \"stop\"),\n        HardBargePattern([\"들어봐\", \"내 말 들어\"], \"provide_info\"),\n    ],\n    \"pt\": [\n        HardBargePattern([\"para\", \"pare\", \"chega\"], \"stop\"),\n        HardBargePattern([\"escuta\", \"me escuta\"], \"provide_info\"),\n    ],\n    \"ru\": [\n        HardBargePattern([\"стоп\", \"хватит\", \"остановись\"], \"stop\"),\n        HardBargePattern([\"послушай\", \"слушай меня\"], \"provide_info\"),\n    ],\n    \"hi\": [\n        HardBargePattern([\"रुको\", \"बस\", \"बंद करो\"], \"stop\"),\n        HardBargePattern([\"सुनो\", \"मेरी सुनो\"], \"provide_info\"),\n    ],\n    \"tr\": [\n        HardBargePattern([\"dur\", \"yeter\", \"kes\"], \"stop\"),\n        HardBargePattern([\"dinle\", \"beni dinle\"], \"provide_info\"),\n    ],\n}\n\n# Command patterns\nCOMMAND_PATTERNS: Dict[SupportedLanguage, List[CommandPattern]] = {\n    \"en\": [\n        CommandPattern([\"stop\", \"stop talking\", \"be quiet\"], \"stop\", \"critical\"),\n        CommandPattern([\"repeat\", \"say again\", \"what did you say\"], \"repeat\", \"high\"),\n        CommandPattern([\"louder\", \"speak up\", \"volume up\"], \"volume_up\", \"medium\"),\n        CommandPattern([\"quieter\", \"speak softer\", \"volume down\"], \"volume_down\", \"medium\"),\n        CommandPattern([\"slower\", \"speak slower\", \"slow down\"], \"speed_down\", \"medium\"),\n        CommandPattern([\"faster\", \"speak faster\", \"speed up\"], \"speed_up\", \"medium\"),\n    ],\n    \"ar\": [\n        CommandPattern([\"توقف\", \"اسكت\", \"كفى\"], \"stop\", \"critical\"),\n        CommandPattern([\"أعد\", \"كرر\", \"ماذا قلت\"], \"repeat\", \"high\"),\n        CommandPattern([\"أعلى\", \"ارفع الصوت\"], \"volume_up\", \"medium\"),\n        CommandPattern([\"أخفض\", \"خفض الصوت\"], \"volume_down\", \"medium\"),\n    ],\n}\n\n# Correction phrases\nCORRECTION_PHRASES: Dict[SupportedLanguage, List[str]] = {\n    \"en\": [\"no\", \"that's wrong\", \"incorrect\", \"not right\", \"actually\", \"you're wrong\"],\n    \"ar\": [\"لا\", \"خطأ\", \"مش صحيح\", \"غلط\", \"في الحقيقة\"],\n    \"es\": [\"no\", \"eso está mal\", \"incorrecto\", \"en realidad\"],\n    \"fr\": [\"non\", \"c'est faux\", \"incorrect\", \"en fait\"],\n    \"de\": [\"nein\", \"das ist falsch\", \"nicht richtig\", \"eigentlich\"],\n    \"zh\": [\"不\", \"不对\", \"错了\", \"其实\"],\n    \"ja\": [\"いいえ\", \"違う\", \"間違い\", \"実は\"],\n    \"ko\": [\"아니\", \"틀려\", \"아니야\", \"사실은\"],\n    \"pt\": [\"não\", \"está errado\", \"incorreto\", \"na verdade\"],\n    \"ru\": [\"нет\", \"это неправильно\", \"неверно\", \"на самом деле\"],\n    \"hi\": [\"नहीं\", \"गलत\", \"सही नहीं\", \"असल में\"],\n    \"tr\": [\"hayır\", \"yanlış\", \"doğru değil\", \"aslında\"],\n}\n\n# Clarification phrases\nCLARIFICATION_PHRASES: Dict[SupportedLanguage, List[str]] = {\n    \"en\": [\n        \"what do you mean\",\n        \"can you explain\",\n        \"i don't understand\",\n        \"what\",\n        \"huh\",\n        \"sorry\",\n        \"pardon\",\n    ],\n    \"ar\": [\"ماذا تعني\", \"ممكن تشرح\", \"لا أفهم\", \"ماذا\", \"عفوا\"],\n    \"es\": [\"qué quieres decir\", \"puedes explicar\", \"no entiendo\", \"qué\", \"perdón\"],\n    \"fr\": [\n        \"qu'est-ce que tu veux dire\",\n        \"peux-tu expliquer\",\n        \"je ne comprends pas\",\n        \"quoi\",\n        \"pardon\",\n    ],\n    \"de\": [\n        \"was meinst du\",\n        \"kannst du erklären\",\n        \"ich verstehe nicht\",\n        \"was\",\n        \"entschuldigung\",\n    ],\n    \"zh\": [\"什么意思\", \"能解释一下\", \"我不明白\", \"什么\", \"对不起\"],\n    \"ja\": [\"どういう意味\", \"説明して\", \"わかりません\", \"何\", \"すみません\"],\n    \"ko\": [\"무슨 말이야\", \"설명해줘\", \"이해가 안 돼\", \"뭐\", \"미안\"],\n    \"pt\": [\n        \"o que você quer dizer\",\n        \"pode explicar\",\n        \"não entendo\",\n        \"o quê\",\n        \"desculpe\",\n    ],\n    \"ru\": [\"что ты имеешь в виду\", \"можешь объяснить\", \"не понимаю\", \"что\", \"извините\"],\n    \"hi\": [\"इसका मतलब क्या है\", \"समझाओ\", \"समझ नहीं आया\", \"क्या\", \"माफ़ करो\"],\n    \"tr\": [\"ne demek istiyorsun\", \"açıklar mısın\", \"anlamadım\", \"ne\", \"pardon\"],\n}\n\n\n# ============================================================================\n# Result Types\n# ============================================================================\n\n\n@dataclass\nclass BackchannelResult:\n    \"\"\"Result from backchannel detection\"\"\"\n\n    is_backchannel: bool\n    matched_pattern: Optional[str] = None\n    score: float = 0.0\n    language: SupportedLanguage = \"en\"\n    should_escalate: bool = False\n\n\n@dataclass\nclass SoftBargeResult:\n    \"\"\"Result from soft barge detection\"\"\"\n\n    is_soft_barge: bool\n    matched_pattern: Optional[str] = None\n    requires_follow_up: bool = False\n    language: SupportedLanguage = \"en\"\n\n\n@dataclass\nclass ClassificationAction:\n    \"\"\"Recommended action based on classification\"\"\"\n\n    type: Literal[\"continue\", \"pause\", \"stop\", \"acknowledge\", \"yield\", \"respond\", \"wait\"]\n    should_acknowledge: bool = False\n    acknowledgment_phrase: Optional[str] = None\n    pause_duration_ms: Optional[int] = None\n    should_save_context: bool = False\n\n\n@dataclass\nclass ClassificationMetadata:\n    \"\"\"Additional metadata about the classification\"\"\"\n\n    vad_probability: float = 0.0\n    during_ai_speech: bool = False\n    time_since_last_utterance_ms: int = 0\n    recent_backchannel_count: int = 0\n    prosodic_urgency: bool = False\n\n\n@dataclass\nclass ClassificationResult:\n    \"\"\"Complete classification result\"\"\"\n\n    classification: BargeInClassificationType\n    intent: UserIntent\n    priority: InterruptionPriority\n    confidence: float\n    language: SupportedLanguage\n    transcript: str\n    duration_ms: int\n    action: ClassificationAction\n    metadata: ClassificationMetadata\n\n\n# ============================================================================\n# Barge-In Classifier\n# ============================================================================\n\n\nclass BargeInClassifier:\n    \"\"\"\n    Server-side barge-in classifier for voice sessions.\n\n    Provides multilingual classification of user interruptions during AI speech.\n    Can be used for:\n    - Complex classification decisions deferred from client\n    - Analytics and ML training data collection\n    - Cross-session learning and personalization\n    \"\"\"\n\n    def __init__(\n        self,\n        language: SupportedLanguage = \"en\",\n        escalation_window_ms: int = 5000,\n        escalation_threshold: int = 3,\n        max_backchannel_duration_ms: int = 800,\n        min_confidence: float = 0.6,\n    ):\n        self.language = language\n        self.escalation_window_ms = escalation_window_ms\n        self.escalation_threshold = escalation_threshold\n        self.max_backchannel_duration_ms = max_backchannel_duration_ms\n        self.min_confidence = min_confidence\n\n        # Escalation tracking\n        self._recent_detections: Dict[str, List[float]] = {}\n\n        # Classification history for analytics\n        self._classification_history: List[ClassificationResult] = []\n        self._max_history_size = 100\n\n        logger.debug(\n            \"BargeInClassifier initialized\",\n            extra={\n                \"language\": language,\n                \"escalation_threshold\": escalation_threshold,\n            },\n        )\n\n    def set_language(self, language: SupportedLanguage) -> None:\n        \"\"\"Update the classification language\"\"\"\n        self.language = language\n        logger.debug(f\"BargeInClassifier language set to {language}\")\n\n    def classify(\n        self,\n        transcript: str,\n        duration_ms: int,\n        vad_probability: float,\n        during_ai_speech: bool,\n        time_since_last_utterance_ms: int = 0,\n    ) -> ClassificationResult:\n        \"\"\"\n        Classify a barge-in event.\n\n        Args:\n            transcript: The user's transcribed speech\n            duration_ms: Duration of the utterance in milliseconds\n            vad_probability: Voice activity detection confidence (0-1)\n            during_ai_speech: Whether the AI was speaking when this occurred\n            time_since_last_utterance_ms: Time since user's last utterance\n\n        Returns:\n            ClassificationResult with classification, intent, and recommended action\n        \"\"\"\n        # Step 1: Check for commands (highest priority)\n        command_result = self._detect_command(transcript)\n        if command_result:\n            result = self._build_result(\n                classification=\"command\",\n                intent=command_result[0],\n                priority=command_result[1],\n                confidence=1.0,\n                transcript=transcript,\n                duration_ms=duration_ms,\n                vad_probability=vad_probability,\n                during_ai_speech=during_ai_speech,\n                time_since_last_utterance_ms=time_since_last_utterance_ms,\n            )\n            self._record_classification(result)\n            return result\n\n        # Step 2: Check for backchannels (if during AI speech)\n        if during_ai_speech:\n            backchannel_result = self._detect_backchannel(transcript, duration_ms, vad_probability)\n\n            if backchannel_result.is_backchannel:\n                result = self._build_result(\n                    classification=\"backchannel\",\n                    intent=\"acknowledge\",\n                    priority=\"low\",\n                    confidence=backchannel_result.score,\n                    transcript=transcript,\n                    duration_ms=duration_ms,\n                    vad_probability=vad_probability,\n                    during_ai_speech=during_ai_speech,\n                    time_since_last_utterance_ms=time_since_last_utterance_ms,\n                )\n                self._record_classification(result)\n                return result\n\n            # Check for escalation\n            if backchannel_result.should_escalate:\n                result = self._build_result(\n                    classification=\"hard_barge\",\n                    intent=\"stop\",\n                    priority=\"high\",\n                    confidence=0.9,\n                    transcript=transcript,\n                    duration_ms=duration_ms,\n                    vad_probability=vad_probability,\n                    during_ai_speech=during_ai_speech,\n                    time_since_last_utterance_ms=time_since_last_utterance_ms,\n                )\n                self._record_classification(result)\n                return result\n\n            # Step 3: Check for soft barge\n            soft_barge_result = self._detect_soft_barge(transcript)\n            if soft_barge_result.is_soft_barge:\n                result = self._build_result(\n                    classification=\"soft_barge\",\n                    intent=\"pause\",\n                    priority=\"medium\",\n                    confidence=0.8,\n                    transcript=transcript,\n                    duration_ms=duration_ms,\n                    vad_probability=vad_probability,\n                    during_ai_speech=during_ai_speech,\n                    time_since_last_utterance_ms=time_since_last_utterance_ms,\n                )\n                self._record_classification(result)\n                return result\n\n        # Step 4: Check for hard barge patterns\n        hard_barge_result = self._detect_hard_barge(transcript)\n        if hard_barge_result:\n            result = self._build_result(\n                classification=\"hard_barge\",\n                intent=hard_barge_result,\n                priority=\"high\",\n                confidence=0.9,\n                transcript=transcript,\n                duration_ms=duration_ms,\n                vad_probability=vad_probability,\n                during_ai_speech=during_ai_speech,\n                time_since_last_utterance_ms=time_since_last_utterance_ms,\n            )\n            self._record_classification(result)\n            return result\n\n        # Step 5: Check for correction patterns\n        if self._detect_correction(transcript):\n            result = self._build_result(\n                classification=\"correction\",\n                intent=\"correct\",\n                priority=\"high\",\n                confidence=0.85,\n                transcript=transcript,\n                duration_ms=duration_ms,\n                vad_probability=vad_probability,\n                during_ai_speech=during_ai_speech,\n                time_since_last_utterance_ms=time_since_last_utterance_ms,\n            )\n            self._record_classification(result)\n            return result\n\n        # Step 6: Check for clarification requests\n        if self._detect_clarification(transcript):\n            result = self._build_result(\n                classification=\"clarification\",\n                intent=\"clarify\",\n                priority=\"medium\",\n                confidence=0.8,\n                transcript=transcript,\n                duration_ms=duration_ms,\n                vad_probability=vad_probability,\n                during_ai_speech=during_ai_speech,\n                time_since_last_utterance_ms=time_since_last_utterance_ms,\n            )\n            self._record_classification(result)\n            return result\n\n        # Step 7: Default classification\n        result = self._build_result(\n            classification=\"unknown\",\n            intent=\"uncertain\",\n            priority=\"medium\",\n            confidence=0.5,\n            transcript=transcript,\n            duration_ms=duration_ms,\n            vad_probability=vad_probability,\n            during_ai_speech=during_ai_speech,\n            time_since_last_utterance_ms=time_since_last_utterance_ms,\n        )\n        self._record_classification(result)\n        return result\n\n    def _detect_backchannel(self, transcript: str, duration_ms: int, confidence: float) -> BackchannelResult:\n        \"\"\"Detect if the transcript is a backchannel utterance\"\"\"\n        normalized = transcript.lower().strip()\n\n        # Too long to be a backchannel\n        if duration_ms > self.max_backchannel_duration_ms:\n            return BackchannelResult(is_backchannel=False, language=self.language)\n\n        patterns = BACKCHANNEL_PATTERNS.get(self.language, BACKCHANNEL_PATTERNS[\"en\"])\n\n        for pattern in patterns:\n            if duration_ms > pattern.max_duration_ms:\n                continue\n\n            for phrase in pattern.phrases:\n                if normalized == phrase or normalized.startswith(phrase + \" \"):\n                    score = confidence * (1 - duration_ms / 1000) * pattern.confidence\n                    should_escalate = self._track_and_check_escalation(phrase)\n\n                    return BackchannelResult(\n                        is_backchannel=score >= self.min_confidence and not should_escalate,\n                        matched_pattern=phrase,\n                        score=score,\n                        language=self.language,\n                        should_escalate=should_escalate,\n                    )\n\n        return BackchannelResult(is_backchannel=False, language=self.language)\n\n    def _detect_soft_barge(self, transcript: str) -> SoftBargeResult:\n        \"\"\"Detect if the transcript is a soft barge-in\"\"\"\n        normalized = transcript.lower().strip()\n        patterns = SOFT_BARGE_PATTERNS.get(self.language, SOFT_BARGE_PATTERNS[\"en\"])\n\n        for pattern in patterns:\n            for phrase in pattern.phrases:\n                if normalized.startswith(phrase) or normalized == phrase:\n                    return SoftBargeResult(\n                        is_soft_barge=True,\n                        matched_pattern=phrase,\n                        requires_follow_up=pattern.requires_follow_up,\n                        language=self.language,\n                    )\n\n        return SoftBargeResult(is_soft_barge=False, language=self.language)\n\n    def _detect_hard_barge(self, transcript: str) -> Optional[UserIntent]:\n        \"\"\"Detect if the transcript is a hard barge-in\"\"\"\n        normalized = transcript.lower().strip()\n        patterns = HARD_BARGE_PATTERNS.get(self.language, HARD_BARGE_PATTERNS[\"en\"])\n\n        for pattern in patterns:\n            for phrase in pattern.phrases:\n                if normalized == phrase or normalized.startswith(phrase + \" \"):\n                    return pattern.intent\n\n        return None\n\n    def _detect_command(self, transcript: str) -> Optional[Tuple[UserIntent, InterruptionPriority]]:\n        \"\"\"Detect if the transcript is a command\"\"\"\n        normalized = transcript.lower().strip()\n        patterns = COMMAND_PATTERNS.get(self.language, COMMAND_PATTERNS[\"en\"])\n\n        for pattern in patterns:\n            for phrase in pattern.phrases:\n                if normalized == phrase or normalized.startswith(phrase + \" \"):\n                    return (\"command\", pattern.priority)\n\n        return None\n\n    def _detect_correction(self, transcript: str) -> bool:\n        \"\"\"Detect if the transcript is a correction\"\"\"\n        normalized = transcript.lower().strip()\n        phrases = CORRECTION_PHRASES.get(self.language, CORRECTION_PHRASES[\"en\"])\n\n        return any(normalized == phrase or normalized.startswith(phrase + \" \") for phrase in phrases)\n\n    def _detect_clarification(self, transcript: str) -> bool:\n        \"\"\"Detect if the transcript is a clarification request\"\"\"\n        normalized = transcript.lower().strip()\n        phrases = CLARIFICATION_PHRASES.get(self.language, CLARIFICATION_PHRASES[\"en\"])\n\n        return any(normalized == phrase or normalized.startswith(phrase + \" \") for phrase in phrases)\n\n    def _track_and_check_escalation(self, pattern: str) -> bool:\n        \"\"\"Track backchannel detections and check for escalation\"\"\"\n        now = time.time() * 1000  # ms\n        timestamps = self._recent_detections.get(pattern, [])\n\n        # Clean old entries\n        cutoff = now - self.escalation_window_ms\n        recent_timestamps = [t for t in timestamps if t > cutoff]\n        recent_timestamps.append(now)\n\n        self._recent_detections[pattern] = recent_timestamps\n\n        # Check if escalation threshold reached\n        return len(recent_timestamps) >= self.escalation_threshold\n\n    def _build_result(\n        self,\n        classification: BargeInClassificationType,\n        intent: UserIntent,\n        priority: InterruptionPriority,\n        confidence: float,\n        transcript: str,\n        duration_ms: int,\n        vad_probability: float,\n        during_ai_speech: bool,\n        time_since_last_utterance_ms: int,\n    ) -> ClassificationResult:\n        \"\"\"Build a complete classification result\"\"\"\n        action = self._determine_action(classification, intent, priority)\n        metadata = ClassificationMetadata(\n            vad_probability=vad_probability,\n            during_ai_speech=during_ai_speech,\n            time_since_last_utterance_ms=time_since_last_utterance_ms,\n            recent_backchannel_count=self._get_total_recent_count(),\n        )\n\n        return ClassificationResult(\n            classification=classification,\n            intent=intent,\n            priority=priority,\n            confidence=confidence,\n            language=self.language,\n            transcript=transcript,\n            duration_ms=duration_ms,\n            action=action,\n            metadata=metadata,\n        )\n\n    def _determine_action(\n        self,\n        classification: BargeInClassificationType,\n        intent: UserIntent,\n        priority: InterruptionPriority,\n    ) -> ClassificationAction:\n        \"\"\"Determine the recommended action based on classification\"\"\"\n        if classification == \"backchannel\":\n            return ClassificationAction(type=\"acknowledge\", should_acknowledge=False)\n\n        if classification == \"soft_barge\":\n            return ClassificationAction(\n                type=\"pause\",\n                should_acknowledge=True,\n                pause_duration_ms=1500,\n                should_save_context=True,\n            )\n\n        if classification in (\"hard_barge\", \"command\"):\n            action_type = \"stop\" if intent == \"stop\" else \"yield\"\n            return ClassificationAction(\n                type=action_type,\n                should_acknowledge=True,\n                should_save_context=True,\n            )\n\n        if classification == \"correction\":\n            return ClassificationAction(\n                type=\"respond\",\n                should_acknowledge=True,\n                should_save_context=True,\n            )\n\n        if classification == \"clarification\":\n            return ClassificationAction(\n                type=\"respond\",\n                should_acknowledge=True,\n                should_save_context=False,\n            )\n\n        # Default\n        return ClassificationAction(\n            type=\"wait\" if priority != \"low\" else \"continue\",\n            should_acknowledge=False,\n        )\n\n    def _get_total_recent_count(self) -> int:\n        \"\"\"Get total count of recent backchannel detections\"\"\"\n        now = time.time() * 1000\n        cutoff = now - self.escalation_window_ms\n        total = 0\n        for timestamps in self._recent_detections.values():\n            total += sum(1 for t in timestamps if t > cutoff)\n        return total\n\n    def _record_classification(self, result: ClassificationResult) -> None:\n        \"\"\"Record classification for analytics\"\"\"\n        self._classification_history.append(result)\n        if len(self._classification_history) > self._max_history_size:\n            self._classification_history.pop(0)\n\n    def reset(self) -> None:\n        \"\"\"Reset all state\"\"\"\n        self._recent_detections.clear()\n        self._classification_history.clear()\n        logger.debug(\"BargeInClassifier state reset\")\n\n    def get_statistics(self) -> Dict:\n        \"\"\"Get classification statistics\"\"\"\n        if not self._classification_history:\n            return {\n                \"total_classifications\": 0,\n                \"backchannel_rate\": 0.0,\n                \"hard_barge_rate\": 0.0,\n                \"average_confidence\": 0.0,\n            }\n\n        history = self._classification_history\n        backchannels = sum(1 for r in history if r.classification == \"backchannel\")\n        hard_barges = sum(1 for r in history if r.classification == \"hard_barge\")\n        avg_confidence = sum(r.confidence for r in history) / len(history)\n\n        return {\n            \"total_classifications\": len(history),\n            \"backchannel_rate\": backchannels / len(history),\n            \"hard_barge_rate\": hard_barges / len(history),\n            \"average_confidence\": avg_confidence,\n            \"dominant_language\": self.language,\n        }\n\n\n# ============================================================================\n# Factory Function\n# ============================================================================\n\n\ndef create_barge_in_classifier(\n    language: SupportedLanguage = \"en\",\n    **kwargs,\n) -> BargeInClassifier:\n    \"\"\"Create a new BargeInClassifier instance\"\"\"\n    return BargeInClassifier(language=language, **kwargs)\n"
}
