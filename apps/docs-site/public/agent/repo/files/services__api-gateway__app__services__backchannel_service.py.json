{
  "path": "services/api-gateway/app/services/backchannel_service.py",
  "language": "python",
  "size": 38245,
  "last_modified": "2025-12-04T21:30:58.282Z",
  "lines": 1049,
  "content": "\"\"\"\nBackchannel Service - Natural Verbal Acknowledgments\n\nProvides natural verbal cues during user speech to show active listening.\nExamples: \"uh-huh\", \"mm-hmm\", \"I see\", \"right\", \"got it\"\n\nFeatures:\n- Pre-cached backchannel audio clips per voice\n- Intelligent timing based on speech patterns\n- Multi-language support\n- Integration with ElevenLabs TTS\n- Emotion-aware phrase selection\n- Event bus integration for context.emotion_alert events\n\nPhase: Voice Mode Backchanneling Enhancement (Phase 3)\n\"\"\"\n\nimport asyncio\nimport hashlib\nimport random\nimport tempfile\nimport time\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nfrom pathlib import Path\nfrom typing import Any, Awaitable, Callable, Dict, List, Optional, Set, Tuple\n\nfrom app.core.config import settings\nfrom app.core.logging import get_logger\nfrom app.services.elevenlabs_service import ElevenLabsService\n\nlogger = get_logger(__name__)\n\n\n# ==============================================================================\n# Data Classes and Enums\n# ==============================================================================\n\n\nclass BackchannelType(str, Enum):\n    \"\"\"Types of backchannels based on conversational function.\"\"\"\n\n    ACKNOWLEDGMENT = \"acknowledgment\"  # \"uh-huh\", \"mm-hmm\"\n    UNDERSTANDING = \"understanding\"  # \"I see\", \"right\", \"got it\"\n    ENCOURAGEMENT = \"encouragement\"  # \"go on\", \"yes\"\n    SURPRISE = \"surprise\"  # \"oh\", \"wow\", \"really\"\n    EMPATHY = \"empathy\"  # \"hmm\", \"I understand\"\n\n\n@dataclass\nclass BackchannelPhrase:\n    \"\"\"A backchannel phrase with metadata.\"\"\"\n\n    text: str\n    type: BackchannelType\n    language: str = \"en\"\n    weight: float = 1.0  # Probability weight for selection\n    min_gap_seconds: float = 5.0  # Minimum gap before this can be used again\n\n\n# Language-specific backchannel phrases\nBACKCHANNEL_PHRASES: Dict[str, List[BackchannelPhrase]] = {\n    \"en\": [\n        # Acknowledgments (most common)\n        BackchannelPhrase(\"uh-huh\", BackchannelType.ACKNOWLEDGMENT, weight=2.0),\n        BackchannelPhrase(\"mm-hmm\", BackchannelType.ACKNOWLEDGMENT, weight=2.0),\n        BackchannelPhrase(\"mhm\", BackchannelType.ACKNOWLEDGMENT, weight=1.5),\n        # Understanding\n        BackchannelPhrase(\"I see\", BackchannelType.UNDERSTANDING, weight=1.5),\n        BackchannelPhrase(\"right\", BackchannelType.UNDERSTANDING, weight=1.5),\n        BackchannelPhrase(\"got it\", BackchannelType.UNDERSTANDING, weight=1.0),\n        BackchannelPhrase(\"okay\", BackchannelType.UNDERSTANDING, weight=1.0),\n        # Encouragement\n        BackchannelPhrase(\"yes\", BackchannelType.ENCOURAGEMENT, weight=1.0),\n        BackchannelPhrase(\"go on\", BackchannelType.ENCOURAGEMENT, weight=0.5),\n        # Empathy\n        BackchannelPhrase(\"hmm\", BackchannelType.EMPATHY, weight=1.0),\n        # Surprise\n        BackchannelPhrase(\"oh\", BackchannelType.SURPRISE, weight=0.8),\n        BackchannelPhrase(\"really\", BackchannelType.SURPRISE, weight=0.5),\n    ],\n    \"ar\": [\n        # Arabic acknowledgments\n        BackchannelPhrase(\"اها\", BackchannelType.ACKNOWLEDGMENT, \"ar\", weight=2.0),\n        BackchannelPhrase(\"نعم\", BackchannelType.UNDERSTANDING, \"ar\", weight=1.5),\n        BackchannelPhrase(\"صح\", BackchannelType.UNDERSTANDING, \"ar\", weight=1.0),\n        BackchannelPhrase(\"طيب\", BackchannelType.UNDERSTANDING, \"ar\", weight=1.0),\n        BackchannelPhrase(\"تمام\", BackchannelType.UNDERSTANDING, \"ar\", weight=1.0),\n        # Empathy\n        BackchannelPhrase(\"آه\", BackchannelType.EMPATHY, \"ar\", weight=1.0),\n    ],\n    \"es\": [\n        BackchannelPhrase(\"ajá\", BackchannelType.ACKNOWLEDGMENT, \"es\", weight=2.0),\n        BackchannelPhrase(\"mm-hmm\", BackchannelType.ACKNOWLEDGMENT, \"es\", weight=1.5),\n        BackchannelPhrase(\"ya\", BackchannelType.UNDERSTANDING, \"es\", weight=1.5),\n        BackchannelPhrase(\"entiendo\", BackchannelType.UNDERSTANDING, \"es\", weight=1.0),\n        BackchannelPhrase(\"claro\", BackchannelType.UNDERSTANDING, \"es\", weight=1.0),\n    ],\n    \"fr\": [\n        BackchannelPhrase(\"mm-hmm\", BackchannelType.ACKNOWLEDGMENT, \"fr\", weight=2.0),\n        BackchannelPhrase(\"oui\", BackchannelType.UNDERSTANDING, \"fr\", weight=1.5),\n        BackchannelPhrase(\"d'accord\", BackchannelType.UNDERSTANDING, \"fr\", weight=1.0),\n        BackchannelPhrase(\"je vois\", BackchannelType.UNDERSTANDING, \"fr\", weight=1.0),\n    ],\n}\n\n\n# Emotion-specific phrase mappings (English)\n# Maps emotion states to preferred backchannel types\nEMOTION_PHRASE_MAP: Dict[str, Dict[str, Any]] = {\n    \"neutral\": {\n        \"preferred_types\": [\n            BackchannelType.ACKNOWLEDGMENT,\n            BackchannelType.UNDERSTANDING,\n        ],\n        \"weight_boost\": 1.0,\n    },\n    \"happy\": {\n        \"preferred_types\": [\n            BackchannelType.ENCOURAGEMENT,\n            BackchannelType.ACKNOWLEDGMENT,\n        ],\n        \"weight_boost\": 1.2,\n    },\n    \"sad\": {\n        \"preferred_types\": [BackchannelType.EMPATHY, BackchannelType.UNDERSTANDING],\n        \"weight_boost\": 1.3,\n        \"extra_phrases\": [\n            BackchannelPhrase(\"I hear you\", BackchannelType.EMPATHY, weight=1.5),\n            BackchannelPhrase(\"I understand\", BackchannelType.EMPATHY, weight=1.5),\n        ],\n    },\n    \"frustrated\": {\n        \"preferred_types\": [BackchannelType.EMPATHY, BackchannelType.UNDERSTANDING],\n        \"weight_boost\": 1.5,\n        \"extra_phrases\": [\n            BackchannelPhrase(\"I hear you\", BackchannelType.EMPATHY, weight=2.0),\n            BackchannelPhrase(\"I understand\", BackchannelType.EMPATHY, weight=2.0),\n            BackchannelPhrase(\"that makes sense\", BackchannelType.UNDERSTANDING, weight=1.5),\n        ],\n        \"reduce_frequency\": True,  # Less frequent backchannels when frustrated\n    },\n    \"anxious\": {\n        \"preferred_types\": [BackchannelType.EMPATHY, BackchannelType.ENCOURAGEMENT],\n        \"weight_boost\": 1.2,\n        \"extra_phrases\": [\n            BackchannelPhrase(\"it's okay\", BackchannelType.EMPATHY, weight=1.5),\n            BackchannelPhrase(\"take your time\", BackchannelType.ENCOURAGEMENT, weight=1.5),\n        ],\n    },\n    \"confused\": {\n        \"preferred_types\": [\n            BackchannelType.UNDERSTANDING,\n            BackchannelType.ENCOURAGEMENT,\n        ],\n        \"weight_boost\": 1.0,\n        \"extra_phrases\": [\n            BackchannelPhrase(\"go on\", BackchannelType.ENCOURAGEMENT, weight=1.5),\n        ],\n    },\n    \"surprised\": {\n        \"preferred_types\": [BackchannelType.SURPRISE, BackchannelType.ACKNOWLEDGMENT],\n        \"weight_boost\": 1.3,\n    },\n}\n\n\n@dataclass\nclass BackchannelTrigger:\n    \"\"\"Result of backchannel timing analysis.\"\"\"\n\n    should_trigger: bool\n    phrase: Optional[BackchannelPhrase] = None\n    reason: str = \"\"\n    confidence: float = 0.0\n\n\n@dataclass\nclass BackchannelAudio:\n    \"\"\"Cached backchannel audio data.\"\"\"\n\n    phrase: str\n    voice_id: str\n    audio_data: bytes\n    format: str = \"pcm_24000\"\n    duration_ms: int = 0\n    cached_at: float = field(default_factory=time.time)\n\n\n@dataclass\nclass BackchannelState:\n    \"\"\"State tracking for backchannel timing in a session.\"\"\"\n\n    last_backchannel_time: float = 0.0\n    last_phrase_used: Optional[str] = None\n    recent_phrases: List[str] = field(default_factory=list)\n    speech_start_time: float = 0.0\n    continuous_speech_ms: int = 0\n    pause_count: int = 0\n    total_backchannels: int = 0\n\n\n@dataclass\nclass UserBackchannelCalibration:\n    \"\"\"Per-user calibration for backchannel timing preferences.\"\"\"\n\n    user_id: str\n    # Timing adjustments (multipliers)\n    min_gap_multiplier: float = 1.0\n    min_speech_multiplier: float = 1.0\n    pause_window_start_ms: int = 150\n    pause_window_end_ms: int = 400\n\n    # Phrase preferences\n    preferred_types: List[BackchannelType] = field(default_factory=list)\n    disliked_phrases: List[str] = field(default_factory=list)\n\n    # Feedback tracking\n    total_backchannels: int = 0\n    positive_feedback: int = 0\n    negative_feedback: int = 0\n\n    # Timestamps\n    created_at: float = field(default_factory=time.time)\n    last_updated: float = field(default_factory=time.time)\n\n    def get_acceptance_rate(self) -> float:\n        \"\"\"Calculate backchannel acceptance rate\"\"\"\n        total_feedback = self.positive_feedback + self.negative_feedback\n        if total_feedback == 0:\n            return 0.5  # Default to neutral\n        return self.positive_feedback / total_feedback\n\n\nclass BackchannelCalibrationService:\n    \"\"\"\n    Manages per-user backchannel timing calibration.\n\n    Learns user preferences through:\n    - Explicit feedback (thumbs up/down)\n    - Implicit signals (interruptions, silence after backchannel)\n    - Conversation outcomes\n    \"\"\"\n\n    # Learning rates\n    POSITIVE_LEARNING_RATE = 0.1\n    NEGATIVE_LEARNING_RATE = 0.15  # Learn faster from negative feedback\n\n    # Adjustment bounds\n    MIN_MULTIPLIER = 0.5\n    MAX_MULTIPLIER = 2.0\n\n    def __init__(self):\n        self._user_calibrations: Dict[str, UserBackchannelCalibration] = {}\n        logger.info(\"BackchannelCalibrationService initialized\")\n\n    def get_calibration(self, user_id: str) -> UserBackchannelCalibration:\n        \"\"\"Get or create calibration for user\"\"\"\n        if user_id not in self._user_calibrations:\n            self._user_calibrations[user_id] = UserBackchannelCalibration(user_id=user_id)\n        return self._user_calibrations[user_id]\n\n    def record_backchannel(\n        self,\n        user_id: str,\n        phrase: BackchannelPhrase,\n        was_accepted: bool,\n        was_interrupted: bool = False,\n    ) -> None:\n        \"\"\"\n        Record backchannel outcome for calibration learning.\n\n        Args:\n            user_id: User identifier\n            phrase: The phrase that was used\n            was_accepted: Whether user continued speaking normally\n            was_interrupted: Whether user interrupted the backchannel\n        \"\"\"\n        cal = self.get_calibration(user_id)\n        cal.total_backchannels += 1\n        cal.last_updated = time.time()\n\n        if was_interrupted:\n            # Strong negative signal - backchannels are too frequent\n            cal.negative_feedback += 1\n            cal.min_gap_multiplier = min(\n                self.MAX_MULTIPLIER,\n                cal.min_gap_multiplier * (1 + self.NEGATIVE_LEARNING_RATE),\n            )\n            logger.debug(f\"User {user_id} interrupted backchannel - increasing gap\")\n\n        elif was_accepted:\n            cal.positive_feedback += 1\n            # Slight positive adjustment\n            cal.min_gap_multiplier = max(\n                self.MIN_MULTIPLIER,\n                cal.min_gap_multiplier * (1 - self.POSITIVE_LEARNING_RATE * 0.5),\n            )\n        else:\n            # Neutral or negative - don't adjust much\n            cal.negative_feedback += 1\n\n        # Track phrase preferences\n        if not was_accepted and phrase.text not in cal.disliked_phrases:\n            if cal.negative_feedback > 3:  # After some negative feedback\n                cal.disliked_phrases.append(phrase.text)\n\n    def record_explicit_feedback(\n        self,\n        user_id: str,\n        is_positive: bool,\n        feedback_type: str = \"general\",\n    ) -> None:\n        \"\"\"\n        Record explicit user feedback about backchannels.\n\n        Args:\n            user_id: User identifier\n            is_positive: Whether feedback was positive\n            feedback_type: Type of feedback (general, too_frequent, too_rare, etc.)\n        \"\"\"\n        cal = self.get_calibration(user_id)\n        cal.last_updated = time.time()\n\n        if is_positive:\n            cal.positive_feedback += 1\n        else:\n            cal.negative_feedback += 1\n\n            # Adjust based on feedback type\n            if feedback_type == \"too_frequent\":\n                cal.min_gap_multiplier = min(\n                    self.MAX_MULTIPLIER,\n                    cal.min_gap_multiplier * 1.3,\n                )\n            elif feedback_type == \"too_rare\":\n                cal.min_gap_multiplier = max(\n                    self.MIN_MULTIPLIER,\n                    cal.min_gap_multiplier * 0.7,\n                )\n            elif feedback_type == \"too_early\":\n                cal.min_speech_multiplier = min(\n                    self.MAX_MULTIPLIER,\n                    cal.min_speech_multiplier * 1.2,\n                )\n            elif feedback_type == \"wrong_timing\":\n                cal.pause_window_start_ms = min(300, cal.pause_window_start_ms + 30)\n                cal.pause_window_end_ms = min(600, cal.pause_window_end_ms + 50)\n\n        logger.info(f\"Recorded explicit feedback for {user_id}: \" f\"positive={is_positive}, type={feedback_type}\")\n\n    def get_adjusted_thresholds(\n        self,\n        user_id: str,\n        base_min_gap_ms: int,\n        base_min_speech_ms: int,\n    ) -> Tuple[int, int, int, int]:\n        \"\"\"\n        Get user-adjusted timing thresholds.\n\n        Returns:\n            Tuple of (min_gap_ms, min_speech_ms, pause_start_ms, pause_end_ms)\n        \"\"\"\n        cal = self.get_calibration(user_id)\n\n        return (\n            int(base_min_gap_ms * cal.min_gap_multiplier),\n            int(base_min_speech_ms * cal.min_speech_multiplier),\n            cal.pause_window_start_ms,\n            cal.pause_window_end_ms,\n        )\n\n    def should_use_phrase(\n        self,\n        user_id: str,\n        phrase: BackchannelPhrase,\n    ) -> bool:\n        \"\"\"Check if a phrase should be used for this user\"\"\"\n        cal = self.get_calibration(user_id)\n        return phrase.text not in cal.disliked_phrases\n\n    def get_user_stats(self, user_id: str) -> Dict[str, Any]:\n        \"\"\"Get calibration statistics for a user\"\"\"\n        cal = self.get_calibration(user_id)\n        return {\n            \"user_id\": user_id,\n            \"total_backchannels\": cal.total_backchannels,\n            \"acceptance_rate\": cal.get_acceptance_rate(),\n            \"min_gap_multiplier\": cal.min_gap_multiplier,\n            \"min_speech_multiplier\": cal.min_speech_multiplier,\n            \"pause_window\": (cal.pause_window_start_ms, cal.pause_window_end_ms),\n            \"disliked_phrases\": cal.disliked_phrases,\n            \"positive_feedback\": cal.positive_feedback,\n            \"negative_feedback\": cal.negative_feedback,\n        }\n\n\n# ==============================================================================\n# Backchannel Timing Logic\n# ==============================================================================\n\n\nclass BackchannelTimingEngine:\n    \"\"\"\n    Determines when to trigger backchannels based on speech patterns.\n\n    Timing rules:\n    - Minimum 5 seconds between backchannels\n    - Only during natural pauses (150-300ms silence)\n    - After sustained speech (2-3 seconds minimum)\n    - Never interrupt mid-sentence\n    - Vary phrase selection to avoid repetition\n    - Emotion-aware phrase selection for empathetic responses\n    \"\"\"\n\n    # Timing constants\n    MIN_GAP_BETWEEN_BACKCHANNELS_MS = 5000  # 5 seconds\n    MIN_SPEECH_BEFORE_BACKCHANNEL_MS = 2000  # 2 seconds\n    OPTIMAL_PAUSE_MIN_MS = 150  # Short pause start\n    OPTIMAL_PAUSE_MAX_MS = 400  # Short pause end (before it becomes long pause)\n    MAX_PHRASE_REPEAT_COUNT = 3  # Max times to use same phrase before cycling\n\n    # Reduced frequency multiplier for frustrated users\n    FRUSTRATED_GAP_MULTIPLIER = 1.5\n\n    def __init__(self, language: str = \"en\", use_emotion_aware: bool = True):\n        \"\"\"\n        Initialize backchannel timing engine.\n\n        Args:\n            language: Language code for phrase selection\n            use_emotion_aware: Whether to use emotion-aware phrase selection.\n                               Set to False for A/B testing control group.\n        \"\"\"\n        self.language = language\n        self._use_emotion_aware = use_emotion_aware\n        self._base_phrases = BACKCHANNEL_PHRASES.get(language, BACKCHANNEL_PHRASES[\"en\"])\n        self._phrases = self._base_phrases.copy()\n        self._phrase_weights = [p.weight for p in self._phrases]\n        self._current_emotion: str = \"neutral\"\n        self._emotion_config: Dict[str, Any] = EMOTION_PHRASE_MAP.get(\"neutral\", {})\n\n    def set_emotion_state(self, emotion: str) -> None:\n        \"\"\"\n        Update the current emotion state for phrase selection.\n\n        Args:\n            emotion: Current dominant emotion (neutral, happy, sad, frustrated, etc.)\n\n        Note:\n            If use_emotion_aware is False (A/B test control group),\n            this method will track the emotion but not adjust phrases.\n        \"\"\"\n        if emotion == self._current_emotion:\n            return\n\n        self._current_emotion = emotion\n        self._emotion_config = EMOTION_PHRASE_MAP.get(emotion, EMOTION_PHRASE_MAP[\"neutral\"])\n\n        # Skip emotion-aware adjustments if disabled (A/B test control group)\n        if not self._use_emotion_aware:\n            logger.debug(f\"Backchannel emotion tracked (not applied): {emotion}\")\n            return\n\n        # Rebuild phrase list with emotion-specific extras\n        self._phrases = self._base_phrases.copy()\n        extra_phrases = self._emotion_config.get(\"extra_phrases\", [])\n        if extra_phrases:\n            self._phrases = self._phrases + extra_phrases\n\n        # Recalculate weights with emotion boost\n        preferred_types = self._emotion_config.get(\"preferred_types\", [])\n        weight_boost = self._emotion_config.get(\"weight_boost\", 1.0)\n\n        self._phrase_weights = []\n        for p in self._phrases:\n            weight = p.weight\n            if p.type in preferred_types:\n                weight *= weight_boost\n            self._phrase_weights.append(weight)\n\n        logger.debug(f\"Backchannel emotion updated to: {emotion}\")\n\n    def get_min_gap_ms(self) -> int:\n        \"\"\"Get minimum gap between backchannels, adjusted for emotion\"\"\"\n        base_gap = self.MIN_GAP_BETWEEN_BACKCHANNELS_MS\n        # Only apply emotion-aware gap adjustment if enabled\n        if self._use_emotion_aware and self._emotion_config.get(\"reduce_frequency\", False):\n            return int(base_gap * self.FRUSTRATED_GAP_MULTIPLIER)\n        return base_gap\n\n    def should_trigger(\n        self,\n        state: BackchannelState,\n        current_time: float,\n        pause_duration_ms: int,\n        is_speaking: bool,\n        emotion_state: Optional[str] = None,\n    ) -> BackchannelTrigger:\n        \"\"\"\n        Determine if a backchannel should be triggered.\n\n        Args:\n            state: Current session backchannel state\n            current_time: Current timestamp\n            pause_duration_ms: Duration of current pause in speech\n            is_speaking: Whether user is currently speaking\n            emotion_state: Optional current emotion state for adaptive timing\n\n        Returns:\n            BackchannelTrigger with decision and selected phrase\n        \"\"\"\n        # Update emotion if provided\n        if emotion_state:\n            self.set_emotion_state(emotion_state)\n\n        # Rule 1: Minimum gap between backchannels (emotion-aware)\n        min_gap = self.get_min_gap_ms()\n        time_since_last = (current_time - state.last_backchannel_time) * 1000\n        if time_since_last < min_gap:\n            return BackchannelTrigger(\n                should_trigger=False,\n                reason=f\"Too soon (last: {time_since_last:.0f}ms ago, min: {min_gap}ms)\",\n            )\n\n        # Rule 2: Need minimum speech before backchannel\n        if state.continuous_speech_ms < self.MIN_SPEECH_BEFORE_BACKCHANNEL_MS:\n            return BackchannelTrigger(\n                should_trigger=False,\n                reason=f\"Not enough speech ({state.continuous_speech_ms}ms)\",\n            )\n\n        # Rule 3: Only trigger during optimal pause window\n        if is_speaking:\n            return BackchannelTrigger(\n                should_trigger=False,\n                reason=\"User still speaking\",\n            )\n\n        if pause_duration_ms < self.OPTIMAL_PAUSE_MIN_MS:\n            return BackchannelTrigger(\n                should_trigger=False,\n                reason=f\"Pause too short ({pause_duration_ms}ms)\",\n            )\n\n        if pause_duration_ms > self.OPTIMAL_PAUSE_MAX_MS:\n            # Pause is too long - might be end of thought, don't backchannel\n            return BackchannelTrigger(\n                should_trigger=False,\n                reason=f\"Pause too long ({pause_duration_ms}ms) - likely end of thought\",\n            )\n\n        # All conditions met - select phrase\n        phrase = self._select_phrase(state)\n\n        return BackchannelTrigger(\n            should_trigger=True,\n            phrase=phrase,\n            reason=\"Optimal timing\",\n            confidence=0.8,\n        )\n\n    def _select_phrase(self, state: BackchannelState) -> BackchannelPhrase:\n        \"\"\"Select a backchannel phrase, avoiding recent repetition.\"\"\"\n        available_phrases = [\n            p for p in self._phrases if p.text not in state.recent_phrases[-self.MAX_PHRASE_REPEAT_COUNT :]\n        ]\n\n        if not available_phrases:\n            # All phrases used recently, reset and use any\n            available_phrases = self._phrases\n\n        # Weighted random selection (not security-sensitive, just for UI variety)\n        weights = [p.weight for p in available_phrases]\n        total_weight = sum(weights)\n        r = random.random() * total_weight  # nosec B311\n\n        cumulative = 0\n        for phrase, weight in zip(available_phrases, weights):\n            cumulative += weight\n            if r <= cumulative:\n                return phrase\n\n        # Fallback\n        return available_phrases[0]\n\n\n# ==============================================================================\n# Backchannel Audio Cache\n# ==============================================================================\n\n\nclass BackchannelAudioCache:\n    \"\"\"\n    Manages pre-generated backchannel audio clips.\n\n    Caches audio per voice_id to avoid repeated TTS calls.\n    \"\"\"\n\n    def __init__(self, cache_dir: Optional[Path] = None):\n        self._cache: Dict[str, BackchannelAudio] = {}\n        self._cache_dir = cache_dir or Path(tempfile.gettempdir()) / \"voiceassist_backchannel_cache\"\n        self._cache_dir.mkdir(parents=True, exist_ok=True)\n        self._generating: Set[str] = set()  # Track in-progress generations\n\n    def _cache_key(self, phrase: str, voice_id: str) -> str:\n        \"\"\"Generate cache key for phrase/voice combination.\"\"\"\n        return hashlib.md5(f\"{phrase}:{voice_id}\".encode(), usedforsecurity=False).hexdigest()\n\n    def get(self, phrase: str, voice_id: str) -> Optional[BackchannelAudio]:\n        \"\"\"Get cached audio for phrase/voice.\"\"\"\n        key = self._cache_key(phrase, voice_id)\n        return self._cache.get(key)\n\n    def put(self, audio: BackchannelAudio) -> None:\n        \"\"\"Store audio in cache.\"\"\"\n        key = self._cache_key(audio.phrase, audio.voice_id)\n        self._cache[key] = audio\n\n    def is_generating(self, phrase: str, voice_id: str) -> bool:\n        \"\"\"Check if audio is currently being generated.\"\"\"\n        key = self._cache_key(phrase, voice_id)\n        return key in self._generating\n\n    def mark_generating(self, phrase: str, voice_id: str) -> None:\n        \"\"\"Mark phrase as being generated.\"\"\"\n        key = self._cache_key(phrase, voice_id)\n        self._generating.add(key)\n\n    def unmark_generating(self, phrase: str, voice_id: str) -> None:\n        \"\"\"Remove generating mark.\"\"\"\n        key = self._cache_key(phrase, voice_id)\n        self._generating.discard(key)\n\n    def get_cache_stats(self) -> Dict:\n        \"\"\"Get cache statistics.\"\"\"\n        return {\n            \"cached_phrases\": len(self._cache),\n            \"generating\": len(self._generating),\n        }\n\n\n# ==============================================================================\n# Backchannel Session\n# ==============================================================================\n\n\nclass BackchannelSession:\n    \"\"\"\n    Manages backchanneling for a voice session.\n\n    Tracks speech patterns and determines when to emit backchannels.\n    Listens for context.emotion_alert events to update phrase selection.\n\n    A/B Testing:\n        Set use_emotion_aware=False for control group to disable\n        emotion-aware phrase selection and timing adjustments.\n    \"\"\"\n\n    def __init__(\n        self,\n        session_id: str,\n        voice_id: str,\n        language: str = \"en\",\n        elevenlabs_service: Optional[ElevenLabsService] = None,\n        audio_cache: Optional[BackchannelAudioCache] = None,\n        on_backchannel: Optional[Callable[[BackchannelAudio], Awaitable[None]]] = None,\n        event_bus: Optional[Any] = None,\n        use_emotion_aware: bool = True,\n    ):\n        self.session_id = session_id\n        self.voice_id = voice_id\n        self.language = language\n        self._elevenlabs = elevenlabs_service\n        self._cache = audio_cache or BackchannelAudioCache()\n        self._on_backchannel = on_backchannel\n        self._event_bus = event_bus\n        self._use_emotion_aware = use_emotion_aware\n\n        # Timing engine with A/B test configuration\n        self._timing = BackchannelTimingEngine(language, use_emotion_aware=use_emotion_aware)\n\n        # State\n        self._state = BackchannelState()\n        self._active = False\n        self._speech_active = False\n        self._pause_start_time: Optional[float] = None\n        self._current_emotion: str = \"neutral\"\n\n    async def start(self) -> None:\n        \"\"\"Start the backchannel session.\"\"\"\n        self._active = True\n        self._state = BackchannelState()\n        logger.info(f\"Backchannel session started: {self.session_id}\")\n\n        # Subscribe to emotion events if event bus is available\n        if self._event_bus:\n            self._subscribe_to_events()\n\n        # Pre-warm cache with common phrases\n        asyncio.create_task(self._prewarm_cache())\n\n    def _subscribe_to_events(self) -> None:\n        \"\"\"Subscribe to relevant events from the event bus\"\"\"\n\n        async def handle_emotion_updated(event):\n            \"\"\"Handle emotion.updated events to update phrase selection\"\"\"\n            if event.session_id != self.session_id:\n                return\n            emotion_data = event.data.get(\"emotion\", {})\n            dominant_emotion = emotion_data.get(\"dominant_emotion\", \"neutral\")\n            await self.set_emotion_state(dominant_emotion)\n\n        async def handle_emotion_alert(event):\n            \"\"\"Handle context.emotion_alert events for significant changes\"\"\"\n            if event.session_id != self.session_id:\n                return\n            # Emotion alerts indicate significant changes - update immediately\n            emotion = event.data.get(\"current_emotion\", \"neutral\")\n            deviation = event.data.get(\"deviation_score\", 0)\n            if deviation > 1.5:  # Significant deviation\n                await self.set_emotion_state(emotion)\n                logger.info(\n                    f\"Backchannel emotion alert: {emotion} \"\n                    f\"(deviation: {deviation:.2f}) for session {self.session_id}\"\n                )\n\n        # Subscribe to events\n        self._event_bus.subscribe(\n            \"emotion.updated\",\n            handle_emotion_updated,\n            priority=0,\n            engine=\"backchannel\",\n        )\n        self._event_bus.subscribe(\n            \"context.emotion_alert\",\n            handle_emotion_alert,\n            priority=5,\n            engine=\"backchannel\",\n        )\n\n    async def set_emotion_state(self, emotion: str) -> None:\n        \"\"\"\n        Update the current emotion state for phrase selection.\n\n        Args:\n            emotion: Current dominant emotion\n        \"\"\"\n        if emotion == self._current_emotion:\n            return\n\n        self._current_emotion = emotion\n        self._timing.set_emotion_state(emotion)\n        logger.debug(f\"Backchannel session {self.session_id} emotion: {emotion}\")\n\n    async def stop(self) -> None:\n        \"\"\"Stop the session.\"\"\"\n        self._active = False\n        logger.info(\n            f\"Backchannel session stopped: {self.session_id}, \" f\"total backchannels: {self._state.total_backchannels}\"\n        )\n\n    async def _prewarm_cache(self) -> None:\n        \"\"\"Pre-generate common backchannel audio clips.\"\"\"\n        phrases = BACKCHANNEL_PHRASES.get(self.language, BACKCHANNEL_PHRASES[\"en\"])\n\n        # Prioritize high-weight phrases\n        sorted_phrases = sorted(phrases, key=lambda p: p.weight, reverse=True)\n\n        for phrase in sorted_phrases[:5]:  # Pre-warm top 5\n            if not self._cache.get(phrase.text, self.voice_id):\n                await self._generate_audio(phrase.text)\n\n    async def on_speech_start(self) -> None:\n        \"\"\"Called when user starts speaking.\"\"\"\n        if not self._active:\n            return\n\n        self._speech_active = True\n        self._state.speech_start_time = time.time()\n        self._pause_start_time = None\n\n    async def on_speech_continue(self, duration_ms: int) -> None:\n        \"\"\"Called periodically while user is speaking.\"\"\"\n        if not self._active or not self._speech_active:\n            return\n\n        self._state.continuous_speech_ms = duration_ms\n\n    async def on_pause_detected(self, pause_duration_ms: int) -> None:\n        \"\"\"\n        Called when a pause is detected in user speech.\n\n        This is the main trigger point for backchannels.\n        \"\"\"\n        if not self._active:\n            return\n\n        if self._pause_start_time is None:\n            self._pause_start_time = time.time()\n\n        current_time = time.time()\n\n        # Check if we should backchannel (with emotion context)\n        trigger = self._timing.should_trigger(\n            state=self._state,\n            current_time=current_time,\n            pause_duration_ms=pause_duration_ms,\n            is_speaking=False,\n            emotion_state=self._current_emotion,\n        )\n\n        if trigger.should_trigger and trigger.phrase:\n            await self._emit_backchannel(trigger.phrase)\n\n    async def on_speech_end(self) -> None:\n        \"\"\"Called when user finishes speaking (end of utterance).\"\"\"\n        self._speech_active = False\n        self._state.continuous_speech_ms = 0\n        self._pause_start_time = None\n\n    async def _emit_backchannel(self, phrase: BackchannelPhrase) -> None:\n        \"\"\"Emit a backchannel audio clip.\"\"\"\n        # Get or generate audio\n        audio = self._cache.get(phrase.text, self.voice_id)\n\n        if not audio:\n            audio = await self._generate_audio(phrase.text)\n\n        if not audio:\n            logger.warning(f\"Failed to get backchannel audio for: {phrase.text}\")\n            return\n\n        # Update state\n        self._state.last_backchannel_time = time.time()\n        self._state.last_phrase_used = phrase.text\n        self._state.recent_phrases.append(phrase.text)\n        self._state.total_backchannels += 1\n\n        # Keep recent phrases list bounded\n        if len(self._state.recent_phrases) > 10:\n            self._state.recent_phrases = self._state.recent_phrases[-10:]\n\n        logger.info(f\"Emitting backchannel: '{phrase.text}' (total: {self._state.total_backchannels})\")\n\n        # Emit to callback\n        if self._on_backchannel:\n            await self._on_backchannel(audio)\n\n    async def _generate_audio(self, phrase: str) -> Optional[BackchannelAudio]:\n        \"\"\"Generate backchannel audio using TTS.\"\"\"\n        if not self._elevenlabs:\n            logger.warning(\"No ElevenLabs service available for backchannel TTS\")\n            return None\n\n        # Check if already generating\n        if self._cache.is_generating(phrase, self.voice_id):\n            # Wait for it to complete\n            for _ in range(20):  # 2 second timeout\n                await asyncio.sleep(0.1)\n                cached = self._cache.get(phrase, self.voice_id)\n                if cached:\n                    return cached\n            return None\n\n        try:\n            self._cache.mark_generating(phrase, self.voice_id)\n\n            # Generate with ElevenLabs\n            # Use specific settings for natural backchannel sound\n            result = await self._elevenlabs.synthesize(\n                text=phrase,\n                voice_id=self.voice_id,\n                stability=0.7,  # Natural but consistent\n                similarity_boost=0.8,\n                style=0.3,  # Slightly expressive\n                output_format=\"pcm_24000\",\n            )\n\n            if not result or not result.audio_data:\n                return None\n\n            audio_data = result.audio_data\n\n            # Calculate duration (PCM 24kHz, 16-bit mono)\n            duration_ms = int(len(audio_data) / 2 / 24000 * 1000)\n\n            audio = BackchannelAudio(\n                phrase=phrase,\n                voice_id=self.voice_id,\n                audio_data=audio_data,\n                format=\"pcm_24000\",\n                duration_ms=duration_ms,\n            )\n\n            self._cache.put(audio)\n            return audio\n\n        except Exception as e:\n            logger.error(f\"Failed to generate backchannel audio: {e}\")\n            return None\n\n        finally:\n            self._cache.unmark_generating(phrase, self.voice_id)\n\n\n# ==============================================================================\n# Backchannel Service\n# ==============================================================================\n\n\nclass BackchannelService:\n    \"\"\"\n    Factory service for creating backchannel sessions.\n\n    Manages shared audio cache and ElevenLabs integration.\n    Supports emotion-aware phrase selection via event bus integration.\n\n    A/B Testing:\n        When policy_service is provided, the service checks the\n        \"emotion_aware_backchannels\" A/B test to determine whether\n        to enable emotion-aware phrase selection for each session.\n    \"\"\"\n\n    def __init__(self, event_bus: Optional[Any] = None, policy_service: Optional[Any] = None):\n        self._cache = BackchannelAudioCache()\n        self._sessions: Dict[str, BackchannelSession] = {}\n        self._elevenlabs: Optional[ElevenLabsService] = None\n        self._event_bus = event_bus\n        self._policy_service = policy_service\n        self._enabled = bool(settings.ELEVENLABS_API_KEY)\n\n        if self._enabled:\n            self._elevenlabs = ElevenLabsService()\n            logger.info(\"Backchannel service initialized\")\n        else:\n            logger.info(\"Backchannel service disabled (no ELEVENLABS_API_KEY)\")\n\n    def set_event_bus(self, event_bus: Any) -> None:\n        \"\"\"Set or update the event bus for emotion event integration\"\"\"\n        self._event_bus = event_bus\n        logger.debug(\"Backchannel service event bus updated\")\n\n    def is_enabled(self) -> bool:\n        \"\"\"Check if backchanneling is available.\"\"\"\n        return self._enabled\n\n    def set_policy_service(self, policy_service: Any) -> None:\n        \"\"\"Set or update the policy service for A/B testing\"\"\"\n        self._policy_service = policy_service\n        logger.debug(\"Backchannel service policy service updated\")\n\n    def _should_use_emotion_aware(self, user_id: Optional[str]) -> bool:\n        \"\"\"\n        Determine if emotion-aware backchannels should be used.\n\n        Checks A/B test variant via policy service if available.\n        \"\"\"\n        if not self._policy_service or not user_id:\n            # Default to enabled when no A/B testing\n            return True\n\n        # Check A/B test variant\n        variant = self._policy_service.get_variant(\"emotion_aware_backchannels\", user_id)\n        if variant == \"static\":\n            return False  # Control group: no emotion awareness\n        elif variant == \"emotion_aware\":\n            return True  # Treatment group: emotion aware\n\n        # Check feature flag as fallback\n        return self._policy_service.is_feature_enabled(\"emotion_aware_backchannels\", user_id)\n\n    async def create_session(\n        self,\n        session_id: str,\n        voice_id: str,\n        language: str = \"en\",\n        on_backchannel: Optional[Callable[[BackchannelAudio], Awaitable[None]]] = None,\n        user_id: Optional[str] = None,\n    ) -> Optional[BackchannelSession]:\n        \"\"\"\n        Create a new backchannel session.\n\n        Args:\n            session_id: Unique session identifier\n            voice_id: ElevenLabs voice ID for TTS\n            language: Language code (en, ar, es, fr)\n            on_backchannel: Callback for backchannel triggers\n            user_id: User ID for A/B test variant determination\n\n        Returns:\n            BackchannelSession or None if disabled\n        \"\"\"\n        if not self._enabled:\n            logger.debug(\"Backchanneling disabled\")\n            return None\n\n        # Determine A/B test variant for emotion-aware backchannels\n        use_emotion_aware = self._should_use_emotion_aware(user_id)\n\n        session = BackchannelSession(\n            session_id=session_id,\n            voice_id=voice_id,\n            language=language,\n            elevenlabs_service=self._elevenlabs,\n            audio_cache=self._cache,\n            on_backchannel=on_backchannel,\n            event_bus=self._event_bus,\n            use_emotion_aware=use_emotion_aware,\n        )\n\n        self._sessions[session_id] = session\n        await session.start()\n\n        # Log A/B test assignment\n        logger.info(f\"Backchannel session created: {session_id}, \" f\"emotion_aware={use_emotion_aware}, user={user_id}\")\n\n        return session\n\n    async def remove_session(self, session_id: str) -> None:\n        \"\"\"Remove and cleanup a backchannel session.\"\"\"\n        session = self._sessions.pop(session_id, None)\n        if session:\n            await session.stop()\n\n    def get_session(self, session_id: str) -> Optional[BackchannelSession]:\n        \"\"\"Get an active session by ID.\"\"\"\n        return self._sessions.get(session_id)\n\n    def get_available_phrases(self, language: str = \"en\") -> List[Dict]:\n        \"\"\"Get available backchannel phrases for a language.\"\"\"\n        phrases = BACKCHANNEL_PHRASES.get(language, BACKCHANNEL_PHRASES[\"en\"])\n        return [\n            {\n                \"text\": p.text,\n                \"type\": p.type.value,\n                \"language\": p.language,\n            }\n            for p in phrases\n        ]\n\n    def get_cache_stats(self) -> Dict:\n        \"\"\"Get cache statistics.\"\"\"\n        return self._cache.get_cache_stats()\n\n\n# Global service instance\nbackchannel_service = BackchannelService()\n"
}
