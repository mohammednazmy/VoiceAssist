{
  "path": "services/api-gateway/app/services/emotion_detection_service.py",
  "language": "python",
  "size": 26915,
  "last_modified": "2025-12-04T12:32:40.271Z",
  "lines": 774,
  "content": "\"\"\"\nEmotion Detection Service - Hume AI Integration\n\nProvides real-time emotion detection from audio for voice mode.\nUses Hume AI's Expression Measurement API to analyze:\n- Valence (positive/negative emotional state)\n- Arousal (energy/activation level)\n- Discrete emotions (joy, sadness, frustration, etc.)\n\nFeatures:\n- Async audio chunk analysis (parallel to STT)\n- Emotion state caching for trending\n- Response style mapping for TTS adaptation\n\nPhase: Voice Mode Emotional Intelligence Enhancement\n\"\"\"\n\nimport asyncio\nimport base64\nimport time\nfrom collections import deque\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nfrom typing import Awaitable, Callable, Dict, List, Optional\n\nimport httpx\nfrom app.core.config import settings\nfrom app.core.logging import get_logger\n\nlogger = get_logger(__name__)\n\n\n# ==============================================================================\n# Data Classes and Enums\n# ==============================================================================\n\n\nclass EmotionCategory(str, Enum):\n    \"\"\"Discrete emotion categories detected from voice.\"\"\"\n\n    NEUTRAL = \"neutral\"\n    JOY = \"joy\"\n    SADNESS = \"sadness\"\n    ANGER = \"anger\"\n    FEAR = \"fear\"\n    SURPRISE = \"surprise\"\n    DISGUST = \"disgust\"\n    CONTEMPT = \"contempt\"\n    FRUSTRATION = \"frustration\"\n    ANXIETY = \"anxiety\"\n    CONFUSION = \"confusion\"\n    EXCITEMENT = \"excitement\"\n    INTEREST = \"interest\"\n    BOREDOM = \"boredom\"\n\n\n@dataclass\nclass EmotionResult:\n    \"\"\"Result of emotion analysis on an audio chunk.\"\"\"\n\n    # Primary emotion detected\n    primary_emotion: EmotionCategory\n    primary_confidence: float  # 0-1\n\n    # Dimensional measures\n    valence: float  # -1 to 1 (negative to positive)\n    arousal: float  # 0 to 1 (calm to excited)\n\n    # All detected emotions with confidence scores\n    emotions: Dict[str, float] = field(default_factory=dict)\n\n    # Metadata\n    timestamp: float = field(default_factory=time.time)\n    audio_duration_ms: int = 0\n\n    def to_dict(self) -> Dict:\n        \"\"\"Convert to dictionary for WebSocket messages.\"\"\"\n        return {\n            \"primary_emotion\": self.primary_emotion.value,\n            \"primary_confidence\": round(self.primary_confidence, 3),\n            \"valence\": round(self.valence, 3),\n            \"arousal\": round(self.arousal, 3),\n            \"emotions\": {k: round(v, 3) for k, v in self.emotions.items()},\n            \"timestamp\": self.timestamp,\n        }\n\n\n@dataclass\nclass EmotionTrend:\n    \"\"\"Trend analysis of emotions over a time window.\"\"\"\n\n    # Average values over window\n    avg_valence: float\n    avg_arousal: float\n\n    # Dominant emotion in window\n    dominant_emotion: EmotionCategory\n    dominant_confidence: float\n\n    # Trend direction\n    valence_trend: str  # \"increasing\", \"decreasing\", \"stable\"\n    arousal_trend: str\n\n    # Window metadata\n    sample_count: int\n    window_duration_ms: int\n\n    def to_dict(self) -> Dict:\n        \"\"\"Convert to dictionary.\"\"\"\n        return {\n            \"avg_valence\": round(self.avg_valence, 3),\n            \"avg_arousal\": round(self.avg_arousal, 3),\n            \"dominant_emotion\": self.dominant_emotion.value,\n            \"dominant_confidence\": round(self.dominant_confidence, 3),\n            \"valence_trend\": self.valence_trend,\n            \"arousal_trend\": self.arousal_trend,\n            \"sample_count\": self.sample_count,\n        }\n\n\n# ==============================================================================\n# Response Style Mapping\n# ==============================================================================\n\n\n@dataclass\nclass ResponseStyleAdjustment:\n    \"\"\"Adjustments to response style based on detected emotion.\"\"\"\n\n    # TTS parameters\n    stability_modifier: float  # Add to base stability (-0.2 to 0.2)\n    speech_rate_modifier: float  # Multiply base rate (0.8 to 1.2)\n    style_modifier: float  # Add to base style (-0.2 to 0.2)\n\n    # LLM response guidance\n    tone_instruction: str  # Instruction to add to system prompt\n    response_length_hint: str  # \"brief\", \"normal\", \"detailed\"\n\n    # Priority flags\n    prioritize_empathy: bool = False\n    prioritize_clarity: bool = False\n    prioritize_urgency: bool = False\n\n\n# Emotion to response style mapping\nEMOTION_STYLE_MAP: Dict[EmotionCategory, ResponseStyleAdjustment] = {\n    EmotionCategory.NEUTRAL: ResponseStyleAdjustment(\n        stability_modifier=0.0,\n        speech_rate_modifier=1.0,\n        style_modifier=0.0,\n        tone_instruction=\"Respond in a calm, professional manner.\",\n        response_length_hint=\"normal\",\n    ),\n    EmotionCategory.FRUSTRATION: ResponseStyleAdjustment(\n        stability_modifier=0.1,  # More stable = calming\n        speech_rate_modifier=0.9,  # Slower = more patient\n        style_modifier=-0.1,  # Less expressive = calming\n        tone_instruction=\"The user seems frustrated. Respond with patience, acknowledge their concern, and provide clear, helpful guidance.\",\n        response_length_hint=\"brief\",\n        prioritize_empathy=True,\n    ),\n    EmotionCategory.ANXIETY: ResponseStyleAdjustment(\n        stability_modifier=0.15,\n        speech_rate_modifier=0.85,\n        style_modifier=-0.05,\n        tone_instruction=\"The user seems anxious. Respond calmly and reassuringly, breaking down information into simple steps.\",\n        response_length_hint=\"brief\",\n        prioritize_empathy=True,\n        prioritize_clarity=True,\n    ),\n    EmotionCategory.CONFUSION: ResponseStyleAdjustment(\n        stability_modifier=0.05,\n        speech_rate_modifier=0.9,\n        style_modifier=0.0,\n        tone_instruction=\"The user seems confused. Provide clear, structured explanations and offer to clarify.\",\n        response_length_hint=\"detailed\",\n        prioritize_clarity=True,\n    ),\n    EmotionCategory.EXCITEMENT: ResponseStyleAdjustment(\n        stability_modifier=-0.1,  # More dynamic\n        speech_rate_modifier=1.1,  # Slightly faster\n        style_modifier=0.1,  # More expressive\n        tone_instruction=\"The user seems excited. Match their energy while staying helpful and professional.\",\n        response_length_hint=\"normal\",\n    ),\n    EmotionCategory.SADNESS: ResponseStyleAdjustment(\n        stability_modifier=0.1,\n        speech_rate_modifier=0.85,\n        style_modifier=-0.1,\n        tone_instruction=\"The user seems sad. Respond with warmth and empathy, acknowledging their feelings.\",\n        response_length_hint=\"brief\",\n        prioritize_empathy=True,\n    ),\n    EmotionCategory.ANGER: ResponseStyleAdjustment(\n        stability_modifier=0.15,\n        speech_rate_modifier=0.85,\n        style_modifier=-0.15,\n        tone_instruction=\"The user seems upset. Respond calmly without being defensive, acknowledge their concern, and focus on solutions.\",\n        response_length_hint=\"brief\",\n        prioritize_empathy=True,\n    ),\n    EmotionCategory.FEAR: ResponseStyleAdjustment(\n        stability_modifier=0.15,\n        speech_rate_modifier=0.8,\n        style_modifier=-0.1,\n        tone_instruction=\"The user seems worried or fearful. Respond with reassurance, provide accurate information, and offer support.\",\n        response_length_hint=\"detailed\",\n        prioritize_empathy=True,\n        prioritize_clarity=True,\n    ),\n    EmotionCategory.JOY: ResponseStyleAdjustment(\n        stability_modifier=-0.05,\n        speech_rate_modifier=1.05,\n        style_modifier=0.1,\n        tone_instruction=\"The user seems happy. Respond warmly and share in their positive energy.\",\n        response_length_hint=\"normal\",\n    ),\n    EmotionCategory.INTEREST: ResponseStyleAdjustment(\n        stability_modifier=0.0,\n        speech_rate_modifier=1.0,\n        style_modifier=0.05,\n        tone_instruction=\"The user seems engaged and interested. Provide thorough, informative responses.\",\n        response_length_hint=\"detailed\",\n    ),\n    EmotionCategory.BOREDOM: ResponseStyleAdjustment(\n        stability_modifier=-0.05,\n        speech_rate_modifier=1.1,\n        style_modifier=0.1,\n        tone_instruction=\"The user may be losing interest. Keep responses concise and engaging.\",\n        response_length_hint=\"brief\",\n    ),\n    EmotionCategory.SURPRISE: ResponseStyleAdjustment(\n        stability_modifier=0.0,\n        speech_rate_modifier=1.0,\n        style_modifier=0.0,\n        tone_instruction=\"Respond naturally to the user's surprise.\",\n        response_length_hint=\"normal\",\n    ),\n    EmotionCategory.DISGUST: ResponseStyleAdjustment(\n        stability_modifier=0.1,\n        speech_rate_modifier=0.95,\n        style_modifier=-0.05,\n        tone_instruction=\"Acknowledge the user's reaction and respond professionally.\",\n        response_length_hint=\"brief\",\n    ),\n    EmotionCategory.CONTEMPT: ResponseStyleAdjustment(\n        stability_modifier=0.1,\n        speech_rate_modifier=0.9,\n        style_modifier=-0.1,\n        tone_instruction=\"Respond professionally and focus on being helpful, regardless of the user's tone.\",\n        response_length_hint=\"brief\",\n    ),\n}\n\n\n# ==============================================================================\n# Hume AI Session\n# ==============================================================================\n\n\nclass EmotionDetectionSession:\n    \"\"\"\n    A session for continuous emotion detection during voice interaction.\n\n    Manages:\n    - Async audio chunk analysis\n    - Emotion state history\n    - Trend calculation\n    \"\"\"\n\n    # Window size for trend analysis\n    TREND_WINDOW_SIZE = 10\n    TREND_TIME_WINDOW_MS = 30000  # 30 seconds\n\n    def __init__(\n        self,\n        session_id: str,\n        api_key: str,\n        secret_key: Optional[str] = None,\n        on_emotion: Optional[Callable[[EmotionResult], Awaitable[None]]] = None,\n    ):\n        self.session_id = session_id\n        self._api_key = api_key\n        self._secret_key = secret_key\n        self._on_emotion = on_emotion\n\n        # Build authorization header\n        # Hume AI supports both API key only and API key + secret key auth\n        auth_headers = {\n            \"X-Hume-Api-Key\": api_key,\n            \"Content-Type\": \"application/json\",\n        }\n        if secret_key:\n            # When using secret key, use Bearer token format\n            auth_headers[\"Authorization\"] = f\"Bearer {api_key}\"\n\n        # HTTP client for Hume API\n        self._client = httpx.AsyncClient(\n            timeout=httpx.Timeout(5.0, connect=2.0),\n            headers=auth_headers,\n        )\n\n        # Emotion history for trending\n        self._emotion_history: deque[EmotionResult] = deque(maxlen=self.TREND_WINDOW_SIZE)\n\n        # Current emotion state\n        self._current_emotion: Optional[EmotionResult] = None\n\n        # Analysis queue for batching\n        self._audio_buffer: bytes = b\"\"\n        self._buffer_start_time: float = 0\n        self._min_buffer_ms = 500  # Analyze every 500ms of audio\n\n        # State\n        self._active = False\n\n    @property\n    def current_emotion(self) -> Optional[EmotionResult]:\n        \"\"\"Get the most recent emotion result.\"\"\"\n        return self._current_emotion\n\n    async def start(self) -> None:\n        \"\"\"Start the emotion detection session.\"\"\"\n        self._active = True\n        self._buffer_start_time = time.time()\n        logger.info(f\"Emotion detection session started: {self.session_id}\")\n\n    async def stop(self) -> None:\n        \"\"\"Stop the session and cleanup.\"\"\"\n        self._active = False\n\n        # Process any remaining audio\n        if self._audio_buffer:\n            await self._analyze_buffer()\n\n        await self._client.aclose()\n        logger.info(f\"Emotion detection session stopped: {self.session_id}\")\n\n    async def add_audio(self, audio_chunk: bytes, sample_rate: int = 16000) -> None:\n        \"\"\"\n        Add audio chunk for emotion analysis.\n\n        Buffers audio and triggers analysis every min_buffer_ms.\n        \"\"\"\n        if not self._active:\n            return\n\n        self._audio_buffer += audio_chunk\n\n        # Calculate buffer duration\n        # PCM16 = 2 bytes per sample\n        samples = len(self._audio_buffer) // 2\n        duration_ms = (samples / sample_rate) * 1000\n\n        if duration_ms >= self._min_buffer_ms:\n            # Trigger async analysis without blocking\n            asyncio.create_task(self._analyze_buffer())\n\n    async def _analyze_buffer(self) -> None:\n        \"\"\"Analyze buffered audio and emit emotion result.\"\"\"\n        if not self._audio_buffer:\n            return\n\n        audio_data = self._audio_buffer\n        buffer_duration_ms = int((len(audio_data) / 2 / 16000) * 1000)\n        self._audio_buffer = b\"\"\n        self._buffer_start_time = time.time()\n\n        try:\n            # Encode audio as base64 for Hume API\n            audio_b64 = base64.b64encode(audio_data).decode()\n\n            # Call Hume AI Expression Measurement API\n            result = await self._call_hume_api(audio_b64, buffer_duration_ms)\n\n            if result:\n                self._current_emotion = result\n                self._emotion_history.append(result)\n\n                # Emit to callback\n                if self._on_emotion:\n                    await self._on_emotion(result)\n\n        except Exception as e:\n            logger.warning(f\"Emotion analysis failed: {e}\")\n\n    async def _call_hume_api(\n        self,\n        audio_b64: str,\n        duration_ms: int,\n    ) -> Optional[EmotionResult]:\n        \"\"\"\n        Call Hume AI Expression Measurement API.\n\n        Uses the streaming prosody model for real-time analysis.\n        \"\"\"\n        try:\n            # Hume AI Expression Measurement API endpoint\n            url = \"https://api.hume.ai/v0/batch/jobs\"\n\n            # Build request payload\n            payload = {\n                \"models\": {\"prosody\": {}},  # Analyze voice prosody for emotions\n                \"urls\": [],  # We're sending raw audio\n                \"text\": [],\n                \"files\": [\n                    {\n                        \"data\": audio_b64,\n                        \"filename\": \"audio.wav\",\n                        \"content_type\": \"audio/wav\",\n                    }\n                ],\n            }\n\n            # For real-time analysis, use the streaming endpoint\n            # Fall back to batch if streaming not available\n            stream_url = \"https://api.hume.ai/v0/stream/models\"\n\n            stream_payload = {\n                \"data\": audio_b64,\n                \"models\": {\"prosody\": {}},\n                \"raw_text\": False,\n            }\n\n            response = await self._client.post(\n                stream_url,\n                json=stream_payload,\n            )\n\n            if response.status_code == 200:\n                data = response.json()\n                return self._parse_hume_response(data, duration_ms)\n\n            elif response.status_code == 401:\n                logger.error(\"Hume AI authentication failed - check API key\")\n                return None\n\n            else:\n                logger.warning(f\"Hume API returned {response.status_code}: {response.text[:200]}\")\n                return None\n\n        except httpx.TimeoutException:\n            logger.warning(\"Hume API timeout - skipping emotion analysis\")\n            return None\n\n        except Exception as e:\n            logger.error(f\"Hume API call failed: {e}\")\n            return None\n\n    def _parse_hume_response(\n        self,\n        data: Dict,\n        duration_ms: int,\n    ) -> Optional[EmotionResult]:\n        \"\"\"Parse Hume AI response into EmotionResult.\"\"\"\n        try:\n            # Hume prosody response structure\n            predictions = data.get(\"prosody\", {}).get(\"predictions\", [])\n\n            if not predictions:\n                return EmotionResult(\n                    primary_emotion=EmotionCategory.NEUTRAL,\n                    primary_confidence=0.5,\n                    valence=0.0,\n                    arousal=0.5,\n                    audio_duration_ms=duration_ms,\n                )\n\n            # Aggregate emotions from predictions\n            emotion_scores: Dict[str, List[float]] = {}\n\n            for pred in predictions:\n                emotions = pred.get(\"emotions\", [])\n                for emotion in emotions:\n                    name = emotion.get(\"name\", \"\").lower()\n                    score = emotion.get(\"score\", 0.0)\n                    if name not in emotion_scores:\n                        emotion_scores[name] = []\n                    emotion_scores[name].append(score)\n\n            # Average scores\n            avg_emotions = {name: sum(scores) / len(scores) for name, scores in emotion_scores.items() if scores}\n\n            # Map Hume emotions to our categories\n            emotion_mapping = {\n                \"joy\": EmotionCategory.JOY,\n                \"sadness\": EmotionCategory.SADNESS,\n                \"anger\": EmotionCategory.ANGER,\n                \"fear\": EmotionCategory.FEAR,\n                \"surprise\": EmotionCategory.SURPRISE,\n                \"disgust\": EmotionCategory.DISGUST,\n                \"contempt\": EmotionCategory.CONTEMPT,\n                \"excitement\": EmotionCategory.EXCITEMENT,\n                \"interest\": EmotionCategory.INTEREST,\n                \"boredom\": EmotionCategory.BOREDOM,\n                \"confusion\": EmotionCategory.CONFUSION,\n                \"anxiety\": EmotionCategory.ANXIETY,\n                \"frustration\": EmotionCategory.FRUSTRATION,\n                # Aliases\n                \"disappointment\": EmotionCategory.FRUSTRATION,\n                \"annoyance\": EmotionCategory.FRUSTRATION,\n                \"nervousness\": EmotionCategory.ANXIETY,\n                \"worry\": EmotionCategory.ANXIETY,\n            }\n\n            # Find primary emotion\n            primary_emotion = EmotionCategory.NEUTRAL\n            primary_confidence = 0.5\n\n            for hume_name, score in sorted(avg_emotions.items(), key=lambda x: x[1], reverse=True):\n                mapped = emotion_mapping.get(hume_name)\n                if mapped and score > primary_confidence:\n                    primary_emotion = mapped\n                    primary_confidence = score\n                    break\n\n            # Calculate valence and arousal from emotion mix\n            valence = self._calculate_valence(avg_emotions)\n            arousal = self._calculate_arousal(avg_emotions)\n\n            return EmotionResult(\n                primary_emotion=primary_emotion,\n                primary_confidence=primary_confidence,\n                valence=valence,\n                arousal=arousal,\n                emotions=avg_emotions,\n                audio_duration_ms=duration_ms,\n            )\n\n        except Exception as e:\n            logger.error(f\"Failed to parse Hume response: {e}\")\n            return None\n\n    def _calculate_valence(self, emotions: Dict[str, float]) -> float:\n        \"\"\"\n        Calculate valence (-1 to 1) from emotion mix.\n\n        Positive emotions increase valence, negative decrease it.\n        \"\"\"\n        positive_emotions = [\n            \"joy\",\n            \"excitement\",\n            \"interest\",\n            \"amusement\",\n            \"contentment\",\n        ]\n        negative_emotions = [\n            \"sadness\",\n            \"anger\",\n            \"fear\",\n            \"disgust\",\n            \"contempt\",\n            \"frustration\",\n            \"anxiety\",\n        ]\n\n        positive_sum = sum(emotions.get(e, 0) for e in positive_emotions)\n        negative_sum = sum(emotions.get(e, 0) for e in negative_emotions)\n\n        total = positive_sum + negative_sum\n        if total == 0:\n            return 0.0\n\n        # Scale to -1 to 1\n        valence = (positive_sum - negative_sum) / max(total, 1)\n        return max(-1.0, min(1.0, valence))\n\n    def _calculate_arousal(self, emotions: Dict[str, float]) -> float:\n        \"\"\"\n        Calculate arousal (0 to 1) from emotion mix.\n\n        High-energy emotions increase arousal, low-energy decrease it.\n        \"\"\"\n        high_arousal = [\"excitement\", \"anger\", \"fear\", \"surprise\", \"joy\"]\n        low_arousal = [\"sadness\", \"boredom\", \"contentment\", \"tiredness\"]\n\n        high_sum = sum(emotions.get(e, 0) for e in high_arousal)\n        low_sum = sum(emotions.get(e, 0) for e in low_arousal)\n\n        total = high_sum + low_sum\n        if total == 0:\n            return 0.5\n\n        # Scale to 0 to 1\n        arousal = high_sum / max(total, 1)\n        return max(0.0, min(1.0, arousal))\n\n    def get_trend(self) -> Optional[EmotionTrend]:\n        \"\"\"Calculate emotion trend from history.\"\"\"\n        if len(self._emotion_history) < 2:\n            return None\n\n        history = list(self._emotion_history)\n\n        # Calculate averages\n        avg_valence = sum(e.valence for e in history) / len(history)\n        avg_arousal = sum(e.arousal for e in history) / len(history)\n\n        # Calculate trends\n        if len(history) >= 3:\n            recent = history[-3:]\n            older = history[:-3] if len(history) > 3 else history[:1]\n\n            recent_valence = sum(e.valence for e in recent) / len(recent)\n            older_valence = sum(e.valence for e in older) / len(older)\n            valence_diff = recent_valence - older_valence\n\n            recent_arousal = sum(e.arousal for e in recent) / len(recent)\n            older_arousal = sum(e.arousal for e in older) / len(older)\n            arousal_diff = recent_arousal - older_arousal\n\n            valence_trend = \"stable\"\n            if valence_diff > 0.1:\n                valence_trend = \"increasing\"\n            elif valence_diff < -0.1:\n                valence_trend = \"decreasing\"\n\n            arousal_trend = \"stable\"\n            if arousal_diff > 0.1:\n                arousal_trend = \"increasing\"\n            elif arousal_diff < -0.1:\n                arousal_trend = \"decreasing\"\n        else:\n            valence_trend = \"stable\"\n            arousal_trend = \"stable\"\n\n        # Find dominant emotion\n        emotion_counts: Dict[EmotionCategory, float] = {}\n        for e in history:\n            emotion_counts[e.primary_emotion] = emotion_counts.get(e.primary_emotion, 0) + e.primary_confidence\n\n        dominant_emotion = max(emotion_counts, key=emotion_counts.get) if emotion_counts else EmotionCategory.NEUTRAL\n        dominant_confidence = emotion_counts.get(dominant_emotion, 0) / len(history)\n\n        # Calculate window duration\n        if history:\n            window_duration_ms = int((history[-1].timestamp - history[0].timestamp) * 1000)\n        else:\n            window_duration_ms = 0\n\n        return EmotionTrend(\n            avg_valence=avg_valence,\n            avg_arousal=avg_arousal,\n            dominant_emotion=dominant_emotion,\n            dominant_confidence=dominant_confidence,\n            valence_trend=valence_trend,\n            arousal_trend=arousal_trend,\n            sample_count=len(history),\n            window_duration_ms=window_duration_ms,\n        )\n\n\n# ==============================================================================\n# Emotion Detection Service\n# ==============================================================================\n\n\nclass EmotionDetectionService:\n    \"\"\"\n    Factory service for creating emotion detection sessions.\n\n    Manages API key validation and session lifecycle.\n    \"\"\"\n\n    def __init__(self):\n        self._api_key = settings.HUME_API_KEY\n        self._secret_key = settings.HUME_SECRET_KEY\n        self._enabled = settings.HUME_ENABLED and bool(self._api_key)\n        self._sessions: Dict[str, EmotionDetectionSession] = {}\n\n        if self._enabled:\n            logger.info(\"Emotion detection service initialized with Hume AI\")\n        else:\n            logger.info(\"Emotion detection service disabled (no HUME_API_KEY)\")\n\n    def is_enabled(self) -> bool:\n        \"\"\"Check if emotion detection is available.\"\"\"\n        return self._enabled\n\n    async def create_session(\n        self,\n        session_id: str,\n        on_emotion: Optional[Callable[[EmotionResult], Awaitable[None]]] = None,\n    ) -> Optional[EmotionDetectionSession]:\n        \"\"\"\n        Create a new emotion detection session.\n\n        Args:\n            session_id: Unique session identifier\n            on_emotion: Callback for emotion detection results\n\n        Returns:\n            EmotionDetectionSession or None if disabled\n        \"\"\"\n        if not self._enabled:\n            logger.debug(\"Emotion detection disabled, skipping session creation\")\n            return None\n\n        session = EmotionDetectionSession(\n            session_id=session_id,\n            api_key=self._api_key,\n            secret_key=self._secret_key,\n            on_emotion=on_emotion,\n        )\n\n        self._sessions[session_id] = session\n        await session.start()\n\n        logger.info(f\"Created emotion detection session: {session_id}\")\n        return session\n\n    async def remove_session(self, session_id: str) -> None:\n        \"\"\"Remove and cleanup an emotion detection session.\"\"\"\n        session = self._sessions.pop(session_id, None)\n        if session:\n            await session.stop()\n            logger.info(f\"Removed emotion detection session: {session_id}\")\n\n    def get_session(self, session_id: str) -> Optional[EmotionDetectionSession]:\n        \"\"\"Get an active session by ID.\"\"\"\n        return self._sessions.get(session_id)\n\n    def get_response_style(\n        self,\n        emotion: EmotionCategory,\n    ) -> ResponseStyleAdjustment:\n        \"\"\"Get response style adjustment for an emotion.\"\"\"\n        return EMOTION_STYLE_MAP.get(\n            emotion,\n            EMOTION_STYLE_MAP[EmotionCategory.NEUTRAL],\n        )\n\n    def build_emotion_context_prompt(\n        self,\n        emotion: Optional[EmotionResult],\n        trend: Optional[EmotionTrend] = None,\n    ) -> str:\n        \"\"\"\n        Build a context prompt for the LLM based on detected emotion.\n\n        This is injected into the system prompt to guide response tone.\n        \"\"\"\n        if not emotion:\n            return \"\"\n\n        style = self.get_response_style(emotion.primary_emotion)\n\n        parts = [\n            \"\\n[EMOTIONAL CONTEXT]\",\n            f\"User emotional state: {emotion.primary_emotion.value} (confidence: {emotion.primary_confidence:.0%})\",\n            f\"Guidance: {style.tone_instruction}\",\n        ]\n\n        if trend:\n            if trend.valence_trend == \"decreasing\":\n                parts.append(\"Note: User's mood appears to be declining. Extra empathy may help.\")\n            elif trend.arousal_trend == \"increasing\" and emotion.arousal > 0.7:\n                parts.append(\"Note: User's energy is rising. Match their pace while staying helpful.\")\n\n        if style.prioritize_empathy:\n            parts.append(\"Priority: Show empathy and understanding.\")\n        if style.prioritize_clarity:\n            parts.append(\"Priority: Be extra clear and structured.\")\n        if style.prioritize_urgency:\n            parts.append(\"Priority: Respond with appropriate urgency.\")\n\n        parts.append(\"[END EMOTIONAL CONTEXT]\\n\")\n\n        return \"\\n\".join(parts)\n\n\n# Global service instance\nemotion_detection_service = EmotionDetectionService()\n"
}
