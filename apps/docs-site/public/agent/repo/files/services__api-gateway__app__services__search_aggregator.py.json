{
  "path": "services/api-gateway/app/services/search_aggregator.py",
  "language": "python",
  "size": 18584,
  "last_modified": "2025-12-04T11:27:00.317Z",
  "lines": 506,
  "content": "\"\"\"\nSearch Aggregator Service (Phase 5 MVP)\n\nPerforms semantic search across the knowledge base using Qdrant vector database.\nReturns relevant document chunks with scores and metadata for RAG.\n\nMVP Implementation:\n- Vector similarity search using Qdrant\n- Configurable top-K results\n- Score-based filtering\n- Result formatting for RAG\n- Multi-level caching (Phase 7 - P2.1)\n\nEnhancements (hardening pass):\n- Timeouts and retries around OpenAI embeddings and Qdrant calls\n- Async offloading of blocking Qdrant client operations\n\nFuture enhancements:\n- Multi-vector search (dense + sparse)\n- Hybrid search (vector + keyword)\n- Query expansion\n- Result re-ranking\n\"\"\"\n\nimport asyncio\nimport logging\nimport time\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List, Optional\n\nfrom app.core.config import settings\nfrom openai import AsyncOpenAI\n\n# Async OpenAI client for embedding generation\n_async_openai_client: AsyncOpenAI | None = None\n\n\ndef get_async_openai_client() -> AsyncOpenAI:\n    \"\"\"Get or create the async OpenAI client.\"\"\"\n    global _async_openai_client\n    if _async_openai_client is None:\n        _async_openai_client = AsyncOpenAI()\n    return _async_openai_client\n\n\n# isort: off\nfrom app.core.metrics import (  # noqa: E402\n    rag_embedding_tokens_total,\n    rag_query_duration_seconds,\n    rag_search_results_total,\n)\nfrom app.services.cache_service import cache_service, generate_cache_key  # noqa: E402\nfrom qdrant_client import QdrantClient  # noqa: E402\nfrom qdrant_client.models import (  # noqa: E402\n    FieldCondition,\n    Filter,\n    MatchAny,\n    MatchValue,\n)\n\n# isort: on\n\nlogger = logging.getLogger(__name__)\n\n\n@dataclass\nclass SearchResult:\n    \"\"\"Represents a search result from the knowledge base.\"\"\"\n\n    chunk_id: str\n    document_id: str\n    content: str\n    score: float\n    metadata: Dict[str, Any]\n\n\nclass SearchAggregator:\n    \"\"\"\n    Search Aggregator for semantic search across medical knowledge base.\n\n    Handles query embedding generation and vector search in Qdrant.\n    \"\"\"\n\n    def __init__(\n        self,\n        qdrant_url: str = \"http://qdrant:6333\",\n        collection_name: str = \"medical_kb\",\n        embedding_model: str = \"text-embedding-3-small\",\n    ):\n        \"\"\"\n        Initialize Search Aggregator.\n\n        Args:\n            qdrant_url: Qdrant server URL\n            collection_name: Name of the collection to search\n            embedding_model: OpenAI embedding model to use\n        \"\"\"\n        # Check if Qdrant is enabled\n        self.qdrant_enabled = getattr(settings, \"QDRANT_ENABLED\", True)\n\n        # Qdrant client is sync; keep a short timeout and execute calls off the event loop\n        if self.qdrant_enabled:\n            self.qdrant_client = QdrantClient(url=qdrant_url, timeout=5.0)\n        else:\n            self.qdrant_client = None\n            logger.warning(\"Qdrant is disabled - search will return empty results\")\n\n        self.collection_name = collection_name\n        self.embedding_model = embedding_model\n\n    async def generate_query_embedding(self, query: str) -> List[float]:\n        \"\"\"\n        Generate embedding vector for search query with caching.\n\n        Args:\n            query: Search query text\n\n        Returns:\n            Query embedding vector\n        \"\"\"\n        # Simple exponential backoff to tolerate transient network/model issues\n        backoff_seconds = [1, 2, 4]\n\n        # Check cache first\n        cache_key = generate_cache_key(\"rag_embedding\", query, model=self.embedding_model)\n        cached_embedding = await cache_service.get(cache_key)\n        if cached_embedding is not None:\n            logger.debug(f\"Using cached embedding for query: {query[:50]}...\")\n            return cached_embedding\n\n        # Generate embedding using async client\n        last_error: Optional[Exception] = None\n        client = get_async_openai_client()\n        for attempt, delay in enumerate(backoff_seconds, start=1):\n            try:\n                start_time = time.time()\n                response = await asyncio.wait_for(\n                    client.embeddings.create(model=self.embedding_model, input=query),\n                    timeout=15,\n                )\n                embedding = response.data[0].embedding\n\n                # Track metrics\n                duration = time.time() - start_time\n                rag_query_duration_seconds.labels(stage=\"embedding\").observe(duration)\n                if getattr(response, \"usage\", None):\n                    rag_embedding_tokens_total.inc(response.usage.total_tokens)\n\n                # Cache the embedding (24 hour TTL - embeddings are stable)\n                await cache_service.set(cache_key, embedding, ttl=86400)\n                return embedding\n\n            except asyncio.TimeoutError as exc:\n                last_error = exc\n                logger.error(\"Embedding generation timed out (attempt %d)\", attempt)\n            except Exception as exc:\n                last_error = exc\n                logger.error(\n                    \"Error generating query embedding (attempt %d): %s\",\n                    attempt,\n                    exc,\n                    exc_info=True,\n                )\n\n            # Backoff before next attempt\n            await asyncio.sleep(delay)\n\n        # If all attempts fail, bubble up the last error\n        raise RuntimeError(f\"Failed to generate embedding after retries: {last_error}\") from last_error\n\n    async def search(\n        self,\n        query: str,\n        top_k: int = 5,\n        score_threshold: float = 0.7,\n        filter_conditions: Optional[Dict[str, Any]] = None,\n    ) -> List[SearchResult]:\n        \"\"\"\n        Perform semantic search against the knowledge base with caching.\n\n        Args:\n            query: Search query text\n            top_k: Number of top results to return\n            score_threshold: Minimum similarity score (0-1)\n            filter_conditions: Optional filters (e.g., source_type, document_id)\n\n        Returns:\n            List of search results sorted by relevance\n        \"\"\"\n        # Return empty results if Qdrant is disabled\n        if not self.qdrant_enabled or self.qdrant_client is None:\n            logger.debug(\"Qdrant disabled - returning empty search results\")\n            return []\n\n        backoff_seconds = [1, 2, 4]\n\n        # Check cache first\n        cache_key = generate_cache_key(\n            \"search_results\",\n            query,\n            top_k=top_k,\n            threshold=score_threshold,\n            filters=str(filter_conditions) if filter_conditions else \"none\",\n        )\n        cached_results = await cache_service.get(cache_key)\n        if cached_results is not None:\n            logger.debug(f\"Using cached search results for query: {query[:50]}...\")\n            rag_search_results_total.observe(len(cached_results))\n            return cached_results\n\n        last_error: Optional[Exception] = None\n        for attempt, delay in enumerate(backoff_seconds, start=1):\n            try:\n                start_time = time.time()\n\n                # Generate query embedding\n                query_embedding = await self.generate_query_embedding(query)\n\n                # Prepare filter if specified\n                search_filter = None\n                if filter_conditions:\n                    filter_must = []\n                    for key, value in filter_conditions.items():\n                        if isinstance(value, (list, tuple, set)):\n                            filter_must.append(FieldCondition(key=key, match=MatchAny(any=list(value))))\n                        else:\n                            filter_must.append(FieldCondition(key=key, match=MatchValue(value=value)))\n                    search_filter = Filter(must=filter_must)\n\n                # Perform vector search in Qdrant off the event loop\n                search_start = time.time()\n                search_results = await asyncio.wait_for(\n                    asyncio.to_thread(\n                        self.qdrant_client.search,\n                        collection_name=self.collection_name,\n                        query_vector=query_embedding,\n                        limit=top_k,\n                        score_threshold=score_threshold,\n                        query_filter=search_filter,\n                    ),\n                    timeout=5,\n                )\n                rag_query_duration_seconds.labels(stage=\"search\").observe(time.time() - search_start)\n\n                # Format results\n                results = []\n                for result in search_results:\n                    search_result = SearchResult(\n                        chunk_id=str(result.id),\n                        document_id=result.payload.get(\"document_id\", \"unknown\"),\n                        content=result.payload.get(\"content\", \"\"),\n                        score=result.score,\n                        metadata={\n                            \"title\": result.payload.get(\"title\", \"Untitled\"),\n                            \"source_type\": result.payload.get(\"source_type\", \"unknown\"),\n                            \"chunk_index\": result.payload.get(\"chunk_index\", 0),\n                            **{\n                                k: v\n                                for k, v in result.payload.items()\n                                if k\n                                not in [\n                                    \"content\",\n                                    \"document_id\",\n                                    \"title\",\n                                    \"source_type\",\n                                    \"chunk_index\",\n                                ]\n                            },\n                        },\n                    )\n                    results.append(search_result)\n\n                # Track metrics\n                rag_query_duration_seconds.labels(stage=\"total\").observe(time.time() - start_time)\n                rag_search_results_total.observe(len(results))\n\n                # Cache search results (30 minute TTL)\n                await cache_service.set(cache_key, results, ttl=1800)\n\n                logger.info(\n                    \"Found %d results for query (top_k=%d, threshold=%s)\",\n                    len(results),\n                    top_k,\n                    score_threshold,\n                )\n                return results\n\n            except asyncio.TimeoutError as exc:\n                last_error = exc\n                logger.error(\"Qdrant search timed out (attempt %d)\", attempt)\n            except Exception as exc:\n                last_error = exc\n                logger.error(\n                    \"Error performing search (attempt %d): %s\",\n                    attempt,\n                    exc,\n                    exc_info=True,\n                )\n\n            await asyncio.sleep(delay)\n\n        logger.error(\"Search failed after retries: %s\", last_error)\n        return []\n\n    async def search_by_document_id(self, document_id: str, top_k: int = 10) -> List[SearchResult]:\n        \"\"\"\n        Retrieve all chunks for a specific document.\n\n        Args:\n            document_id: Document identifier\n            top_k: Maximum number of chunks to return\n\n        Returns:\n            List of document chunks\n        \"\"\"\n        # Return empty results if Qdrant is disabled\n        if not self.qdrant_enabled or self.qdrant_client is None:\n            logger.debug(\"Qdrant disabled - returning empty document chunks\")\n            return []\n\n        backoff_seconds = [1, 2, 4]\n        last_error: Optional[Exception] = None\n\n        for attempt, delay in enumerate(backoff_seconds, start=1):\n            try:\n                # Search with document_id filter off the event loop\n                search_results = await asyncio.wait_for(\n                    asyncio.to_thread(\n                        self.qdrant_client.scroll,\n                        collection_name=self.collection_name,\n                        scroll_filter=Filter(\n                            must=[\n                                FieldCondition(\n                                    key=\"document_id\",\n                                    match=MatchValue(value=document_id),\n                                )\n                            ]\n                        ),\n                        limit=top_k,\n                    ),\n                    timeout=5,\n                )\n\n                # Format results\n                results = []\n                for result in search_results[0]:  # scroll returns (points, next_page_offset)\n                    search_result = SearchResult(\n                        chunk_id=str(result.id),\n                        document_id=result.payload.get(\"document_id\", \"unknown\"),\n                        content=result.payload.get(\"content\", \"\"),\n                        score=1.0,  # Not a similarity search\n                        metadata={\n                            \"title\": result.payload.get(\"title\", \"Untitled\"),\n                            \"source_type\": result.payload.get(\"source_type\", \"unknown\"),\n                            \"chunk_index\": result.payload.get(\"chunk_index\", 0),\n                            **{\n                                k: v\n                                for k, v in result.payload.items()\n                                if k\n                                not in [\n                                    \"content\",\n                                    \"document_id\",\n                                    \"title\",\n                                    \"source_type\",\n                                    \"chunk_index\",\n                                ]\n                            },\n                        },\n                    )\n                    results.append(search_result)\n\n                logger.info(\"Retrieved %d chunks for document %s\", len(results), document_id)\n                return results\n\n            except asyncio.TimeoutError as exc:\n                last_error = exc\n                logger.error(\n                    \"Qdrant scroll timed out for document %s (attempt %d)\",\n                    document_id,\n                    attempt,\n                )\n            except Exception as exc:\n                last_error = exc\n                logger.error(\n                    \"Error retrieving document chunks for %s (attempt %d): %s\",\n                    document_id,\n                    attempt,\n                    exc,\n                    exc_info=True,\n                )\n\n            await asyncio.sleep(delay)\n\n        logger.error(\"Failed to retrieve document %s after retries: %s\", document_id, last_error)\n        return []\n\n    def format_context_for_rag(self, search_results: List[SearchResult]) -> str:\n        \"\"\"\n        Format search results into context string for RAG.\n\n        Args:\n            search_results: List of search results\n\n        Returns:\n            Formatted context string\n        \"\"\"\n        if not search_results:\n            return \"\"\n\n        context_parts = []\n        for i, result in enumerate(search_results, 1):\n            source_type = result.metadata.get(\"source_type\", \"unknown\")\n            source_tag = source_type.upper()\n            context_parts.append(\n                f\"[Source {i} | {source_tag}] {result.metadata.get('title', 'Unknown')} \"\n                f\"(Score: {result.score:.2f})\\n{result.content}\\n\"\n            )\n\n        return \"\\n\".join(context_parts)\n\n    def extract_citations(self, search_results: List[SearchResult]) -> List[Dict[str, Any]]:\n        \"\"\"\n        Extract citation information from search results.\n\n        Args:\n            search_results: List of search results\n\n        Returns:\n            List of citation dictionaries\n        \"\"\"\n        citations = []\n        seen_documents = set()\n\n        for result in search_results:\n            # Only include each document once\n            if result.document_id in seen_documents:\n                continue\n\n            seen_documents.add(result.document_id)\n\n            citation = {\n                \"id\": result.document_id,\n                \"source_type\": result.metadata.get(\"source_type\", \"unknown\"),\n                \"title\": result.metadata.get(\"title\", \"Untitled\"),\n                \"url\": result.metadata.get(\"url\"),\n                \"relevance_score\": result.score,\n                \"source_tag\": result.metadata.get(\"source_type\", \"unknown\").upper(),\n            }\n            citations.append(citation)\n\n        return citations\n\n    def synthesize_across_documents(self, search_results: List[SearchResult]) -> Dict[str, Any]:\n        \"\"\"Create a cross-document synthesis summary.\n\n        Groups results by document and returns a condensed context string and\n        lightweight metadata for auditing.\n        \"\"\"\n\n        if not search_results:\n            return {\"context\": \"\", \"documents\": []}\n\n        grouped: Dict[str, Dict[str, Any]] = {}\n        for result in search_results:\n            doc_group = grouped.setdefault(\n                result.document_id,\n                {\n                    \"title\": result.metadata.get(\"title\", \"Untitled\"),\n                    \"score\": 0.0,\n                    \"chunks\": [],\n                },\n            )\n            doc_group[\"score\"] = max(doc_group[\"score\"], result.score)\n            doc_group[\"chunks\"].append(result.content)\n\n        context_sections: List[str] = []\n        documents_meta: List[Dict[str, Any]] = []\n\n        for idx, (document_id, doc) in enumerate(grouped.items(), start=1):\n            snippet = \" \".join(doc[\"chunks\"][:2])\n            snippet = (snippet[:600] + \"â€¦\") if len(snippet) > 600 else snippet\n            context_sections.append(f\"[Doc {idx}: {doc['title']}] {snippet}\")\n            documents_meta.append(\n                {\n                    \"document_id\": document_id,\n                    \"title\": doc[\"title\"],\n                    \"score\": doc[\"score\"],\n                    \"chunks\": len(doc[\"chunks\"]),\n                }\n            )\n\n        return {\"context\": \"\\n\\n\".join(context_sections), \"documents\": documents_meta}\n\n    def confidence_score(self, search_results: List[SearchResult]) -> float:\n        \"\"\"Estimate confidence for retrieval stage based on scores and coverage.\"\"\"\n\n        if not search_results:\n            return 0.2\n\n        top_scores = sorted([r.score for r in search_results], reverse=True)[:3]\n        avg_top = sum(top_scores) / len(top_scores)\n        doc_coverage = len({r.document_id for r in search_results})\n        coverage_boost = min(doc_coverage / 5, 1.0) * 0.2\n        normalized = min(max(avg_top, 0.0), 1.0)\n        return min(1.0, 0.6 * normalized + coverage_boost + 0.2)\n"
}
