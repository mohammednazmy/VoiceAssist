{
  "path": "services/api-gateway/app/services/sentence_chunker.py",
  "language": "python",
  "size": 16224,
  "last_modified": "2025-12-04T11:27:00.139Z",
  "lines": 479,
  "content": "\"\"\"\nPhrase Chunker for TTS (Low-Latency Optimized)\n\nIntelligently splits streaming LLM output into phrase-sized chunks\nfor optimal TTS processing with MINIMAL LATENCY.\n\nStrategy (Balanced - prioritizes speed while maintaining naturalness):\n- Primary: Split on sentence boundaries (. ! ?)\n- Secondary: Split on clause boundaries (, ; : —) after MIN_CHUNK_CHARS\n- Emergency: Force split at MAX_CHUNK_CHARS (80 chars for low latency)\n\nLatency Optimization: Reduced from sentence-level (100+ chars) to clause-level\n(~50 chars) to achieve 200-400ms faster time-to-first-audio.\n\nPhase: Voice Mode Latency Optimization\n\"\"\"\n\nfrom dataclasses import dataclass, field\nfrom typing import List, Optional\n\nfrom app.core.logging import get_logger\n\nlogger = get_logger(__name__)\n\n\n@dataclass\nclass ChunkerConfig:\n    \"\"\"\n    Configuration for the phrase chunker.\n\n    Latency-optimized defaults (Balanced approach):\n    - min_chunk_chars: 15 (avoid tiny fragments that sound choppy)\n    - optimal_chunk_chars: 50 (trigger clause splitting early for low latency)\n    - max_chunk_chars: 80 (force split before TTS buffer fills)\n\n    Previous sentence-level defaults were: 20, 100, 200\n    New phrase-level defaults save 200-400ms on time-to-first-audio.\n    \"\"\"\n\n    min_chunk_chars: int = 15  # Don't send tiny fragments (was 20)\n    optimal_chunk_chars: int = 50  # Start looking for clause breaks early (was 100)\n    max_chunk_chars: int = 80  # Force split for low latency (was 200)\n    abbreviations: List[str] = field(\n        default_factory=lambda: [\n            \"Dr.\",\n            \"Mr.\",\n            \"Mrs.\",\n            \"Ms.\",\n            \"Prof.\",\n            \"Sr.\",\n            \"Jr.\",\n            \"vs.\",\n            \"etc.\",\n            \"e.g.\",\n            \"i.e.\",\n            \"Fig.\",\n            \"fig.\",\n            \"vol.\",\n            \"Vol.\",\n            \"No.\",\n            \"no.\",\n            \"pp.\",\n            \"p.\",\n            \"U.S.\",\n            \"U.K.\",\n            \"U.N.\",\n            \"St.\",\n            \"Ave.\",\n            \"Blvd.\",\n        ]\n    )\n\n\n@dataclass\nclass AdaptiveChunkerConfig:\n    \"\"\"\n    Adaptive chunking configuration for optimal latency AND naturalness.\n\n    Strategy: Use small chunks for fast time-to-first-audio (TTFA),\n    then switch to larger natural chunks for better prosody.\n\n    This achieves both goals:\n    - Fast first response (~150ms TTFA vs ~400ms with large chunks)\n    - Natural sounding speech (full sentences after first chunk)\n    \"\"\"\n\n    # First chunk settings (optimized for fast TTFA)\n    first_chunk_min: int = 20  # Minimum for first chunk\n    first_chunk_optimal: int = 30  # Trigger clause split for first chunk\n    first_chunk_max: int = 50  # Force split first chunk by this point\n\n    # Subsequent chunk settings (optimized for naturalness)\n    subsequent_min: int = 40  # Meaningful phrases\n    subsequent_optimal: int = 120  # Full sentences for natural prosody\n    subsequent_max: int = 200  # Allow complete thoughts\n\n    # How many chunks before switching to natural mode\n    chunks_before_natural: int = 1\n\n    # Enable/disable adaptive behavior\n    enabled: bool = True\n\n\nclass SentenceChunker:\n    \"\"\"\n    Low-latency phrase chunker for TTS processing.\n\n    Optimized for minimal time-to-first-audio while maintaining natural prosody.\n\n    Features:\n    - Splits at clause boundaries (commas, semicolons, colons) for early TTS\n    - Falls back to sentence boundaries when found\n    - Force splits at 80 chars to prevent TTS buffer delays\n    - Handles abbreviations to avoid false splits\n    - Balanced approach: speed vs naturalness\n\n    Latency Impact:\n    - Previous (sentence-level): Wait for full sentences = 200-400ms delay\n    - New (phrase-level): Split at clauses ~50 chars = faster response\n\n    Usage:\n        chunker = SentenceChunker()\n\n        # During LLM streaming:\n        for token in llm_stream:\n            chunks = chunker.add_token(token)\n            for chunk in chunks:\n                await tts.speak(chunk)\n\n        # At stream end:\n        final_chunk = chunker.flush()\n        if final_chunk:\n            await tts.speak(final_chunk)\n    \"\"\"\n\n    # Sentence ending punctuation\n    SENTENCE_ENDERS = {\".\", \"!\", \"?\"}\n\n    # Clause ending punctuation (primary split points for low latency)\n    # These are natural pause points where TTS prosody remains acceptable\n    # Note: Using single chars for efficient iteration; ellipsis handled separately\n    CLAUSE_ENDERS = {\",\", \";\", \":\", \"\\n\", \"—\", \"–\", \"-\"}\n\n    # Multi-character patterns to check (handled in _find_clause_boundary)\n    CLAUSE_PATTERNS = [\" - \", \"...\", \" — \"]\n\n    # Quotation marks that might follow sentence enders\n    QUOTE_MARKS = {'\"', \"'\", '\"', '\"', \"\"\", \"\"\", \")\", \"]\"}\n\n    def __init__(\n        self,\n        config: Optional[ChunkerConfig] = None,\n        adaptive_config: Optional[AdaptiveChunkerConfig] = None,\n    ):\n        self.config = config or ChunkerConfig()\n        self._adaptive_config = adaptive_config\n        self._buffer = \"\"\n        self._chunks_emitted = 0\n        self._total_chars_processed = 0\n\n        # Adaptive mode tracking\n        self._adaptive_mode = adaptive_config is not None and adaptive_config.enabled\n        self._first_chunk_emitted = False\n\n    def _get_effective_limits(self) -> tuple:\n        \"\"\"\n        Get the effective chunk limits based on adaptive mode state.\n\n        Returns:\n            Tuple of (min_chars, optimal_chars, max_chars)\n        \"\"\"\n        if not self._adaptive_mode or not self._adaptive_config:\n            return (\n                self.config.min_chunk_chars,\n                self.config.optimal_chunk_chars,\n                self.config.max_chunk_chars,\n            )\n\n        # Check if we should still use first-chunk limits\n        if not self._first_chunk_emitted or (self._chunks_emitted < self._adaptive_config.chunks_before_natural):\n            # Use smaller limits for fast TTFA\n            return (\n                self._adaptive_config.first_chunk_min,\n                self._adaptive_config.first_chunk_optimal,\n                self._adaptive_config.first_chunk_max,\n            )\n        else:\n            # Use natural limits for better prosody\n            return (\n                self._adaptive_config.subsequent_min,\n                self._adaptive_config.subsequent_optimal,\n                self._adaptive_config.subsequent_max,\n            )\n\n    def add_token(self, token: str) -> List[str]:\n        \"\"\"\n        Add a token and return any complete chunks.\n\n        Args:\n            token: A token from the LLM stream (can be partial word, word, or punctuation)\n\n        Returns:\n            List of complete chunks ready for TTS (may be empty)\n        \"\"\"\n        if not token:\n            return []\n\n        self._buffer += token\n        self._total_chars_processed += len(token)\n\n        return self._extract_chunks()\n\n    def _extract_chunks(self) -> List[str]:\n        \"\"\"Extract any complete chunks from the buffer.\"\"\"\n        chunks = []\n\n        while True:\n            chunk = self._try_extract_chunk()\n            if chunk is None:\n                break\n            chunks.append(chunk)\n            self._chunks_emitted += 1\n\n            # Track first chunk for adaptive mode\n            if not self._first_chunk_emitted:\n                self._first_chunk_emitted = True\n                if self._adaptive_mode:\n                    logger.debug(f\"First chunk emitted ({len(chunk)} chars), \" \"switching to natural chunking mode\")\n\n        return chunks\n\n    def _try_extract_chunk(self) -> Optional[str]:\n        \"\"\"Try to extract a single chunk from the buffer.\"\"\"\n        if not self._buffer:\n            return None\n\n        # Get effective limits (adaptive or static)\n        min_chars, optimal_chars, max_chars = self._get_effective_limits()\n\n        # Check for sentence boundary\n        sentence_end = self._find_sentence_boundary()\n        if sentence_end is not None and sentence_end >= min_chars:\n            chunk = self._buffer[:sentence_end].strip()\n            self._buffer = self._buffer[sentence_end:].lstrip()\n            if chunk:\n                return chunk\n\n        # Check for clause boundary if buffer is getting long\n        if len(self._buffer) >= optimal_chars:\n            clause_end = self._find_clause_boundary(min_chars, optimal_chars)\n            if clause_end is not None and clause_end >= min_chars:\n                chunk = self._buffer[:clause_end].strip()\n                self._buffer = self._buffer[clause_end:].lstrip()\n                if chunk:\n                    return chunk\n\n        # Force split if buffer exceeds max\n        if len(self._buffer) >= max_chars:\n            # Try to split at last space\n            split_point = self._find_word_boundary(max_chars)\n            chunk = self._buffer[:split_point].strip()\n            self._buffer = self._buffer[split_point:].lstrip()\n            if chunk:\n                logger.debug(f\"Force split at {split_point} chars\")\n                return chunk\n\n        return None\n\n    def _find_sentence_boundary(self) -> Optional[int]:\n        \"\"\"\n        Find the position after a sentence ending punctuation.\n\n        Returns:\n            Position after the sentence end (including trailing quotes),\n            or None if no sentence boundary found.\n        \"\"\"\n        for i, char in enumerate(self._buffer):\n            if char in self.SENTENCE_ENDERS:\n                # Check if this is an abbreviation (not a real sentence end)\n                if self._is_abbreviation(i):\n                    continue\n\n                # Find the end position (including trailing quotes/spaces)\n                end_pos = i + 1\n                while end_pos < len(self._buffer) and self._buffer[end_pos] in self.QUOTE_MARKS:\n                    end_pos += 1\n\n                # Check for space after (confirms sentence end)\n                if end_pos >= len(self._buffer) or self._buffer[end_pos] in {\n                    \" \",\n                    \"\\n\",\n                    \"\\t\",\n                }:\n                    return end_pos\n\n        return None\n\n    def _find_clause_boundary(self, min_chars: int, optimal_chars: int) -> Optional[int]:\n        \"\"\"\n        Find the position after a clause ending punctuation.\n\n        Checks both single-character clause enders and multi-character patterns\n        like ' - ' and '...' for natural phrase breaks.\n\n        Args:\n            min_chars: Minimum chunk size (adaptive or static)\n            optimal_chars: Optimal chunk size to search around (adaptive or static)\n\n        Returns:\n            Position after the clause end, or None if not found.\n        \"\"\"\n        # First check for multi-character patterns (they take priority)\n        for pattern in self.CLAUSE_PATTERNS:\n            # Search backwards from optimal position\n            search_area = self._buffer[: min(len(self._buffer), optimal_chars + 20)]\n            idx = search_area.rfind(pattern)\n            if idx >= min_chars:\n                return idx + len(pattern)\n\n        # Search backwards from optimal position for single char enders\n        search_start = min(len(self._buffer), optimal_chars + 20)\n\n        for i in range(search_start - 1, min_chars - 1, -1):\n            if i < len(self._buffer) and self._buffer[i] in self.CLAUSE_ENDERS:\n                # Skip standalone hyphens in compound words (e.g., \"self-aware\")\n                if self._buffer[i] == \"-\":\n                    # Only split on dash if surrounded by spaces\n                    if i > 0 and i < len(self._buffer) - 1:\n                        if self._buffer[i - 1] != \" \" or self._buffer[i + 1] != \" \":\n                            continue\n                return i + 1\n\n        # Fallback: search forward\n        for i in range(min_chars, len(self._buffer)):\n            if self._buffer[i] in self.CLAUSE_ENDERS:\n                if self._buffer[i] == \"-\":\n                    if i > 0 and i < len(self._buffer) - 1:\n                        if self._buffer[i - 1] != \" \" or self._buffer[i + 1] != \" \":\n                            continue\n                return i + 1\n\n        return None\n\n    def _find_word_boundary(self, max_pos: int) -> int:\n        \"\"\"\n        Find a word boundary (space) for emergency splitting.\n\n        Args:\n            max_pos: Maximum position to search\n\n        Returns:\n            Position of word boundary, or max_pos if none found\n        \"\"\"\n        # Search backwards for a space\n        for i in range(min(max_pos, len(self._buffer)) - 1, max_pos // 2, -1):\n            if self._buffer[i] == \" \":\n                return i + 1\n\n        # No good split point found, force split at max\n        return min(max_pos, len(self._buffer))\n\n    def _is_abbreviation(self, period_pos: int) -> bool:\n        \"\"\"\n        Check if the period at given position is part of an abbreviation.\n\n        Args:\n            period_pos: Position of the period in buffer\n\n        Returns:\n            True if this appears to be an abbreviation\n        \"\"\"\n        if self._buffer[period_pos] != \".\":\n            return False\n\n        # Check against known abbreviations\n        for abbrev in self.config.abbreviations:\n            abbrev_len = len(abbrev)\n            start = period_pos - abbrev_len + 1\n            if start >= 0:\n                candidate = self._buffer[start : period_pos + 1]\n                if candidate == abbrev:\n                    return True\n\n        # Check for single uppercase letter followed by period (initials)\n        if period_pos >= 1:\n            prev_char = self._buffer[period_pos - 1]\n            if prev_char.isupper():\n                if period_pos < 2 or not self._buffer[period_pos - 2].isalpha():\n                    return True\n\n        # Check for decimal number (e.g., \"3.14\")\n        if period_pos >= 1 and period_pos < len(self._buffer) - 1:\n            prev_char = self._buffer[period_pos - 1]\n            next_char = self._buffer[period_pos + 1]\n            if prev_char.isdigit() and next_char.isdigit():\n                return True\n\n        return False\n\n    def flush(self) -> Optional[str]:\n        \"\"\"\n        Flush any remaining content in the buffer.\n\n        Call this at the end of the LLM stream to get the final chunk.\n\n        Returns:\n            Remaining content as a chunk, or None if buffer is empty\n        \"\"\"\n        if not self._buffer:\n            return None\n\n        chunk = self._buffer.strip()\n        self._buffer = \"\"\n\n        if chunk:\n            self._chunks_emitted += 1\n            return chunk\n\n        return None\n\n    def reset(self) -> None:\n        \"\"\"Reset the chunker state for a new stream.\"\"\"\n        self._buffer = \"\"\n        self._chunks_emitted = 0\n        self._total_chars_processed = 0\n        self._first_chunk_emitted = False  # Reset adaptive mode tracking\n\n    def get_stats(self) -> dict:\n        \"\"\"Get chunking statistics.\"\"\"\n        return {\n            \"chunks_emitted\": self._chunks_emitted,\n            \"total_chars_processed\": self._total_chars_processed,\n            \"buffer_length\": len(self._buffer),\n        }\n\n\nclass StreamingSentenceChunker:\n    \"\"\"\n    Async wrapper for sentence chunking with streaming callbacks.\n\n    Provides a simpler interface for the voice pipeline.\n    \"\"\"\n\n    def __init__(\n        self,\n        on_chunk: callable,\n        config: Optional[ChunkerConfig] = None,\n    ):\n        \"\"\"\n        Initialize the streaming chunker.\n\n        Args:\n            on_chunk: Async callback for each complete chunk\n            config: Optional chunker configuration\n        \"\"\"\n        self.chunker = SentenceChunker(config)\n        self.on_chunk = on_chunk\n\n    async def process_token(self, token: str) -> None:\n        \"\"\"\n        Process a token and emit any complete chunks.\n\n        Args:\n            token: Token from LLM stream\n        \"\"\"\n        chunks = self.chunker.add_token(token)\n        for chunk in chunks:\n            await self.on_chunk(chunk)\n\n    async def finish(self) -> None:\n        \"\"\"Finish processing and emit any remaining content.\"\"\"\n        final = self.chunker.flush()\n        if final:\n            await self.on_chunk(final)\n\n    def reset(self) -> None:\n        \"\"\"Reset for a new stream.\"\"\"\n        self.chunker.reset()\n"
}
