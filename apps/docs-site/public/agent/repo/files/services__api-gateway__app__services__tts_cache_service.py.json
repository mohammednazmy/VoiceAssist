{
  "path": "services/api-gateway/app/services/tts_cache_service.py",
  "language": "python",
  "size": 14604,
  "last_modified": "2025-12-05T03:04:38.216Z",
  "lines": 497,
  "content": "\"\"\"\nTTS Cache Service - Efficient caching for text-to-speech synthesis\n\nVoice Mode v4 - Phase 1 Foundation\n\nProvides multi-level caching for TTS outputs:\n- L1: In-memory LRU cache for hot phrases\n- L2: Redis cache for persistent storage\n- Supports both raw and SSML-enriched outputs\n- Pronunciation-enhanced caching with lexicon integration\n\"\"\"\n\nimport asyncio\nimport hashlib\nimport logging\nfrom dataclasses import dataclass, field\nfrom datetime import datetime, timezone\nfrom enum import Enum\nfrom functools import lru_cache\nfrom typing import Any, Dict, List, Optional, Tuple\n\nlogger = logging.getLogger(__name__)\n\n\nclass CacheLevel(Enum):\n    \"\"\"Cache storage levels.\"\"\"\n    L1_MEMORY = \"l1_memory\"  # In-memory LRU\n    L2_REDIS = \"l2_redis\"  # Redis persistent\n    MISS = \"miss\"  # Cache miss\n\n\n@dataclass\nclass TTSCacheConfig:\n    \"\"\"Configuration for TTS caching.\"\"\"\n    # L1 Memory cache\n    l1_enabled: bool = True\n    l1_max_size: int = 500  # Max entries in memory\n    l1_max_audio_size_bytes: int = 1_000_000  # 1MB max per entry\n\n    # L2 Redis cache\n    l2_enabled: bool = True\n    l2_ttl_seconds: int = 86400  # 24 hours\n    l2_prefix: str = \"tts_cache\"\n\n    # Cache key settings\n    include_voice_id: bool = True\n    include_speed: bool = True\n    include_pitch: bool = False\n\n    # SSML handling\n    cache_ssml_separately: bool = True\n\n    # Metrics\n    enable_metrics: bool = True\n\n\n@dataclass\nclass CacheEntry:\n    \"\"\"A cached TTS audio entry.\"\"\"\n    audio_data: bytes\n    voice_id: str\n    text_hash: str\n    ssml: bool\n    created_at: datetime\n    hit_count: int = 0\n    size_bytes: int = 0\n\n    def __post_init__(self):\n        self.size_bytes = len(self.audio_data)\n\n\n@dataclass\nclass CacheMetrics:\n    \"\"\"Metrics for TTS cache performance.\"\"\"\n    l1_hits: int = 0\n    l1_misses: int = 0\n    l2_hits: int = 0\n    l2_misses: int = 0\n    total_requests: int = 0\n    bytes_served_from_cache: int = 0\n    cache_fills: int = 0\n\n    @property\n    def l1_hit_rate(self) -> float:\n        total = self.l1_hits + self.l1_misses\n        return self.l1_hits / total if total > 0 else 0.0\n\n    @property\n    def l2_hit_rate(self) -> float:\n        total = self.l2_hits + self.l2_misses\n        return self.l2_hits / total if total > 0 else 0.0\n\n    @property\n    def overall_hit_rate(self) -> float:\n        if self.total_requests == 0:\n            return 0.0\n        return (self.l1_hits + self.l2_hits) / self.total_requests\n\n\nclass TTSCacheService:\n    \"\"\"\n    Multi-level TTS caching service.\n\n    Provides efficient caching for text-to-speech synthesis with:\n    - Fast in-memory L1 cache for frequently used phrases\n    - Persistent L2 Redis cache for longer-term storage\n    - Support for both raw text and SSML-enriched inputs\n    \"\"\"\n\n    def __init__(\n        self,\n        config: Optional[TTSCacheConfig] = None,\n        redis_client: Optional[Any] = None\n    ):\n        self.config = config or TTSCacheConfig()\n        self._redis = redis_client\n        self._l1_cache: Dict[str, CacheEntry] = {}\n        self._l1_access_order: List[str] = []  # For LRU eviction\n        self._metrics = CacheMetrics()\n        self._initialized = False\n\n    async def initialize(self, redis_client: Optional[Any] = None) -> None:\n        \"\"\"Initialize the cache service.\"\"\"\n        if self._initialized:\n            return\n\n        if redis_client:\n            self._redis = redis_client\n\n        logger.info(\n            \"Initializing TTSCacheService\",\n            extra={\n                \"l1_enabled\": self.config.l1_enabled,\n                \"l1_max_size\": self.config.l1_max_size,\n                \"l2_enabled\": self.config.l2_enabled,\n                \"l2_ttl\": self.config.l2_ttl_seconds,\n            }\n        )\n\n        self._initialized = True\n\n    def cache_key(\n        self,\n        text: str,\n        voice_id: str,\n        ssml: bool = False,\n        speed: float = 1.0,\n        pitch: float = 1.0\n    ) -> str:\n        \"\"\"\n        Generate a unique cache key for TTS request.\n\n        Args:\n            text: Text to synthesize\n            voice_id: Voice identifier\n            ssml: Whether text contains SSML\n            speed: Speech speed multiplier\n            pitch: Pitch adjustment\n\n        Returns:\n            Unique cache key string\n        \"\"\"\n        # Create hash of text content\n        text_hash = hashlib.sha256(text.encode('utf-8')).hexdigest()[:16]\n\n        # Build key components\n        parts = [self.config.l2_prefix, text_hash]\n\n        if self.config.include_voice_id:\n            parts.append(voice_id)\n\n        if self.config.cache_ssml_separately:\n            parts.append(\"ssml\" if ssml else \"raw\")\n\n        if self.config.include_speed and speed != 1.0:\n            parts.append(f\"s{speed:.2f}\")\n\n        if self.config.include_pitch and pitch != 1.0:\n            parts.append(f\"p{pitch:.2f}\")\n\n        return \":\".join(parts)\n\n    async def get(\n        self,\n        text: str,\n        voice_id: str,\n        ssml: bool = False,\n        speed: float = 1.0,\n        pitch: float = 1.0\n    ) -> Tuple[Optional[bytes], CacheLevel]:\n        \"\"\"\n        Get cached TTS audio if available.\n\n        Args:\n            text: Text to synthesize\n            voice_id: Voice identifier\n            ssml: Whether text contains SSML\n            speed: Speech speed multiplier\n            pitch: Pitch adjustment\n\n        Returns:\n            Tuple of (audio_bytes or None, cache_level)\n        \"\"\"\n        self._metrics.total_requests += 1\n        key = self.cache_key(text, voice_id, ssml, speed, pitch)\n\n        # Try L1 (memory) first\n        if self.config.l1_enabled and key in self._l1_cache:\n            entry = self._l1_cache[key]\n            entry.hit_count += 1\n            self._update_lru(key)\n            self._metrics.l1_hits += 1\n            self._metrics.bytes_served_from_cache += entry.size_bytes\n            logger.debug(f\"TTS cache L1 hit: {key[:32]}...\")\n            return entry.audio_data, CacheLevel.L1_MEMORY\n\n        self._metrics.l1_misses += 1\n\n        # Try L2 (Redis) if available\n        if self.config.l2_enabled and self._redis:\n            try:\n                cached = await self._redis.get(key)\n                if cached:\n                    self._metrics.l2_hits += 1\n                    self._metrics.bytes_served_from_cache += len(cached)\n\n                    # Promote to L1\n                    if self.config.l1_enabled:\n                        await self._store_l1(key, cached, voice_id, ssml)\n\n                    logger.debug(f\"TTS cache L2 hit: {key[:32]}...\")\n                    return cached, CacheLevel.L2_REDIS\n            except Exception as e:\n                logger.warning(f\"Redis cache get error: {e}\")\n\n        self._metrics.l2_misses += 1\n        return None, CacheLevel.MISS\n\n    async def set(\n        self,\n        text: str,\n        voice_id: str,\n        audio_data: bytes,\n        ssml: bool = False,\n        speed: float = 1.0,\n        pitch: float = 1.0\n    ) -> str:\n        \"\"\"\n        Cache TTS audio output.\n\n        Args:\n            text: Original text\n            voice_id: Voice identifier\n            audio_data: Generated audio bytes\n            ssml: Whether text contains SSML\n            speed: Speech speed multiplier\n            pitch: Pitch adjustment\n\n        Returns:\n            Cache key used\n        \"\"\"\n        key = self.cache_key(text, voice_id, ssml, speed, pitch)\n\n        # Store in L1\n        if self.config.l1_enabled:\n            await self._store_l1(key, audio_data, voice_id, ssml)\n\n        # Store in L2\n        if self.config.l2_enabled and self._redis:\n            try:\n                await self._redis.setex(\n                    key,\n                    self.config.l2_ttl_seconds,\n                    audio_data\n                )\n            except Exception as e:\n                logger.warning(f\"Redis cache set error: {e}\")\n\n        self._metrics.cache_fills += 1\n        logger.debug(f\"TTS cached: {key[:32]}... ({len(audio_data)} bytes)\")\n\n        return key\n\n    async def _store_l1(\n        self,\n        key: str,\n        audio_data: bytes,\n        voice_id: str,\n        ssml: bool\n    ) -> None:\n        \"\"\"Store entry in L1 cache with LRU eviction.\"\"\"\n        # Check size limit\n        if len(audio_data) > self.config.l1_max_audio_size_bytes:\n            return  # Too large for L1\n\n        # Evict if at capacity\n        while len(self._l1_cache) >= self.config.l1_max_size:\n            self._evict_lru()\n\n        # Create and store entry\n        entry = CacheEntry(\n            audio_data=audio_data,\n            voice_id=voice_id,\n            text_hash=key.split(\":\")[1],\n            ssml=ssml,\n            created_at=datetime.now(timezone.utc)\n        )\n\n        self._l1_cache[key] = entry\n        self._l1_access_order.append(key)\n\n    def _update_lru(self, key: str) -> None:\n        \"\"\"Update LRU order for a key.\"\"\"\n        if key in self._l1_access_order:\n            self._l1_access_order.remove(key)\n        self._l1_access_order.append(key)\n\n    def _evict_lru(self) -> None:\n        \"\"\"Evict least recently used entry from L1.\"\"\"\n        if not self._l1_access_order:\n            return\n\n        lru_key = self._l1_access_order.pop(0)\n        if lru_key in self._l1_cache:\n            del self._l1_cache[lru_key]\n\n    async def get_or_generate(\n        self,\n        text: str,\n        voice_id: str,\n        generator_func,\n        ssml: bool = False,\n        speed: float = 1.0,\n        pitch: float = 1.0,\n        pronunciation_enriched: bool = False\n    ) -> bytes:\n        \"\"\"\n        Get cached audio or generate and cache.\n\n        Args:\n            text: Text to synthesize\n            voice_id: Voice identifier\n            generator_func: Async function to generate TTS if cache miss\n            ssml: Whether text contains SSML\n            speed: Speech speed multiplier\n            pitch: Pitch adjustment\n            pronunciation_enriched: Whether text has pronunciation markup\n\n        Returns:\n            Audio bytes (from cache or newly generated)\n        \"\"\"\n        # Check cache first\n        cached, level = await self.get(text, voice_id, ssml, speed, pitch)\n        if cached:\n            return cached\n\n        # Generate new audio\n        audio_data = await generator_func(text, voice_id, speed=speed)\n\n        # Cache the result\n        await self.set(text, voice_id, audio_data, ssml, speed, pitch)\n\n        return audio_data\n\n    async def invalidate(\n        self,\n        text: Optional[str] = None,\n        voice_id: Optional[str] = None,\n        pattern: Optional[str] = None\n    ) -> int:\n        \"\"\"\n        Invalidate cache entries.\n\n        Args:\n            text: Specific text to invalidate\n            voice_id: Invalidate all entries for a voice\n            pattern: Redis key pattern for bulk invalidation\n\n        Returns:\n            Number of entries invalidated\n        \"\"\"\n        count = 0\n\n        # Invalidate L1\n        if text and voice_id:\n            key = self.cache_key(text, voice_id)\n            if key in self._l1_cache:\n                del self._l1_cache[key]\n                if key in self._l1_access_order:\n                    self._l1_access_order.remove(key)\n                count += 1\n\n        elif voice_id:\n            # Remove all entries for this voice\n            keys_to_remove = [\n                k for k, v in self._l1_cache.items()\n                if v.voice_id == voice_id\n            ]\n            for key in keys_to_remove:\n                del self._l1_cache[key]\n                if key in self._l1_access_order:\n                    self._l1_access_order.remove(key)\n                count += 1\n\n        # Invalidate L2\n        if self.config.l2_enabled and self._redis and pattern:\n            try:\n                keys = await self._redis.keys(pattern)\n                if keys:\n                    await self._redis.delete(*keys)\n                    count += len(keys)\n            except Exception as e:\n                logger.warning(f\"Redis invalidation error: {e}\")\n\n        logger.info(f\"Invalidated {count} TTS cache entries\")\n        return count\n\n    def get_metrics(self) -> CacheMetrics:\n        \"\"\"Get current cache metrics.\"\"\"\n        return self._metrics\n\n    def reset_metrics(self) -> None:\n        \"\"\"Reset cache metrics.\"\"\"\n        self._metrics = CacheMetrics()\n\n    def get_l1_stats(self) -> Dict[str, Any]:\n        \"\"\"Get L1 cache statistics.\"\"\"\n        total_size = sum(e.size_bytes for e in self._l1_cache.values())\n        return {\n            \"entries\": len(self._l1_cache),\n            \"max_entries\": self.config.l1_max_size,\n            \"total_size_bytes\": total_size,\n            \"avg_entry_size\": total_size / len(self._l1_cache) if self._l1_cache else 0,\n            \"voices_cached\": len(set(e.voice_id for e in self._l1_cache.values())),\n        }\n\n    async def warm_cache(\n        self,\n        phrases: List[str],\n        voice_id: str,\n        generator_func\n    ) -> int:\n        \"\"\"\n        Pre-warm cache with common phrases.\n\n        Args:\n            phrases: List of common phrases to cache\n            voice_id: Voice to use\n            generator_func: TTS generator function\n\n        Returns:\n            Number of phrases cached\n        \"\"\"\n        count = 0\n        for phrase in phrases:\n            try:\n                cached, _ = await self.get(phrase, voice_id)\n                if not cached:\n                    audio = await generator_func(phrase, voice_id)\n                    await self.set(phrase, voice_id, audio)\n                    count += 1\n            except Exception as e:\n                logger.warning(f\"Cache warm error for '{phrase[:30]}...': {e}\")\n\n        logger.info(f\"Warmed TTS cache with {count} phrases\")\n        return count\n\n\n# Singleton instance\n_tts_cache_service: Optional[TTSCacheService] = None\n\n\ndef get_tts_cache_service() -> TTSCacheService:\n    \"\"\"Get or create the singleton TTSCacheService instance.\"\"\"\n    global _tts_cache_service\n    if _tts_cache_service is None:\n        _tts_cache_service = TTSCacheService()\n    return _tts_cache_service\n\n\n# Common phrases to pre-warm (medical context)\nCOMMON_PHRASES = [\n    \"I understand.\",\n    \"Let me check that for you.\",\n    \"Based on the information provided,\",\n    \"According to the clinical guidelines,\",\n    \"Is there anything else you'd like to know?\",\n    \"Could you please clarify?\",\n    \"One moment please.\",\n    \"I'm searching for that information.\",\n    \"Here's what I found:\",\n    \"The recommended dosage is\",\n    \"Please consult your physician.\",\n    \"That's an important question.\",\n    \"Let me explain further.\",\n]\n"
}
