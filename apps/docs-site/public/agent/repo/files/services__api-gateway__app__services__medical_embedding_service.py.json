{
  "path": "services/api-gateway/app/services/medical_embedding_service.py",
  "language": "python",
  "size": 18363,
  "last_modified": "2025-12-04T23:44:49.028Z",
  "lines": 558,
  "content": "\"\"\"\nMedical Embedding Service\n\nProvides medical-specific embeddings using specialized language models:\n- PubMedBERT: Biomedical text embeddings trained on PubMed\n- BioGPT: Medical text generation and understanding\n- SciBERT: Scientific text embeddings\n\nThese models provide better semantic understanding for medical queries\ncompared to general-purpose embedding models.\n\"\"\"\n\nimport asyncio\nfrom dataclasses import dataclass\nfrom enum import Enum\nfrom typing import Any, Dict, List\n\nfrom app.core.config import settings\nfrom app.core.logging import get_logger\n\nlogger = get_logger(__name__)\n\n\nclass MedicalModelType(Enum):\n    \"\"\"Supported medical language models\"\"\"\n\n    PUBMEDBERT = \"pubmedbert\"\n    BIOGPT = \"biogpt\"\n    SCIBERT = \"scibert\"\n\n\n@dataclass\nclass ModelConfig:\n    \"\"\"Configuration for a medical language model\"\"\"\n\n    name: str\n    model_id: str\n    tokenizer_id: str\n    max_length: int = 512\n    embedding_dim: int = 768\n    supports_generation: bool = False\n    revision: str = \"main\"  # Pinned revision for reproducibility\n\n\n# Model configurations with pinned revisions\nMODEL_CONFIGS = {\n    MedicalModelType.PUBMEDBERT: ModelConfig(\n        name=\"PubMedBERT\",\n        model_id=\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\",\n        tokenizer_id=\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\",\n        max_length=512,\n        embedding_dim=768,\n        supports_generation=False,\n        revision=\"v1.1\",  # Pinned stable release\n    ),\n    MedicalModelType.BIOGPT: ModelConfig(\n        name=\"BioGPT\",\n        model_id=\"microsoft/biogpt\",\n        tokenizer_id=\"microsoft/biogpt\",\n        max_length=1024,\n        embedding_dim=1024,\n        supports_generation=True,\n        revision=\"main\",  # Pinned to main branch\n    ),\n    MedicalModelType.SCIBERT: ModelConfig(\n        name=\"SciBERT\",\n        model_id=\"allenai/scibert_scivocab_uncased\",\n        tokenizer_id=\"allenai/scibert_scivocab_uncased\",\n        max_length=512,\n        embedding_dim=768,\n        supports_generation=False,\n        revision=\"main\",  # Pinned to main branch\n    ),\n}\n\n\n@dataclass\nclass EmbeddingResult:\n    \"\"\"Result of embedding generation\"\"\"\n\n    embedding: List[float]\n    model: str\n    text_length: int\n    truncated: bool\n    metadata: Dict[str, Any]\n\n\n@dataclass\nclass GenerationResult:\n    \"\"\"Result of text generation\"\"\"\n\n    generated_text: str\n    model: str\n    prompt_length: int\n    generation_length: int\n    metadata: Dict[str, Any]\n\n\nclass MedicalEmbeddingService:\n    \"\"\"\n    Medical-specific embeddings using BioGPT and PubMedBERT.\n\n    This service provides high-quality embeddings for medical text,\n    enabling better semantic search and retrieval for healthcare queries.\n\n    Note: For production use, this requires:\n    - transformers library\n    - torch library\n    - Sufficient GPU memory for model inference\n\n    The service gracefully degrades to OpenAI embeddings when medical\n    models are not available.\n    \"\"\"\n\n    def __init__(self, lazy_load: bool = True):\n        \"\"\"\n        Initialize the medical embedding service.\n\n        Args:\n            lazy_load: If True, models are loaded on first use.\n                       If False, models are loaded immediately.\n        \"\"\"\n        self._models: Dict[MedicalModelType, Dict[str, Any]] = {}\n        self._models_loaded = False\n        self._lazy_load = lazy_load\n        self._device = \"cuda\" if self._check_cuda_available() else \"cpu\"\n        self._fallback_enabled = True\n\n        logger.info(\n            \"MedicalEmbeddingService initialized\",\n            extra={\n                \"lazy_load\": lazy_load,\n                \"device\": self._device,\n            },\n        )\n\n        if not lazy_load:\n            self._load_all_models()\n\n    def _check_cuda_available(self) -> bool:\n        \"\"\"Check if CUDA is available for GPU acceleration.\"\"\"\n        try:\n            import torch\n\n            return torch.cuda.is_available()\n        except ImportError:\n            return False\n\n    def _load_all_models(self) -> None:\n        \"\"\"Load all medical models.\"\"\"\n        for model_type in MedicalModelType:\n            self._load_model(model_type)\n        self._models_loaded = True\n\n    def _load_model(self, model_type: MedicalModelType) -> bool:\n        \"\"\"\n        Load a specific medical model.\n\n        Args:\n            model_type: Type of model to load\n\n        Returns:\n            True if model loaded successfully, False otherwise\n        \"\"\"\n        if model_type in self._models:\n            return True\n\n        config = MODEL_CONFIGS[model_type]\n\n        try:\n            from transformers import AutoModel, AutoTokenizer\n\n            logger.info(f\"Loading medical model: {config.name} (revision: {config.revision})\")\n\n            # nosec B615 - revision pinned via config.revision\n            tokenizer = AutoTokenizer.from_pretrained(config.tokenizer_id, revision=config.revision)  # nosec B615\n\n            if model_type == MedicalModelType.BIOGPT:\n                from transformers import BioGptForCausalLM, BioGptTokenizer\n\n                # nosec B615 - revision pinned via config.revision\n                tokenizer = BioGptTokenizer.from_pretrained(config.tokenizer_id, revision=config.revision)  # nosec B615\n                model = BioGptForCausalLM.from_pretrained(config.model_id, revision=config.revision)  # nosec B615\n            else:\n                model = AutoModel.from_pretrained(config.model_id, revision=config.revision)  # nosec B615\n\n            model = model.to(self._device)\n            model.eval()\n\n            self._models[model_type] = {\n                \"tokenizer\": tokenizer,\n                \"model\": model,\n                \"config\": config,\n            }\n\n            logger.info(\n                f\"Medical model loaded: {config.name}\",\n                extra={\n                    \"model\": config.model_id,\n                    \"device\": self._device,\n                },\n            )\n            return True\n\n        except ImportError as e:\n            logger.warning(\n                f\"Failed to load medical model (missing dependency): {e}\",\n                extra={\"model\": config.name},\n            )\n            return False\n        except Exception as e:\n            logger.error(\n                f\"Failed to load medical model: {e}\",\n                extra={\"model\": config.name, \"error\": str(e)},\n            )\n            return False\n\n    def _ensure_model_loaded(self, model_type: MedicalModelType) -> bool:\n        \"\"\"Ensure a model is loaded, loading it if necessary.\"\"\"\n        if model_type in self._models:\n            return True\n        return self._load_model(model_type)\n\n    async def generate_embedding(\n        self,\n        text: str,\n        model_type: MedicalModelType = MedicalModelType.PUBMEDBERT,\n        pooling: str = \"cls\",\n    ) -> EmbeddingResult:\n        \"\"\"\n        Generate embeddings using a medical language model.\n\n        Args:\n            text: Input text to embed\n            model_type: Type of medical model to use\n            pooling: Pooling strategy (\"cls\" for CLS token, \"mean\" for mean pooling)\n\n        Returns:\n            EmbeddingResult with embedding vector and metadata\n\n        Raises:\n            ValueError: If model type is not supported\n            RuntimeError: If embedding generation fails\n        \"\"\"\n        if model_type not in MODEL_CONFIGS:\n            raise ValueError(f\"Unsupported model type: {model_type}\")\n\n        config = MODEL_CONFIGS[model_type]\n\n        # Check if model supports embeddings (not just generation)\n        if config.supports_generation and model_type == MedicalModelType.BIOGPT:\n            # BioGPT is primarily for generation, use a different model for embeddings\n            logger.warning(\"BioGPT is not optimal for embeddings, falling back to PubMedBERT\")\n            model_type = MedicalModelType.PUBMEDBERT\n            config = MODEL_CONFIGS[model_type]\n\n        # Try to load the model\n        if not self._ensure_model_loaded(model_type):\n            if self._fallback_enabled:\n                return await self._fallback_embedding(text, model_type)\n            raise RuntimeError(f\"Failed to load model: {model_type.value}\")\n\n        model_data = self._models[model_type]\n        tokenizer = model_data[\"tokenizer\"]\n        model = model_data[\"model\"]\n\n        try:\n            import torch\n\n            # Tokenize input\n            inputs = tokenizer(\n                text,\n                return_tensors=\"pt\",\n                truncation=True,\n                max_length=config.max_length,\n                padding=True,\n            )\n\n            # Move to device\n            inputs = {k: v.to(self._device) for k, v in inputs.items()}\n\n            # Check if text was truncated\n            truncated = len(tokenizer.encode(text)) > config.max_length\n\n            # Generate embeddings\n            with torch.no_grad():\n                outputs = model(**inputs)\n\n            # Apply pooling strategy\n            if pooling == \"cls\":\n                # Use [CLS] token embedding\n                embedding = outputs.last_hidden_state[:, 0, :].squeeze()\n            elif pooling == \"mean\":\n                # Mean pooling over all tokens\n                attention_mask = inputs[\"attention_mask\"]\n                token_embeddings = outputs.last_hidden_state\n                input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n                sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n                sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n                embedding = (sum_embeddings / sum_mask).squeeze()\n            else:\n                raise ValueError(f\"Unknown pooling strategy: {pooling}\")\n\n            # Convert to list\n            embedding_list = embedding.cpu().numpy().tolist()\n\n            return EmbeddingResult(\n                embedding=embedding_list,\n                model=config.name,\n                text_length=len(text),\n                truncated=truncated,\n                metadata={\n                    \"model_id\": config.model_id,\n                    \"pooling\": pooling,\n                    \"embedding_dim\": len(embedding_list),\n                    \"device\": self._device,\n                },\n            )\n\n        except Exception as e:\n            logger.error(\n                f\"Embedding generation failed: {e}\",\n                extra={\"model\": model_type.value, \"text_length\": len(text)},\n            )\n            if self._fallback_enabled:\n                return await self._fallback_embedding(text, model_type)\n            raise RuntimeError(f\"Embedding generation failed: {e}\")\n\n    async def generate_embeddings_batch(\n        self,\n        texts: List[str],\n        model_type: MedicalModelType = MedicalModelType.PUBMEDBERT,\n        pooling: str = \"cls\",\n        batch_size: int = 32,\n    ) -> List[EmbeddingResult]:\n        \"\"\"\n        Generate embeddings for multiple texts in batches.\n\n        Args:\n            texts: List of texts to embed\n            model_type: Type of medical model to use\n            pooling: Pooling strategy\n            batch_size: Number of texts to process per batch\n\n        Returns:\n            List of EmbeddingResult objects\n        \"\"\"\n        results = []\n\n        for i in range(0, len(texts), batch_size):\n            batch = texts[i : i + batch_size]\n\n            # Process batch in parallel\n            batch_results = await asyncio.gather(\n                *[self.generate_embedding(text, model_type, pooling) for text in batch]\n            )\n            results.extend(batch_results)\n\n        return results\n\n    async def generate_text(\n        self,\n        prompt: str,\n        max_length: int = 200,\n        temperature: float = 0.7,\n        top_p: float = 0.9,\n        num_return_sequences: int = 1,\n    ) -> GenerationResult:\n        \"\"\"\n        Generate medical text using BioGPT.\n\n        Args:\n            prompt: Input prompt for generation\n            max_length: Maximum length of generated text\n            temperature: Sampling temperature (higher = more creative)\n            top_p: Nucleus sampling parameter\n            num_return_sequences: Number of sequences to generate\n\n        Returns:\n            GenerationResult with generated text and metadata\n        \"\"\"\n        model_type = MedicalModelType.BIOGPT\n\n        if not self._ensure_model_loaded(model_type):\n            raise RuntimeError(\"BioGPT model not available for text generation\")\n\n        model_data = self._models[model_type]\n        tokenizer = model_data[\"tokenizer\"]\n        model = model_data[\"model\"]\n        config = model_data[\"config\"]\n\n        try:\n            import torch\n\n            # Tokenize prompt\n            inputs = tokenizer(prompt, return_tensors=\"pt\")\n            inputs = {k: v.to(self._device) for k, v in inputs.items()}\n\n            prompt_length = inputs[\"input_ids\"].shape[1]\n\n            # Generate text\n            with torch.no_grad():\n                outputs = model.generate(\n                    **inputs,\n                    max_length=min(max_length + prompt_length, config.max_length),\n                    temperature=temperature,\n                    top_p=top_p,\n                    do_sample=True,\n                    num_return_sequences=num_return_sequences,\n                    pad_token_id=tokenizer.eos_token_id,\n                )\n\n            # Decode generated text\n            generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n            # Remove the prompt from the generated text\n            if generated_text.startswith(prompt):\n                generated_text = generated_text[len(prompt) :].strip()\n\n            return GenerationResult(\n                generated_text=generated_text,\n                model=config.name,\n                prompt_length=len(prompt),\n                generation_length=len(generated_text),\n                metadata={\n                    \"model_id\": config.model_id,\n                    \"temperature\": temperature,\n                    \"top_p\": top_p,\n                    \"max_length\": max_length,\n                },\n            )\n\n        except Exception as e:\n            logger.error(\n                f\"Text generation failed: {e}\",\n                extra={\"prompt_length\": len(prompt)},\n            )\n            raise RuntimeError(f\"Text generation failed: {e}\")\n\n    async def _fallback_embedding(\n        self,\n        text: str,\n        original_model: MedicalModelType,\n    ) -> EmbeddingResult:\n        \"\"\"\n        Generate embedding using fallback method (OpenAI API).\n\n        Args:\n            text: Input text\n            original_model: The model that was originally requested\n\n        Returns:\n            EmbeddingResult with OpenAI embedding\n        \"\"\"\n        try:\n            import httpx\n\n            async with httpx.AsyncClient(timeout=30.0) as client:\n                response = await client.post(\n                    \"https://api.openai.com/v1/embeddings\",\n                    headers={\n                        \"Authorization\": f\"Bearer {settings.OPENAI_API_KEY}\",\n                        \"Content-Type\": \"application/json\",\n                    },\n                    json={\n                        \"model\": \"text-embedding-3-small\",\n                        \"input\": text[:8000],  # Truncate if needed\n                    },\n                )\n                response.raise_for_status()\n                data = response.json()\n\n                embedding = data[\"data\"][0][\"embedding\"]\n\n                logger.info(\n                    \"Used OpenAI fallback for embedding\",\n                    extra={\"original_model\": original_model.value},\n                )\n\n                return EmbeddingResult(\n                    embedding=embedding,\n                    model=\"text-embedding-3-small (fallback)\",\n                    text_length=len(text),\n                    truncated=len(text) > 8000,\n                    metadata={\n                        \"fallback\": True,\n                        \"original_model\": original_model.value,\n                        \"embedding_dim\": len(embedding),\n                    },\n                )\n\n        except Exception as e:\n            logger.error(f\"Fallback embedding also failed: {e}\")\n            raise RuntimeError(f\"All embedding methods failed: {e}\")\n\n    def get_model_info(self, model_type: MedicalModelType) -> Dict[str, Any]:\n        \"\"\"Get information about a medical model.\"\"\"\n        config = MODEL_CONFIGS.get(model_type)\n        if not config:\n            return {\"error\": \"Unknown model type\"}\n\n        loaded = model_type in self._models\n\n        return {\n            \"name\": config.name,\n            \"model_id\": config.model_id,\n            \"embedding_dim\": config.embedding_dim,\n            \"max_length\": config.max_length,\n            \"supports_generation\": config.supports_generation,\n            \"loaded\": loaded,\n            \"device\": self._device if loaded else None,\n        }\n\n    def get_available_models(self) -> List[Dict[str, Any]]:\n        \"\"\"Get information about all available models.\"\"\"\n        return [self.get_model_info(mt) for mt in MedicalModelType]\n\n    async def compute_similarity(\n        self,\n        text1: str,\n        text2: str,\n        model_type: MedicalModelType = MedicalModelType.PUBMEDBERT,\n    ) -> float:\n        \"\"\"\n        Compute semantic similarity between two texts.\n\n        Args:\n            text1: First text\n            text2: Second text\n            model_type: Model to use for embeddings\n\n        Returns:\n            Cosine similarity score (0-1)\n        \"\"\"\n        # Generate embeddings for both texts\n        result1 = await self.generate_embedding(text1, model_type)\n        result2 = await self.generate_embedding(text2, model_type)\n\n        # Compute cosine similarity\n        import math\n\n        emb1 = result1.embedding\n        emb2 = result2.embedding\n\n        dot_product = sum(a * b for a, b in zip(emb1, emb2))\n        norm1 = math.sqrt(sum(a * a for a in emb1))\n        norm2 = math.sqrt(sum(b * b for b in emb2))\n\n        if norm1 == 0 or norm2 == 0:\n            return 0.0\n\n        similarity = dot_product / (norm1 * norm2)\n        return max(0.0, min(1.0, similarity))  # Clamp to [0, 1]\n\n\n# Global service instance (lazy loaded)\nmedical_embedding_service = MedicalEmbeddingService(lazy_load=True)\n"
}
