{
  "path": "services/api-gateway/app/services/thinker_service.py",
  "language": "python",
  "size": 40114,
  "last_modified": "2025-12-04T12:32:40.272Z",
  "lines": 1073,
  "content": "\"\"\"\nThinker Service - LLM Orchestration for Voice Pipeline\n\nUnified reasoning service that manages:\n- Conversation context (shared between voice and chat modes)\n- Streaming LLM responses with token callbacks\n- Tool/function calling with result injection\n- RAG context retrieval\n\nPhase: Thinker/Talker Voice Pipeline Migration\n\"\"\"\n\nimport json\nimport time\nimport uuid\nfrom dataclasses import dataclass, field\nfrom datetime import datetime, timedelta\nfrom enum import Enum\nfrom typing import Any, AsyncIterator, Awaitable, Callable, Dict, List, Optional\n\nimport pytz\nfrom app.core.config import settings\nfrom app.core.logging import get_logger\nfrom app.services.llm_client import LLMClient, LLMRequest, ToolCall\nfrom app.services.repair_strategy_service import RepairStrategy, repair_strategy_service\nfrom app.services.tools.tool_service import ToolExecutionContext, tool_service\n\nlogger = get_logger(__name__)\n\n\n# ==============================================================================\n# Data Classes\n# ==============================================================================\n\n\nclass ThinkingState(str, Enum):\n    \"\"\"State of the thinking process.\"\"\"\n\n    IDLE = \"idle\"\n    PROCESSING = \"processing\"\n    TOOL_CALLING = \"tool_calling\"\n    GENERATING = \"generating\"\n    COMPLETE = \"complete\"\n    CANCELLED = \"cancelled\"\n    ERROR = \"error\"\n\n\n@dataclass\nclass ConversationMessage:\n    \"\"\"A message in the conversation history.\"\"\"\n\n    role: str  # \"user\", \"assistant\", \"system\", \"tool\"\n    content: str\n    message_id: str = field(default_factory=lambda: str(uuid.uuid4()))\n    timestamp: float = field(default_factory=time.time)\n    source_mode: str = \"chat\"  # \"chat\" or \"voice\"\n    tool_call_id: Optional[str] = None  # For tool results\n    tool_calls: Optional[List[Dict]] = None  # For assistant messages with tool calls\n    citations: Optional[List[Dict]] = None\n\n\n@dataclass\nclass ToolCallEvent:\n    \"\"\"Event emitted when a tool is called.\"\"\"\n\n    tool_id: str\n    tool_name: str\n    arguments: Dict[str, Any]\n\n\n@dataclass\nclass ToolResultEvent:\n    \"\"\"Event emitted when a tool returns a result.\"\"\"\n\n    tool_id: str\n    tool_name: str\n    result: Any\n    citations: Optional[List[Dict]] = None\n\n\n@dataclass\nclass ThinkerResponse:\n    \"\"\"Response from the Thinker service.\"\"\"\n\n    text: str\n    message_id: str\n    citations: Optional[List[Dict]] = None\n    tool_calls_made: List[str] = field(default_factory=list)\n    latency_ms: int = 0\n    tokens_used: int = 0\n    state: ThinkingState = ThinkingState.COMPLETE\n\n    # Phase 7: Confidence scoring for conversational repair\n    confidence: float = 1.0  # 0-1, AI's confidence in understanding the query\n    needs_clarification: bool = False  # True if AI needs more info from user\n    repair_applied: bool = False  # True if a repair strategy was applied\n\n\n@dataclass\nclass ThinkerMetrics:\n    \"\"\"Metrics for a thinking session.\"\"\"\n\n    total_tokens: int = 0\n    tool_calls_count: int = 0\n    first_token_latency_ms: int = 0\n    total_latency_ms: int = 0\n    cancelled: bool = False\n\n\n# ==============================================================================\n# Conversation Context Manager\n# ==============================================================================\n\n\nclass ConversationContext:\n    \"\"\"\n    Manages conversation history for a session.\n\n    Features:\n    - Stores message history\n    - Formats messages for LLM API\n    - Handles token limits via truncation\n    - Tracks mode (voice/chat) per message\n    \"\"\"\n\n    MAX_HISTORY_MESSAGES = 20\n    MAX_CONTEXT_TOKENS = 8000  # Reserve tokens for response\n\n    def __init__(self, conversation_id: str, system_prompt: Optional[str] = None):\n        self.conversation_id = conversation_id\n        self.messages: List[ConversationMessage] = []\n        self.system_prompt = system_prompt or self._default_system_prompt()\n\n    def _default_system_prompt(self) -> str:\n        # Get current time in user's timezone (default to Eastern)\n        tz = pytz.timezone(\"America/New_York\")\n        now = datetime.now(tz)\n\n        return f\"\"\"You are VoiceAssist, a helpful AI voice assistant.\n\nCURRENT TIME CONTEXT:\n- Current date: {now.strftime(\"%A, %B %d, %Y\")}\n- Current time: {now.strftime(\"%I:%M %p %Z\")}\n- Day of week: {now.strftime(\"%A\")}\n- ISO format: {now.isoformat()}\n\nWhen the user says relative times, calculate from current time:\n- \"in one hour\" → {(now + timedelta(hours=1)).strftime(\"%I:%M %p\")}\n- \"in 30 minutes\" → {(now + timedelta(minutes=30)).strftime(\"%I:%M %p\")}\n- \"tomorrow\" → {(now + timedelta(days=1)).strftime(\"%A, %B %d, %Y\")}\n- \"next week\" → {(now + timedelta(days=7)).strftime(\"%A, %B %d, %Y\")}\n\nYou MUST calculate dates/times yourself. NEVER ask the user for the current time.\n\nCONVERSATION MEMORY:\nYou have access to the full conversation history. Use it to:\n- Remember what the user said earlier (names, times, preferences)\n- NEVER ask for information the user already provided\n- Reference previous context naturally: \"For that 'test event' you mentioned...\"\n- If the user says \"make it longer\" or \"change the time\", update what you just discussed\n\nAVAILABLE TOOLS:\nYou have access to the following tools - use them when relevant:\n- calendar_create_event: Create events on the user's connected calendar (Google, Microsoft, Apple, Nextcloud)\n- calendar_list_events: View the user's upcoming schedule and appointments\n- calendar_update_event: Modify existing events (change time, title, description, location)\n- calendar_delete_event: Remove/cancel events from the calendar\n- web_search: Search the web for current information\n- pubmed_search: Search medical literature and research papers\n- medical_calculator: Calculate medical scores (Wells DVT, CHA2DS2-VASc, BMI, eGFR, etc.)\n- kb_search: Search the medical knowledge base for clinical information\n\nCALENDAR TOOL USAGE:\n- When asked about calendar/schedule, USE calendar_list_events\n- When asked to add/create an appointment, USE calendar_create_event\n- When asked to change/modify/reschedule an event, USE calendar_update_event\n- When asked to delete/remove/cancel an event, USE calendar_delete_event\n- For update/delete: First use calendar_list_events to get the event_id, then use the appropriate tool\n\nHANDLING PARTIAL INFORMATION:\nWhen the user provides incomplete info, DO NOT ask rigid follow-up questions.\nInstead:\n- Use reasonable defaults (1 hour duration, no description)\n- Infer from context (if they mentioned a time earlier, use it)\n- Confirm naturally: \"I'll create a 1-hour event at 3:30 PM. Sound good?\"\n\nGRACEFUL CLARIFICATION:\nWhen you need more info, ask naturally:\n- BAD: \"What would you like to name this event?\"\n- GOOD: \"Sure, what should I call it?\"\n- BAD: \"Could you please tell me the start time?\"\n- GOOD: \"When should I schedule it?\"\n\nPROACTIVE ASSISTANCE:\nAfter completing a task, offer relevant follow-ups briefly:\n- \"Done! Want me to add a reminder?\"\n- \"Created! Should I check for conflicts?\"\n- \"Found 3 events. Want me to read them out?\"\n\nKEY BEHAVIORS:\n- Keep responses concise and natural for voice\n- Use short sentences (max 15-20 words)\n- Avoid abbreviations - say \"blood pressure\" not \"BP\"\n- Acknowledge questions briefly before answering\n- Ask clarifying questions naturally when truly needed\n- For medical questions, recommend consulting a healthcare professional\n\nRESPONSE STRUCTURE:\n1. Brief acknowledgment (1-2 words: \"Sure.\", \"Got it.\")\n2. Core answer in 2-3 short sentences\n3. Offer to elaborate if complex (\"Would you like more details?\")\n\"\"\"\n\n    def add_message(\n        self,\n        role: str,\n        content: str,\n        source_mode: str = \"chat\",\n        tool_call_id: Optional[str] = None,\n        tool_calls: Optional[List[Dict]] = None,\n    ) -> ConversationMessage:\n        \"\"\"Add a message to the conversation.\"\"\"\n        message = ConversationMessage(\n            role=role,\n            content=content,\n            source_mode=source_mode,\n            tool_call_id=tool_call_id,\n            tool_calls=tool_calls,\n        )\n        self.messages.append(message)\n\n        # Trim old messages if needed (smart trim to preserve tool call chains)\n        if len(self.messages) > self.MAX_HISTORY_MESSAGES:\n            self._smart_trim()\n\n        return message\n\n    def _smart_trim(self) -> None:\n        \"\"\"\n        Trim messages while preserving tool call chains.\n\n        OpenAI requires: assistant (with tool_calls) -> tool (with tool_call_id)\n        We can't break this chain or the API will reject the request.\n        \"\"\"\n        if len(self.messages) <= self.MAX_HISTORY_MESSAGES:\n            return\n\n        # Find the first safe trim point (not in the middle of a tool call chain)\n        trim_target = len(self.messages) - self.MAX_HISTORY_MESSAGES\n        trim_point = 0\n\n        for i in range(trim_target):\n            msg = self.messages[i]\n            next_msg = self.messages[i + 1] if i + 1 < len(self.messages) else None\n\n            # Don't trim an assistant message if the next message is a tool result\n            if msg.role == \"assistant\" and msg.tool_calls and next_msg and next_msg.role == \"tool\":\n                continue\n\n            # Don't trim a tool message (it needs its preceding assistant message)\n            if msg.role == \"tool\":\n                continue\n\n            trim_point = i + 1\n\n        self.messages = self.messages[trim_point:]\n\n    def get_messages_for_llm(self) -> List[Dict]:\n        \"\"\"\n        Get messages formatted for LLM API.\n\n        Validates message ordering to ensure tool messages follow their\n        corresponding assistant message with tool_calls (OpenAI requirement).\n        \"\"\"\n        messages = [{\"role\": \"system\", \"content\": self.system_prompt}]\n\n        # Track tool_call_ids that have been declared by assistant messages\n        declared_tool_call_ids: set = set()\n\n        for msg in self.messages:\n            # Track tool_calls from assistant messages\n            if msg.role == \"assistant\" and msg.tool_calls:\n                for tc in msg.tool_calls:\n                    if isinstance(tc, dict) and \"id\" in tc:\n                        declared_tool_call_ids.add(tc[\"id\"])\n\n            # Skip orphaned tool messages (no preceding assistant with matching tool_calls)\n            if msg.role == \"tool\" and msg.tool_call_id:\n                if msg.tool_call_id not in declared_tool_call_ids:\n                    logger.warning(f\"Skipping orphaned tool message with tool_call_id={msg.tool_call_id}\")\n                    continue\n\n            message_dict = {\"role\": msg.role, \"content\": msg.content}\n\n            if msg.tool_call_id:\n                message_dict[\"tool_call_id\"] = msg.tool_call_id\n\n            if msg.tool_calls:\n                message_dict[\"tool_calls\"] = msg.tool_calls\n\n            messages.append(message_dict)\n\n        return messages\n\n    def clear(self) -> None:\n        \"\"\"Clear conversation history.\"\"\"\n        self.messages = []\n\n\n# ==============================================================================\n# Tool Registry\n# ==============================================================================\n\n\nclass ToolRegistry:\n    \"\"\"\n    Registry of available tools for the Thinker.\n\n    Tools are registered with their OpenAI function schema and\n    an async handler function.\n    \"\"\"\n\n    def __init__(self):\n        self._tools: Dict[str, Dict] = {}\n        self._handlers: Dict[str, Callable] = {}\n\n    def register(\n        self,\n        name: str,\n        description: str,\n        parameters: Dict,\n        handler: Callable[[Dict], Awaitable[Any]],\n    ) -> None:\n        \"\"\"Register a tool with its schema and handler.\"\"\"\n        self._tools[name] = {\n            \"type\": \"function\",\n            \"function\": {\n                \"name\": name,\n                \"description\": description,\n                \"parameters\": parameters,\n            },\n        }\n        self._handlers[name] = handler\n\n    def get_tools_schema(self) -> List[Dict]:\n        \"\"\"Get all tool schemas for LLM API.\"\"\"\n        return list(self._tools.values())\n\n    async def execute(self, tool_name: str, arguments: Dict, user_id: Optional[str] = None) -> Any:\n        \"\"\"Execute a tool and return its result.\"\"\"\n        handler = self._handlers.get(tool_name)\n        if not handler:\n            raise ValueError(f\"Unknown tool: {tool_name}\")\n\n        return await handler(arguments, user_id)\n\n    def has_tools(self) -> bool:\n        \"\"\"Check if any tools are registered.\"\"\"\n        return len(self._tools) > 0\n\n\n# ==============================================================================\n# Thinker Service\n# ==============================================================================\n\n\nclass ThinkerService:\n    \"\"\"\n    Unified reasoning service for the Thinker/Talker pipeline.\n\n    Handles:\n    - Conversation context management (persisted across turns)\n    - Streaming LLM responses with token callbacks\n    - Tool calling with result injection\n    - Cancellation support\n\n    Usage:\n        thinker = ThinkerService()\n\n        # Create a session\n        session = thinker.create_session(\n            conversation_id=\"conv-123\",\n            on_token=handle_token,\n            on_tool_call=handle_tool_call,\n            on_tool_result=handle_tool_result,\n        )\n\n        # Process user input\n        response = await session.think(\"What are the symptoms of diabetes?\")\n    \"\"\"\n\n    # Class-level context storage for conversation memory\n    _conversation_contexts: Dict[str, ConversationContext] = {}\n    _context_last_access: Dict[str, float] = {}\n    CONTEXT_TTL_SECONDS = 3600  # 1 hour TTL for conversation contexts\n\n    def __init__(self):\n        self._llm_client = LLMClient(\n            cloud_model=settings.MODEL_SELECTION_DEFAULT or \"gpt-4o\",\n            openai_api_key=settings.OPENAI_API_KEY,\n            openai_timeout_sec=settings.OPENAI_TIMEOUT_SEC,\n        )\n        self._tool_registry = ToolRegistry()\n        self._setup_default_tools()\n\n    def _get_or_create_context(\n        self,\n        conversation_id: str,\n        system_prompt: Optional[str] = None,\n    ) -> ConversationContext:\n        \"\"\"\n        Get existing conversation context or create a new one.\n\n        Maintains conversation memory across multiple turns/sessions.\n        \"\"\"\n        # Clean up old contexts first\n        self._cleanup_old_contexts()\n\n        # Check if we have an existing context\n        if conversation_id in ThinkerService._conversation_contexts:\n            context = ThinkerService._conversation_contexts[conversation_id]\n            ThinkerService._context_last_access[conversation_id] = time.time()\n            logger.debug(f\"Reusing existing context for {conversation_id} \" f\"with {len(context.messages)} messages\")\n            return context\n\n        # Create new context\n        context = ConversationContext(conversation_id, system_prompt)\n        ThinkerService._conversation_contexts[conversation_id] = context\n        ThinkerService._context_last_access[conversation_id] = time.time()\n        logger.info(f\"Created new conversation context for {conversation_id}\")\n        return context\n\n    def _cleanup_old_contexts(self) -> None:\n        \"\"\"Remove conversation contexts that haven't been accessed recently.\"\"\"\n        current_time = time.time()\n        expired = [\n            conv_id\n            for conv_id, last_access in ThinkerService._context_last_access.items()\n            if current_time - last_access > self.CONTEXT_TTL_SECONDS\n        ]\n        for conv_id in expired:\n            del ThinkerService._conversation_contexts[conv_id]\n            del ThinkerService._context_last_access[conv_id]\n            logger.debug(f\"Cleaned up expired context: {conv_id}\")\n\n    def _setup_default_tools(self) -> None:\n        \"\"\"Set up default tools for voice mode queries.\"\"\"\n        # Register tools from the unified ToolService\n        # This gives voice mode access to calendar, search, medical tools, etc.\n\n        # Get all OpenAI-formatted tool schemas\n        all_tools = tool_service.get_openai_tools()\n        logger.info(f\"Registering {len(all_tools)} tools for voice mode\")\n\n        for tool_def in all_tools:\n            func = tool_def.get(\"function\", {})\n            tool_name = func.get(\"name\", \"\")\n\n            # Create a wrapper handler that bridges to the unified tool service\n            async def tool_handler(\n                arguments: Dict,\n                user_id: Optional[str] = None,\n                _tool_name: str = tool_name,  # Capture tool name in closure\n            ) -> Any:\n                from app.core.database import AsyncSessionLocal\n\n                # Create execution context with real user_id and database session\n                async with AsyncSessionLocal() as db_session:\n                    context = ToolExecutionContext(\n                        user_id=user_id or \"anonymous\",\n                        mode=\"voice\",\n                        db_session=db_session,\n                    )\n\n                    # Execute via the unified service\n                    result = await tool_service.execute(_tool_name, arguments, context)\n\n                    # Return the result in a format suitable for LLM\n                    if result.success:\n                        # Include both message and data for tools that return structured data\n                        # This ensures the LLM can see event IDs, search results, etc.\n                        if result.data and result.message:\n                            # Format data as JSON for structured results\n                            import json\n\n                            data_str = json.dumps(result.data, default=str)\n                            return f\"{result.message}\\n\\nData: {data_str}\"\n                        return result.message or result.data or \"Done.\"\n                    else:\n                        error_msg = result.error or \"Tool execution failed.\"\n                        logger.warning(f\"Tool {_tool_name} failed: {error_msg}\")\n                        return error_msg\n\n            # Register with our internal registry\n            self._tool_registry.register(\n                name=tool_name,\n                description=func.get(\"description\", \"\"),\n                parameters=func.get(\"parameters\", {}),\n                handler=tool_handler,\n            )\n\n        logger.info(\n            f\"Tool registry initialized with {len(all_tools)} tools: \" f\"{[t['function']['name'] for t in all_tools]}\"\n        )\n\n    def create_session(\n        self,\n        conversation_id: str,\n        on_token: Optional[Callable[[str], Awaitable[None]]] = None,\n        on_tool_call: Optional[Callable[[ToolCallEvent], Awaitable[None]]] = None,\n        on_tool_result: Optional[Callable[[ToolResultEvent], Awaitable[None]]] = None,\n        system_prompt: Optional[str] = None,\n        user_id: Optional[str] = None,\n    ) -> \"ThinkerSession\":\n        \"\"\"\n        Create a new thinking session.\n\n        Reuses existing conversation context if available for the conversation_id,\n        maintaining memory across multiple turns in the same conversation.\n\n        Args:\n            conversation_id: Unique conversation identifier\n            on_token: Callback for each generated token\n            on_tool_call: Callback when a tool is called\n            on_tool_result: Callback when a tool returns\n            system_prompt: Optional custom system prompt\n            user_id: User ID for tool authentication (required for calendar, etc.)\n\n        Returns:\n            ThinkerSession for processing queries\n        \"\"\"\n        # Get or create context - this maintains conversation memory across turns\n        context = self._get_or_create_context(conversation_id, system_prompt)\n\n        return ThinkerSession(\n            llm_client=self._llm_client,\n            tool_registry=self._tool_registry,\n            context=context,\n            on_token=on_token,\n            on_tool_call=on_tool_call,\n            on_tool_result=on_tool_result,\n            user_id=user_id,\n        )\n\n    def register_tool(\n        self,\n        name: str,\n        description: str,\n        parameters: Dict,\n        handler: Callable[[Dict], Awaitable[Any]],\n    ) -> None:\n        \"\"\"Register a tool for use in thinking sessions.\"\"\"\n        self._tool_registry.register(name, description, parameters, handler)\n\n\nclass ThinkerSession:\n    \"\"\"\n    A single thinking session with streaming support.\n\n    Manages the flow:\n    1. Receive user input\n    2. Add to conversation context\n    3. Call LLM with streaming\n    4. Handle tool calls if needed\n    5. Stream response tokens to callback\n    \"\"\"\n\n    def __init__(\n        self,\n        llm_client: LLMClient,\n        tool_registry: ToolRegistry,\n        context: ConversationContext,\n        on_token: Optional[Callable[[str], Awaitable[None]]] = None,\n        on_tool_call: Optional[Callable[[ToolCallEvent], Awaitable[None]]] = None,\n        on_tool_result: Optional[Callable[[ToolResultEvent], Awaitable[None]]] = None,\n        user_id: Optional[str] = None,\n    ):\n        self._llm_client = llm_client\n        self._tool_registry = tool_registry\n        self._context = context\n        self._on_token = on_token\n        self._on_tool_call = on_tool_call\n        self._on_tool_result = on_tool_result\n        self._user_id = user_id  # User ID for tool authentication\n\n        self._state = ThinkingState.IDLE\n        self._cancelled = False\n        self._metrics = ThinkerMetrics()\n        self._start_time: Optional[float] = None\n        self._first_token_time: Optional[float] = None\n\n    @property\n    def state(self) -> ThinkingState:\n        \"\"\"Get current state.\"\"\"\n        return self._state\n\n    def is_cancelled(self) -> bool:\n        \"\"\"Check if session was cancelled.\"\"\"\n        return self._cancelled\n\n    async def think(\n        self,\n        user_input: str,\n        source_mode: str = \"voice\",\n        emotion_context: Optional[Dict] = None,\n        memory_context: Optional[Dict] = None,\n        transcript_confidence: float = 1.0,\n        session_id: Optional[str] = None,\n    ) -> ThinkerResponse:\n        \"\"\"\n        Process user input and generate a response.\n\n        Args:\n            user_input: The user's question or statement\n            source_mode: \"voice\" or \"chat\"\n            emotion_context: Optional emotion context for response adaptation\n                - emotion: EmotionResult from emotion detection\n                - trend: EmotionTrend for trending analysis\n                - prompt_addition: String to add to system prompt\n            memory_context: Optional conversation memory context (Phase 4)\n                - current_topic: What user is discussing\n                - recent_entities: People/places mentioned\n                - emotional_state: User's emotional state\n                - context_items: Other relevant context\n            transcript_confidence: STT confidence score (0-1) for Phase 7 repair\n            session_id: Session ID for repair strategy tracking\n\n        Returns:\n            ThinkerResponse with the generated text and metadata\n        \"\"\"\n        if self._cancelled:\n            return ThinkerResponse(\n                text=\"\",\n                message_id=\"\",\n                state=ThinkingState.CANCELLED,\n            )\n\n        self._start_time = time.time()\n        self._state = ThinkingState.PROCESSING\n        message_id = str(uuid.uuid4())\n\n        # Add user message to context\n        self._context.add_message(\"user\", user_input, source_mode=source_mode)\n\n        try:\n            # Build LLM request\n            messages = self._context.get_messages_for_llm()\n\n            # Inject emotion context into system prompt if available\n            if emotion_context and emotion_context.get(\"prompt_addition\"):\n                # Find the system message and append emotion context\n                for msg in messages:\n                    if msg.get(\"role\") == \"system\":\n                        msg[\"content\"] = msg[\"content\"] + \"\\n\" + emotion_context[\"prompt_addition\"]\n                        break\n\n            # Phase 4: Inject memory context into system prompt if available\n            if memory_context:\n                memory_prompt = self._build_memory_prompt(memory_context)\n                if memory_prompt:\n                    for msg in messages:\n                        if msg.get(\"role\") == \"system\":\n                            msg[\"content\"] = msg[\"content\"] + \"\\n\" + memory_prompt\n                            break\n\n            # Get tool schemas if available\n            tools = None\n            if self._tool_registry.has_tools():\n                tools = self._tool_registry.get_tools_schema()\n\n            # Stream response\n            self._state = ThinkingState.GENERATING\n            full_response = \"\"\n            text_before_tool = \"\"  # Track text before any tool calls\n            tool_calls_made = []\n            had_tool_calls = False\n\n            async for chunk in self._stream_llm(messages, tools):\n                if self._cancelled:\n                    self._state = ThinkingState.CANCELLED\n                    break\n\n                if isinstance(chunk, str):\n                    # Text token\n                    full_response += chunk\n                    if not had_tool_calls:\n                        text_before_tool += chunk\n                    if self._on_token:\n                        await self._on_token(chunk)\n\n                    # Track first token latency\n                    if self._first_token_time is None:\n                        self._first_token_time = time.time()\n                        self._metrics.first_token_latency_ms = int((self._first_token_time - self._start_time) * 1000)\n\n                elif isinstance(chunk, ToolCall):\n                    # Tool call - _handle_tool_call adds all necessary messages to context\n                    # and calls LLM again to get response based on tool result\n                    self._state = ThinkingState.TOOL_CALLING\n                    tool_calls_made.append(chunk.name)\n                    had_tool_calls = True\n                    # Pass any text that came before this tool call\n                    follow_up = await self._handle_tool_call(chunk, text_before_tool)\n                    full_response += follow_up\n                    text_before_tool = \"\"  # Reset for potential next tool call\n                    self._state = ThinkingState.GENERATING\n\n            # Only add assistant response if there were NO tool calls\n            # (tool calls handle their own context additions)\n            if not had_tool_calls and full_response:\n                self._context.add_message(\n                    \"assistant\",\n                    full_response,\n                    source_mode=source_mode,\n                )\n\n            self._metrics.total_latency_ms = int((time.time() - self._start_time) * 1000)\n\n            if self._cancelled:\n                self._state = ThinkingState.CANCELLED\n                self._metrics.cancelled = True\n            else:\n                self._state = ThinkingState.COMPLETE\n\n            # Phase 7: Apply conversational repair strategy if needed\n            response_confidence, possible_interpretations = self._estimate_confidence(user_input, transcript_confidence)\n\n            repair_applied = False\n            needs_clarification = False\n            final_response = full_response\n\n            # Only apply repair strategies in voice mode and when confidence is low\n            if source_mode == \"voice\" and response_confidence < 0.7:\n                repair_recommendation = repair_strategy_service.get_repair_recommendation(\n                    transcript=user_input,\n                    transcript_confidence=transcript_confidence,\n                    response_confidence=response_confidence,\n                    session_id=session_id,\n                    possible_interpretations=possible_interpretations,\n                )\n\n                if repair_recommendation.strategy != RepairStrategy.NO_REPAIR:\n                    final_response = repair_strategy_service.apply_repair(repair_recommendation, full_response)\n                    repair_applied = True\n                    needs_clarification = repair_recommendation.strategy in [\n                        RepairStrategy.ECHO_CHECK,\n                        RepairStrategy.CLARIFY_SPECIFIC,\n                        RepairStrategy.REQUEST_REPHRASE,\n                    ]\n\n                    logger.info(\n                        f\"Applied repair strategy: {repair_recommendation.strategy.value}, \"\n                        f\"confidence: {response_confidence:.2f}\"\n                    )\n\n            return ThinkerResponse(\n                text=final_response,\n                message_id=message_id,\n                tool_calls_made=tool_calls_made,\n                latency_ms=self._metrics.total_latency_ms,\n                tokens_used=self._metrics.total_tokens,\n                state=self._state,\n                confidence=response_confidence,\n                needs_clarification=needs_clarification,\n                repair_applied=repair_applied,\n            )\n\n        except Exception as e:\n            logger.error(f\"Thinker error: {e}\")\n            self._state = ThinkingState.ERROR\n            return ThinkerResponse(\n                text=f\"I apologize, but I encountered an error: {str(e)}\",\n                message_id=message_id,\n                state=ThinkingState.ERROR,\n            )\n\n    async def _stream_llm(\n        self,\n        messages: List[Dict],\n        tools: Optional[List[Dict]] = None,\n    ) -> AsyncIterator[str | ToolCall]:\n        \"\"\"\n        Stream LLM response, yielding text tokens and tool calls.\n\n        Args:\n            messages: Conversation messages for LLM\n            tools: Optional tool schemas\n\n        Yields:\n            Text tokens (str) or ToolCall objects\n        \"\"\"\n        req = LLMRequest(\n            messages=messages,\n            tools=tools,\n            tool_choice=\"auto\" if tools else None,\n            temperature=0.7,\n            max_tokens=1024,\n        )\n\n        # Use streaming generation\n        accumulated_text = []\n\n        async def chunk_callback(chunk: str) -> None:\n            accumulated_text.append(chunk)\n\n        response = await self._llm_client.stream_generate(req, on_chunk=chunk_callback)\n\n        # Yield accumulated text\n        for chunk in accumulated_text:\n            yield chunk\n\n        # Check for tool calls\n        if response.tool_calls:\n            for tool_call in response.tool_calls:\n                yield tool_call\n\n        self._metrics.total_tokens = response.used_tokens\n\n    async def _handle_tool_call(\n        self,\n        tool_call: ToolCall,\n        preceding_text: str = \"\",\n    ) -> str:\n        \"\"\"\n        Handle a tool call from the LLM.\n\n        Args:\n            tool_call: The tool call to execute\n            preceding_text: Any text the LLM generated before the tool call\n\n        Returns the follow-up response from the LLM after processing the tool result.\n        \"\"\"\n        try:\n            # Parse arguments\n            arguments = json.loads(tool_call.arguments)\n            logger.info(f\"Executing tool: {tool_call.name} with args: {arguments}\")\n\n            # Notify callback\n            if self._on_tool_call:\n                await self._on_tool_call(\n                    ToolCallEvent(\n                        tool_id=tool_call.id,\n                        tool_name=tool_call.name,\n                        arguments=arguments,\n                    )\n                )\n\n            # IMPORTANT: Add the assistant message WITH tool_calls BEFORE adding tool result\n            # OpenAI requires: assistant (with tool_calls) -> tool (with tool_call_id)\n            # Include any preceding text in this message\n            self._context.add_message(\n                \"assistant\",\n                preceding_text,  # Include text that came before tool call\n                tool_calls=[\n                    {\n                        \"id\": tool_call.id,\n                        \"type\": \"function\",\n                        \"function\": {\n                            \"name\": tool_call.name,\n                            \"arguments\": tool_call.arguments,\n                        },\n                    }\n                ],\n            )\n\n            # Execute tool with user_id for authentication\n            self._metrics.tool_calls_count += 1\n            result = await self._tool_registry.execute(tool_call.name, arguments, self._user_id)\n            result_str = json.dumps(result) if not isinstance(result, str) else result\n            logger.info(f\"Tool {tool_call.name} returned (user_id={self._user_id}): {result_str[:200]}...\")\n\n            # Add tool result to context\n            self._context.add_message(\n                \"tool\",\n                result_str,\n                tool_call_id=tool_call.id,\n            )\n\n            # Notify callback\n            if self._on_tool_result:\n                await self._on_tool_result(\n                    ToolResultEvent(\n                        tool_id=tool_call.id,\n                        tool_name=tool_call.name,\n                        result=result,\n                    )\n                )\n\n            # Now call LLM again to get the response based on tool result\n            messages = self._context.get_messages_for_llm()\n            follow_up_response = \"\"\n\n            async for chunk in self._stream_llm(messages, tools=None):\n                if self._cancelled:\n                    break\n                if isinstance(chunk, str):\n                    follow_up_response += chunk\n                    if self._on_token:\n                        await self._on_token(chunk)\n\n            # Add the follow-up response to context\n            if follow_up_response:\n                self._context.add_message(\n                    \"assistant\",\n                    follow_up_response,\n                )\n\n            return follow_up_response\n\n        except Exception as e:\n            logger.error(f\"Tool execution error: {e}\")\n            # Add the assistant message with tool_calls first\n            self._context.add_message(\n                \"assistant\",\n                preceding_text,\n                tool_calls=[\n                    {\n                        \"id\": tool_call.id,\n                        \"type\": \"function\",\n                        \"function\": {\n                            \"name\": tool_call.name,\n                            \"arguments\": tool_call.arguments,\n                        },\n                    }\n                ],\n            )\n            # Add error result to context\n            self._context.add_message(\n                \"tool\",\n                f\"Error: {str(e)}\",\n                tool_call_id=tool_call.id,\n            )\n            error_response = f\"I encountered an error while trying to help: {str(e)}\"\n            # Add error response to context\n            self._context.add_message(\"assistant\", error_response)\n            return error_response\n\n    async def cancel(self) -> None:\n        \"\"\"Cancel the thinking session.\"\"\"\n        self._cancelled = True\n        self._state = ThinkingState.CANCELLED\n        self._metrics.cancelled = True\n        logger.info(\"Thinker session cancelled\")\n\n    def _build_memory_prompt(self, memory_context: Dict) -> Optional[str]:\n        \"\"\"\n        Build a prompt addition from conversation memory context.\n\n        Phase 4: Memory & Context Enhancement\n\n        Args:\n            memory_context: Dict with conversation context summary\n                - current_topic: What the user is discussing\n                - recent_entities: People/places/terms mentioned\n                - emotional_state: User's current emotional state\n                - context_items: Other relevant context\n\n        Returns:\n            String to append to system prompt, or None if empty context\n        \"\"\"\n        parts = []\n\n        # Add current topic\n        if memory_context.get(\"current_topic\"):\n            parts.append(f\"Current topic: {memory_context['current_topic']}\")\n\n        # Add recent entities for reference resolution\n        entities = memory_context.get(\"recent_entities\", [])\n        if entities:\n            parts.append(f\"Recently mentioned: {', '.join(entities[:5])}\")\n\n        # Add emotional state for tone adaptation\n        if memory_context.get(\"emotional_state\"):\n            parts.append(f\"User's emotional state: {memory_context['emotional_state']}\")\n\n        if not parts:\n            return None\n\n        return \"\\n[Conversation Context]\\n\" + \"\\n\".join(parts)\n\n    def _estimate_confidence(\n        self,\n        user_input: str,\n        transcript_confidence: float = 1.0,\n    ) -> tuple[float, list[str]]:\n        \"\"\"\n        Phase 7: Estimate AI's confidence in understanding the user's query.\n\n        Analyzes query characteristics to determine how confident we should be\n        in our understanding before responding.\n\n        Args:\n            user_input: The user's transcribed text\n            transcript_confidence: STT confidence score (0-1)\n\n        Returns:\n            Tuple of (confidence_score, possible_interpretations)\n        \"\"\"\n        import re\n\n        confidence = 1.0\n        possible_interpretations: list[str] = []\n\n        # Factor 1: Transcript confidence from STT\n        if transcript_confidence < 0.7:\n            confidence *= 0.7\n        elif transcript_confidence < 0.85:\n            confidence *= 0.85\n\n        # Factor 2: Query length (very short queries are often ambiguous)\n        words = user_input.strip().split()\n        word_count = len(words)\n        if word_count <= 2:\n            confidence *= 0.7\n            possible_interpretations.append(\"short query - may need clarification\")\n        elif word_count <= 4:\n            confidence *= 0.85\n\n        # Factor 3: Ambiguous pronouns without clear referent\n        ambiguous_pronouns = re.findall(r\"\\b(it|that|this|those|them|they|he|she)\\b\", user_input.lower())\n        if ambiguous_pronouns and word_count < 6:\n            # Check if we have recent context that could resolve the pronoun\n            recent_messages = self._context.messages[-3:] if len(self._context.messages) >= 3 else []\n            has_context = len(recent_messages) > 0\n            if not has_context:\n                confidence *= 0.75\n                possible_interpretations.append(f\"ambiguous reference: '{ambiguous_pronouns[0]}'\")\n\n        # Factor 4: Question words that suggest incomplete information\n        incomplete_patterns = [\n            r\"^\\s*(what|which|where|when|how)\\s*\\??\\s*$\",  # Single question word\n            r\"^\\s*(um+|uh+|er+|ah+)\\s\",  # Hesitation markers at start\n        ]\n        for pattern in incomplete_patterns:\n            if re.search(pattern, user_input.lower()):\n                confidence *= 0.6\n                possible_interpretations.append(\"incomplete query detected\")\n                break\n\n        # Factor 5: Multiple possible interpretations\n        # Check for words that have multiple common meanings\n        ambiguous_terms = {\n            \"book\": [\"make a reservation\", \"a physical book\"],\n            \"set\": [\"configure\", \"a collection\"],\n            \"run\": [\"execute\", \"physical exercise\"],\n            \"check\": [\"verify\", \"medical checkup\", \"a bank check\"],\n            \"light\": [\"turn on light\", \"not heavy\"],\n        }\n        for term, meanings in ambiguous_terms.items():\n            if re.search(rf\"\\b{term}\\b\", user_input.lower()):\n                # Only flag if context doesn't disambiguate\n                if word_count < 5:\n                    confidence *= 0.85\n                    possible_interpretations.extend(meanings[:2])\n                    break\n\n        # Cap confidence between 0.3 and 1.0\n        confidence = max(0.3, min(1.0, confidence))\n\n        return confidence, possible_interpretations\n\n    def get_context(self) -> ConversationContext:\n        \"\"\"Get the conversation context.\"\"\"\n        return self._context\n\n    def get_metrics(self) -> ThinkerMetrics:\n        \"\"\"Get session metrics.\"\"\"\n        return self._metrics\n\n\n# Global service instance\nthinker_service = ThinkerService()\n"
}
