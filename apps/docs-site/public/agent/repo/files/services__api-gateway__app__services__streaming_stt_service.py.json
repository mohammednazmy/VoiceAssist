{
  "path": "services/api-gateway/app/services/streaming_stt_service.py",
  "language": "python",
  "size": 20230,
  "last_modified": "2025-12-04T11:27:00.715Z",
  "lines": 589,
  "content": "\"\"\"\nStreaming Speech-to-Text Service\n\nProvides real-time speech transcription with multiple provider support.\nPrimary: Deepgram (100-150ms latency, true streaming)\nFallback: OpenAI Whisper (batch API)\n\nPhase: Thinker/Talker Voice Pipeline Migration\n\"\"\"\n\nimport asyncio\nimport json\nimport time\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nfrom typing import Awaitable, Callable, Dict, List, Optional\n\nimport httpx\nimport websockets\nfrom app.core.config import settings\nfrom app.core.logging import get_logger\nfrom websockets.exceptions import ConnectionClosed\n\nlogger = get_logger(__name__)\n\n\n# ==============================================================================\n# Data Classes\n# ==============================================================================\n\n\nclass STTProvider(str, Enum):\n    \"\"\"Supported STT providers.\"\"\"\n\n    DEEPGRAM = \"deepgram\"\n    WHISPER = \"whisper\"\n\n\n@dataclass\nclass TranscriptChunk:\n    \"\"\"A chunk of transcribed text.\"\"\"\n\n    text: str\n    is_final: bool\n    confidence: float = 0.0\n    start_time: float = 0.0\n    end_time: float = 0.0\n    words: List[Dict] = field(default_factory=list)\n    # Prosody-related fields\n    speech_final: bool = False  # True when speech segment is complete\n\n\n@dataclass\nclass TranscriptionResult:\n    \"\"\"Final result of a transcription session.\"\"\"\n\n    text: str\n    confidence: float\n    duration_ms: int\n    provider: STTProvider\n    language: Optional[str] = None\n    words: List[Dict] = field(default_factory=list)\n\n\n@dataclass\nclass STTSessionConfig:\n    \"\"\"Configuration for an STT session.\"\"\"\n\n    language: str = \"en\"\n    sample_rate: int = 16000\n    encoding: str = \"linear16\"  # PCM16\n    channels: int = 1\n    interim_results: bool = True\n    punctuate: bool = True\n    # Endpointing: Time of silence before considering speech ended\n    # 800ms allows for natural pauses in speech (was 120ms - too aggressive)\n    endpointing_ms: int = 800\n    # Utterance end: Time after speech stops before finalizing\n    # 1500ms allows users to think mid-sentence (was 1000ms hardcoded)\n    utterance_end_ms: int = 1500\n    vad_events: bool = True\n    smart_format: bool = True\n\n\n# ==============================================================================\n# Deepgram Streaming STT\n# ==============================================================================\n\n\nclass DeepgramStreamingSession:\n    \"\"\"\n    Real-time streaming STT session using Deepgram WebSocket API.\n\n    Features:\n    - True streaming transcription (100-150ms latency)\n    - Interim results as user speaks\n    - Endpoint detection (speech end)\n    - VAD events\n    \"\"\"\n\n    WEBSOCKET_URL = \"wss://api.deepgram.com/v1/listen\"\n\n    def __init__(\n        self,\n        api_key: str,\n        config: STTSessionConfig,\n        on_partial: Callable[[str, float], Awaitable[None]],\n        on_final: Callable[[str], Awaitable[None]],\n        on_endpoint: Callable[[], Awaitable[None]],\n        on_speech_start: Optional[Callable[[], Awaitable[None]]] = None,\n        on_words: Optional[Callable[[List[Dict]], Awaitable[None]]] = None,\n    ):\n        self.api_key = api_key\n        self.config = config\n        self.on_partial = on_partial\n        self.on_final = on_final\n        self.on_endpoint = on_endpoint\n        self.on_speech_start = on_speech_start\n        self.on_words = on_words  # Callback for word-level data (prosody analysis)\n\n        self._websocket: Optional[websockets.WebSocketClientProtocol] = None\n        self._running = False\n        self._receive_task: Optional[asyncio.Task] = None\n        self._final_transcript = \"\"\n        self._start_time: float = 0\n\n    def _build_url(self) -> str:\n        \"\"\"Build WebSocket URL with query parameters.\"\"\"\n        params = [\n            f\"language={self.config.language}\",\n            f\"sample_rate={self.config.sample_rate}\",\n            f\"encoding={self.config.encoding}\",\n            f\"channels={self.config.channels}\",\n            f\"interim_results={str(self.config.interim_results).lower()}\",\n            f\"punctuate={str(self.config.punctuate).lower()}\",\n            f\"endpointing={self.config.endpointing_ms}\",\n            f\"vad_events={str(self.config.vad_events).lower()}\",\n            f\"smart_format={str(self.config.smart_format).lower()}\",\n            \"model=nova-2\",  # Best accuracy/latency balance\n            f\"utterance_end_ms={self.config.utterance_end_ms}\",  # Send UtteranceEnd after speech stops\n        ]\n        logger.info(\n            f\"[Deepgram] Connecting with params: endpointing={self.config.endpointing_ms}ms, \"\n            f\"utterance_end={self.config.utterance_end_ms}ms\"\n        )\n        return f\"{self.WEBSOCKET_URL}?{'&'.join(params)}\"\n\n    async def start(self) -> bool:\n        \"\"\"Start the streaming session.\"\"\"\n        if self._running:\n            logger.warning(\"Deepgram session already running\")\n            return False\n\n        try:\n            url = self._build_url()\n            headers = {\"Authorization\": f\"Token {self.api_key}\"}\n\n            self._websocket = await websockets.connect(\n                url,\n                additional_headers=headers,\n                ping_interval=20,\n                ping_timeout=10,\n            )\n\n            self._running = True\n            self._start_time = time.time()\n            self._final_transcript = \"\"\n\n            # Start receiving messages\n            self._receive_task = asyncio.create_task(self._receive_loop())\n\n            logger.info(\"Deepgram streaming session started\")\n            return True\n\n        except Exception as e:\n            logger.error(f\"Failed to start Deepgram session: {e}\")\n            self._running = False\n            return False\n\n    async def send_audio(self, audio_chunk: bytes) -> None:\n        \"\"\"Send audio chunk to Deepgram.\"\"\"\n        if not self._running or not self._websocket:\n            if not hasattr(self, \"_drop_logged\"):\n                logger.warning(f\"[Deepgram] Dropping audio: running={self._running}, ws={self._websocket is not None}\")\n                self._drop_logged = True\n            return\n\n        try:\n            # Track audio sent\n            if not hasattr(self, \"_audio_sent_count\"):\n                self._audio_sent_count = 0\n            self._audio_sent_count += 1\n            if self._audio_sent_count % 50 == 0:\n                logger.debug(f\"[Deepgram] Sent audio chunk #{self._audio_sent_count}, {len(audio_chunk)} bytes\")\n            await self._websocket.send(audio_chunk)\n        except ConnectionClosed:\n            logger.warning(\"Deepgram connection closed while sending audio\")\n            self._running = False\n        except Exception as e:\n            logger.error(f\"Error sending audio to Deepgram: {e}\")\n\n    async def stop(self) -> str:\n        \"\"\"Stop the session and return final transcript.\"\"\"\n        if not self._running:\n            return self._final_transcript\n\n        self._running = False\n\n        try:\n            # Send close message to Deepgram\n            if self._websocket:\n                await self._websocket.send(json.dumps({\"type\": \"CloseStream\"}))\n                await asyncio.sleep(0.1)  # Allow final messages\n                await self._websocket.close()\n        except Exception as e:\n            logger.debug(f\"Error closing Deepgram connection: {e}\")\n\n        # Cancel receive task\n        if self._receive_task:\n            self._receive_task.cancel()\n            try:\n                await self._receive_task\n            except asyncio.CancelledError:\n                pass\n\n        logger.info(\n            \"Deepgram session stopped\",\n            extra={\n                \"duration_ms\": int((time.time() - self._start_time) * 1000),\n                \"transcript_length\": len(self._final_transcript),\n            },\n        )\n\n        return self._final_transcript\n\n    async def _receive_loop(self) -> None:\n        \"\"\"Receive and process messages from Deepgram.\"\"\"\n        try:\n            while self._running and self._websocket:\n                try:\n                    message = await asyncio.wait_for(self._websocket.recv(), timeout=30.0)\n                    await self._handle_message(message)\n                except asyncio.TimeoutError:\n                    # Send keepalive\n                    if self._websocket:\n                        await self._websocket.send(json.dumps({\"type\": \"KeepAlive\"}))\n                except ConnectionClosed:\n                    logger.info(\"Deepgram connection closed\")\n                    break\n        except asyncio.CancelledError:\n            pass\n        except Exception as e:\n            logger.error(f\"Error in Deepgram receive loop: {e}\")\n        finally:\n            self._running = False\n\n    async def _handle_message(self, message: str) -> None:\n        \"\"\"Handle a message from Deepgram.\"\"\"\n        try:\n            data = json.loads(message)\n            msg_type = data.get(\"type\", \"\")\n\n            # Log all message types for debugging\n            if not hasattr(self, \"_dg_msg_counts\"):\n                self._dg_msg_counts = {}\n            self._dg_msg_counts[msg_type] = self._dg_msg_counts.get(msg_type, 0) + 1\n            if self._dg_msg_counts[msg_type] <= 3 or self._dg_msg_counts[msg_type] % 50 == 0:\n                logger.info(f\"[Deepgram] Received {msg_type} message #{self._dg_msg_counts[msg_type]}\")\n\n            if msg_type == \"Results\":\n                await self._handle_results(data)\n            elif msg_type == \"UtteranceEnd\":\n                # Speech endpoint detected\n                logger.info(\"[Deepgram] UtteranceEnd - speech endpoint detected\")\n                await self.on_endpoint()\n            elif msg_type == \"SpeechStarted\":\n                logger.info(\"[Deepgram] SpeechStarted detected - triggering barge-in callback\")\n                if self.on_speech_start:\n                    await self.on_speech_start()\n            elif msg_type == \"Metadata\":\n                logger.info(f\"[Deepgram] Metadata: request_id={data.get('request_id', 'N/A')}\")\n            elif msg_type == \"Error\":\n                logger.error(f\"[Deepgram] ERROR: {data}\")\n            else:\n                logger.debug(f\"[Deepgram] Unknown message type: {msg_type}\")\n\n        except json.JSONDecodeError:\n            logger.warning(f\"Invalid JSON from Deepgram: {message[:100]}\")\n\n    async def _handle_results(self, data: dict) -> None:\n        \"\"\"Handle transcription results.\"\"\"\n        channel = data.get(\"channel\", {})\n        alternatives = channel.get(\"alternatives\", [])\n\n        if not alternatives:\n            logger.debug(\"[Deepgram] Results with no alternatives\")\n            return\n\n        best = alternatives[0]\n        transcript = best.get(\"transcript\", \"\")\n        confidence = best.get(\"confidence\", 0.0)\n        is_final = data.get(\"is_final\", False)\n        words = best.get(\"words\", [])\n\n        # Log all results for debugging\n        if transcript:\n            logger.info(f\"[Deepgram] TRANSCRIPT: '{transcript}' (final={is_final}, conf={confidence:.2f})\")\n        else:\n            # Log empty transcripts occasionally\n            if not hasattr(self, \"_empty_count\"):\n                self._empty_count = 0\n            self._empty_count += 1\n            if self._empty_count <= 3 or self._empty_count % 20 == 0:\n                logger.debug(f\"[Deepgram] Empty transcript #{self._empty_count} (final={is_final})\")\n            return\n\n        # Send word-level data for prosody analysis\n        if words and self.on_words:\n            await self.on_words(words)\n\n        if is_final:\n            # Append to final transcript\n            if self._final_transcript:\n                self._final_transcript += \" \" + transcript\n            else:\n                self._final_transcript = transcript\n\n            logger.info(f\"[Deepgram] Calling on_final callback with: '{transcript}'\")\n            await self.on_final(transcript)\n        else:\n            # Interim result\n            logger.info(f\"[Deepgram] Calling on_partial callback with: '{transcript}'\")\n            await self.on_partial(transcript, confidence)\n\n\n# ==============================================================================\n# Whisper Fallback STT\n# ==============================================================================\n\n\nclass WhisperSTTService:\n    \"\"\"\n    OpenAI Whisper STT service (batch mode fallback).\n\n    Used when Deepgram is unavailable or for offline processing.\n    Higher latency (~500ms) but more robust.\n    \"\"\"\n\n    TRANSCRIPTION_URL = \"https://api.openai.com/v1/audio/transcriptions\"\n\n    def __init__(self, api_key: str):\n        self.api_key = api_key\n        self._http_client: Optional[httpx.AsyncClient] = None\n\n    async def _get_client(self) -> httpx.AsyncClient:\n        \"\"\"Get or create HTTP client.\"\"\"\n        if self._http_client is None or self._http_client.is_closed:\n            self._http_client = httpx.AsyncClient(\n                timeout=30.0,\n                limits=httpx.Limits(max_connections=5),\n            )\n        return self._http_client\n\n    async def transcribe(\n        self,\n        audio_data: bytes,\n        language: str = \"en\",\n        prompt: Optional[str] = None,\n    ) -> TranscriptionResult:\n        \"\"\"\n        Transcribe audio using Whisper API.\n\n        Args:\n            audio_data: Raw audio bytes (WAV, MP3, etc.)\n            language: Language code\n            prompt: Optional prompt for context\n\n        Returns:\n            TranscriptionResult with text and metadata\n        \"\"\"\n        start_time = time.time()\n\n        try:\n            client = await self._get_client()\n\n            # Prepare multipart form data\n            files = {\n                \"file\": (\"audio.wav\", audio_data, \"audio/wav\"),\n            }\n            data = {\n                \"model\": \"whisper-1\",\n                \"language\": language,\n                \"response_format\": \"verbose_json\",\n            }\n            if prompt:\n                data[\"prompt\"] = prompt\n\n            response = await client.post(\n                self.TRANSCRIPTION_URL,\n                headers={\"Authorization\": f\"Bearer {self.api_key}\"},\n                files=files,\n                data=data,\n            )\n\n            if response.status_code != 200:\n                raise ValueError(f\"Whisper API error: {response.status_code} - {response.text}\")\n\n            result = response.json()\n            duration_ms = int((time.time() - start_time) * 1000)\n\n            return TranscriptionResult(\n                text=result.get(\"text\", \"\"),\n                confidence=1.0,  # Whisper doesn't provide confidence\n                duration_ms=duration_ms,\n                provider=STTProvider.WHISPER,\n                language=result.get(\"language\"),\n                words=result.get(\"words\", []),\n            )\n\n        except Exception as e:\n            logger.error(f\"Whisper transcription failed: {e}\")\n            raise\n\n    async def close(self) -> None:\n        \"\"\"Close the HTTP client.\"\"\"\n        if self._http_client:\n            await self._http_client.aclose()\n            self._http_client = None\n\n\n# ==============================================================================\n# Unified Streaming STT Service\n# ==============================================================================\n\n\nclass StreamingSTTService:\n    \"\"\"\n    Unified streaming STT service with provider fallback.\n\n    Primary: Deepgram (streaming, 100-150ms latency)\n    Fallback: Whisper (batch, ~500ms latency)\n\n    Usage:\n        service = StreamingSTTService()\n        session = await service.create_session(\n            on_partial=handle_partial,\n            on_final=handle_final,\n            on_endpoint=handle_endpoint,\n        )\n        await session.start()\n        await session.send_audio(audio_chunk)\n        final_text = await session.stop()\n    \"\"\"\n\n    def __init__(self):\n        self.deepgram_api_key = settings.DEEPGRAM_API_KEY\n        self.openai_api_key = settings.OPENAI_API_KEY\n\n        self.deepgram_enabled = bool(self.deepgram_api_key)\n        self.whisper_enabled = bool(self.openai_api_key)\n\n        self._whisper_service: Optional[WhisperSTTService] = None\n\n        logger.info(\n            \"StreamingSTTService initialized\",\n            extra={\n                \"deepgram_enabled\": self.deepgram_enabled,\n                \"whisper_enabled\": self.whisper_enabled,\n            },\n        )\n\n    def is_streaming_available(self) -> bool:\n        \"\"\"Check if streaming STT is available.\"\"\"\n        return self.deepgram_enabled\n\n    def is_fallback_available(self) -> bool:\n        \"\"\"Check if fallback STT is available.\"\"\"\n        return self.whisper_enabled\n\n    async def create_session(\n        self,\n        on_partial: Callable[[str, float], Awaitable[None]],\n        on_final: Callable[[str], Awaitable[None]],\n        on_endpoint: Callable[[], Awaitable[None]],\n        on_speech_start: Optional[Callable[[], Awaitable[None]]] = None,\n        on_words: Optional[Callable[[List[Dict]], Awaitable[None]]] = None,\n        config: Optional[STTSessionConfig] = None,\n    ) -> DeepgramStreamingSession:\n        \"\"\"\n        Create a new streaming STT session.\n\n        Args:\n            on_partial: Callback for partial transcripts (text, confidence)\n            on_final: Callback for final transcripts\n            on_endpoint: Callback for speech endpoint detection\n            on_speech_start: Optional callback for speech start (barge-in)\n            on_words: Optional callback for word-level data (prosody analysis)\n            config: Optional session configuration\n\n        Returns:\n            DeepgramStreamingSession instance\n\n        Raises:\n            ValueError: If Deepgram is not available\n        \"\"\"\n        if not self.deepgram_enabled:\n            raise ValueError(\"Deepgram STT is not configured. Check DEEPGRAM_API_KEY.\")\n\n        config = config or STTSessionConfig()\n\n        return DeepgramStreamingSession(\n            api_key=self.deepgram_api_key,\n            config=config,\n            on_partial=on_partial,\n            on_final=on_final,\n            on_endpoint=on_endpoint,\n            on_speech_start=on_speech_start,\n            on_words=on_words,\n        )\n\n    async def transcribe_batch(\n        self,\n        audio_data: bytes,\n        language: str = \"en\",\n        prompt: Optional[str] = None,\n    ) -> TranscriptionResult:\n        \"\"\"\n        Transcribe audio using batch API (Whisper fallback).\n\n        Use this when streaming is unavailable or for offline processing.\n\n        Args:\n            audio_data: Raw audio bytes\n            language: Language code\n            prompt: Optional context prompt\n\n        Returns:\n            TranscriptionResult with transcribed text\n        \"\"\"\n        if not self.whisper_enabled:\n            raise ValueError(\"Whisper STT is not configured. Check OPENAI_API_KEY.\")\n\n        if self._whisper_service is None:\n            self._whisper_service = WhisperSTTService(self.openai_api_key)\n\n        return await self._whisper_service.transcribe(\n            audio_data=audio_data,\n            language=language,\n            prompt=prompt,\n        )\n\n    async def transcribe_with_fallback(\n        self,\n        audio_data: bytes,\n        language: str = \"en\",\n    ) -> TranscriptionResult:\n        \"\"\"\n        Transcribe audio with automatic provider fallback.\n\n        Tries Deepgram first (via batch endpoint), falls back to Whisper.\n\n        Args:\n            audio_data: Raw audio bytes\n            language: Language code\n\n        Returns:\n            TranscriptionResult with provider information\n        \"\"\"\n        # For batch transcription, use Whisper (more reliable)\n        # Deepgram streaming is preferred for real-time\n        if self.whisper_enabled:\n            try:\n                return await self.transcribe_batch(audio_data, language)\n            except Exception as e:\n                logger.warning(f\"Whisper transcription failed, no fallback: {e}\")\n                raise\n\n        raise ValueError(\"No STT provider available\")\n\n    async def close(self) -> None:\n        \"\"\"Clean up resources.\"\"\"\n        if self._whisper_service:\n            await self._whisper_service.close()\n            self._whisper_service = None\n\n\n# Global service instance\nstreaming_stt_service = StreamingSTTService()\n"
}
