{
  "path": "services/api-gateway/app/services/reranking_service.py",
  "language": "python",
  "size": 18798,
  "last_modified": "2025-12-04T11:27:00.151Z",
  "lines": 597,
  "content": "\"\"\"\nRe-ranking Service (Phase 5 - Advanced RAG)\n\nProvides cross-encoder re-ranking for improved search relevance.\n\nFeatures:\n- Cross-encoder scoring (query-document pair classification)\n- Multiple re-ranking strategies\n- Cohere Rerank API integration\n- Local cross-encoder model support (sentence-transformers)\n- Score calibration and normalization\n\nRe-ranking improves precision by scoring each document\nagainst the query, rather than relying only on embedding similarity.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport logging\nfrom dataclasses import dataclass\nfrom enum import Enum\nfrom typing import Any, Dict, List, Optional, Tuple\n\nfrom app.core.config import settings\nfrom app.services.cache_service import cache_service, generate_cache_key\nfrom openai import AsyncOpenAI\n\nlogger = logging.getLogger(__name__)\n\n# Shared async OpenAI client\n_async_openai_client: AsyncOpenAI | None = None\n\n\ndef get_async_openai_client() -> AsyncOpenAI:\n    \"\"\"Get or create async OpenAI client.\"\"\"\n    global _async_openai_client\n    if _async_openai_client is None:\n        _async_openai_client = AsyncOpenAI()\n    return _async_openai_client\n\n\nclass RerankerType(str, Enum):\n    \"\"\"Available re-ranker types.\"\"\"\n\n    COHERE = \"cohere\"  # Cohere Rerank API\n    OPENAI = \"openai\"  # OpenAI embeddings similarity\n    CROSS_ENCODER = \"cross_encoder\"  # Local cross-encoder model\n    LLM_BASED = \"llm_based\"  # LLM-based re-ranking\n    NONE = \"none\"  # No re-ranking\n\n\n@dataclass\nclass RerankedResult:\n    \"\"\"Result with re-ranking score.\"\"\"\n\n    chunk_id: str\n    document_id: str\n    content: str\n    original_score: float\n    rerank_score: float\n    final_score: float\n    metadata: Dict[str, Any]\n\n\n@dataclass\nclass RerankerConfig:\n    \"\"\"Configuration for re-ranker.\"\"\"\n\n    reranker_type: RerankerType = RerankerType.COHERE\n    top_n: int = 10  # Number of results to return after re-ranking\n    model: str = \"rerank-english-v3.0\"  # Cohere model\n    score_weight: float = 0.7  # Weight for rerank score vs original\n    min_relevance_score: float = 0.0\n    cache_ttl: int = 3600  # Cache TTL in seconds\n\n\nclass CohereReranker:\n    \"\"\"\n    Re-ranker using Cohere Rerank API.\n\n    Cohere Rerank provides high-quality cross-encoder re-ranking\n    with support for multiple languages and domains.\n\n    Models:\n    - rerank-english-v3.0: English documents\n    - rerank-multilingual-v3.0: Multilingual support\n    - rerank-english-v2.0: Previous generation\n    \"\"\"\n\n    def __init__(\n        self,\n        api_key: Optional[str] = None,\n        model: str = \"rerank-english-v3.0\",\n    ):\n        self.api_key = api_key or getattr(settings, \"COHERE_API_KEY\", None)\n        self.model = model\n        self._client = None\n\n    async def _get_client(self):\n        \"\"\"Get or create Cohere client.\"\"\"\n        if self._client is None:\n            try:\n                import cohere\n\n                self._client = cohere.AsyncClient(self.api_key)\n            except ImportError:\n                logger.warning(\"Cohere package not installed. Install with: pip install cohere\")\n                raise\n        return self._client\n\n    async def rerank(\n        self,\n        query: str,\n        documents: List[str],\n        top_n: int = 10,\n    ) -> List[Tuple[int, float]]:\n        \"\"\"\n        Re-rank documents using Cohere Rerank.\n\n        Args:\n            query: Search query\n            documents: List of document texts\n            top_n: Number of top results to return\n\n        Returns:\n            List of (original_index, score) tuples sorted by relevance\n        \"\"\"\n        if not self.api_key:\n            logger.warning(\"Cohere API key not configured, skipping rerank\")\n            return [(i, 1.0) for i in range(min(top_n, len(documents)))]\n\n        try:\n            client = await self._get_client()\n            response = await client.rerank(\n                model=self.model,\n                query=query,\n                documents=documents,\n                top_n=top_n,\n                return_documents=False,\n            )\n\n            return [(r.index, r.relevance_score) for r in response.results]\n\n        except Exception as e:\n            logger.error(f\"Cohere rerank error: {e}\")\n            # Fallback to original order\n            return [(i, 1.0) for i in range(min(top_n, len(documents)))]\n\n\nclass OpenAIReranker:\n    \"\"\"\n    Re-ranker using OpenAI embeddings for similarity scoring.\n\n    This is a lightweight alternative that computes cosine similarity\n    between query and document embeddings. Less accurate than\n    cross-encoders but doesn't require additional API calls if\n    embeddings are cached.\n    \"\"\"\n\n    def __init__(\n        self,\n        model: str = \"text-embedding-3-small\",\n    ):\n        self.model = model\n\n    async def _get_embedding(self, text: str) -> List[float]:\n        \"\"\"Get embedding for text.\"\"\"\n        import openai\n\n        response = await openai.embeddings.create(\n            model=self.model,\n            input=text,\n        )\n        return response.data[0].embedding\n\n    def _cosine_similarity(\n        self,\n        vec1: List[float],\n        vec2: List[float],\n    ) -> float:\n        \"\"\"Compute cosine similarity between two vectors.\"\"\"\n        import math\n\n        dot_product = sum(a * b for a, b in zip(vec1, vec2))\n        norm1 = math.sqrt(sum(a * a for a in vec1))\n        norm2 = math.sqrt(sum(b * b for b in vec2))\n\n        if norm1 == 0 or norm2 == 0:\n            return 0.0\n\n        return dot_product / (norm1 * norm2)\n\n    async def rerank(\n        self,\n        query: str,\n        documents: List[str],\n        top_n: int = 10,\n    ) -> List[Tuple[int, float]]:\n        \"\"\"\n        Re-rank using embedding similarity.\n\n        Args:\n            query: Search query\n            documents: List of document texts\n            top_n: Number of top results to return\n\n        Returns:\n            List of (original_index, score) tuples\n        \"\"\"\n        try:\n            # Get query embedding\n            query_embedding = await self._get_embedding(query)\n\n            # Get document embeddings (could batch this)\n            scores = []\n            for i, doc in enumerate(documents):\n                doc_embedding = await self._get_embedding(doc[:8000])  # Truncate long docs\n                similarity = self._cosine_similarity(query_embedding, doc_embedding)\n                scores.append((i, similarity))\n\n            # Sort by score\n            scores.sort(key=lambda x: x[1], reverse=True)\n            return scores[:top_n]\n\n        except Exception as e:\n            logger.error(f\"OpenAI rerank error: {e}\")\n            return [(i, 1.0) for i in range(min(top_n, len(documents)))]\n\n\nclass LLMReranker:\n    \"\"\"\n    Re-ranker using LLM to score document relevance.\n\n    Uses GPT-4 or similar to assess relevance on a scale.\n    More expensive but can incorporate complex reasoning.\n    \"\"\"\n\n    def __init__(\n        self,\n        model: str = \"gpt-4o-mini\",\n    ):\n        self.model = model\n\n    async def rerank(\n        self,\n        query: str,\n        documents: List[str],\n        top_n: int = 10,\n    ) -> List[Tuple[int, float]]:\n        \"\"\"\n        Re-rank using LLM scoring.\n\n        Args:\n            query: Search query\n            documents: List of document texts\n            top_n: Number of top results to return\n\n        Returns:\n            List of (original_index, score) tuples\n        \"\"\"\n        client = get_async_openai_client()\n        scores = []\n\n        for i, doc in enumerate(documents[:20]):  # Limit to first 20 for cost\n            try:\n                prompt = f\"\"\"Rate the relevance of this document to the query on a scale of 0-10.\nOnly respond with a number.\n\nQuery: {query}\n\nDocument: {doc[:2000]}\n\nRelevance score (0-10):\"\"\"\n\n                response = await client.chat.completions.create(\n                    model=self.model,\n                    messages=[{\"role\": \"user\", \"content\": prompt}],\n                    max_tokens=5,\n                    temperature=0,\n                )\n\n                score_text = response.choices[0].message.content.strip()\n                score = float(score_text) / 10.0  # Normalize to 0-1\n                scores.append((i, min(1.0, max(0.0, score))))\n\n            except Exception as e:\n                logger.error(f\"LLM scoring error for doc {i}: {e}\")\n                scores.append((i, 0.5))  # Default score\n\n        # Sort by score\n        scores.sort(key=lambda x: x[1], reverse=True)\n        return scores[:top_n]\n\n\nclass CrossEncoderReranker:\n    \"\"\"\n    Re-ranker using local cross-encoder model.\n\n    Uses sentence-transformers cross-encoder models for scoring.\n    Requires GPU for efficient inference.\n\n    Models:\n    - cross-encoder/ms-marco-MiniLM-L-6-v2: Fast, general purpose\n    - cross-encoder/ms-marco-MiniLM-L-12-v2: More accurate\n    - cross-encoder/ms-marco-TinyBERT-L-2-v2: Fastest, less accurate\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name: str = \"cross-encoder/ms-marco-MiniLM-L-6-v2\",\n        device: str = \"cpu\",\n    ):\n        self.model_name = model_name\n        self.device = device\n        self._model = None\n\n    async def _get_model(self):\n        \"\"\"Load cross-encoder model lazily.\"\"\"\n        if self._model is None:\n            try:\n                from sentence_transformers import CrossEncoder\n\n                self._model = CrossEncoder(self.model_name, device=self.device)\n            except ImportError:\n                logger.warning(\n                    \"sentence-transformers not installed. \" \"Install with: pip install sentence-transformers\"\n                )\n                raise\n        return self._model\n\n    async def rerank(\n        self,\n        query: str,\n        documents: List[str],\n        top_n: int = 10,\n    ) -> List[Tuple[int, float]]:\n        \"\"\"\n        Re-rank using cross-encoder.\n\n        Args:\n            query: Search query\n            documents: List of document texts\n            top_n: Number of top results to return\n\n        Returns:\n            List of (original_index, score) tuples\n        \"\"\"\n        try:\n            model = await self._get_model()\n\n            # Create query-document pairs\n            pairs = [[query, doc] for doc in documents]\n\n            # Score all pairs (run in thread to avoid blocking)\n            scores = await asyncio.to_thread(model.predict, pairs)\n\n            # Normalize scores to 0-1 using sigmoid\n            import math\n\n            normalized_scores = [1 / (1 + math.exp(-s)) for s in scores]\n\n            # Create (index, score) tuples and sort\n            indexed_scores = list(enumerate(normalized_scores))\n            indexed_scores.sort(key=lambda x: x[1], reverse=True)\n\n            return indexed_scores[:top_n]\n\n        except Exception as e:\n            logger.error(f\"Cross-encoder rerank error: {e}\")\n            return [(i, 1.0) for i in range(min(top_n, len(documents)))]\n\n\nclass RerankingService:\n    \"\"\"\n    Main re-ranking service that orchestrates different re-rankers.\n\n    Provides a unified interface for re-ranking search results\n    with caching and fallback handling.\n    \"\"\"\n\n    def __init__(\n        self,\n        config: Optional[RerankerConfig] = None,\n    ):\n        self.config = config or RerankerConfig()\n\n        # Initialize re-rankers based on config\n        self._rerankers: Dict[RerankerType, Any] = {}\n\n        if self.config.reranker_type == RerankerType.COHERE:\n            self._rerankers[RerankerType.COHERE] = CohereReranker(model=self.config.model)\n        elif self.config.reranker_type == RerankerType.OPENAI:\n            self._rerankers[RerankerType.OPENAI] = OpenAIReranker()\n        elif self.config.reranker_type == RerankerType.CROSS_ENCODER:\n            self._rerankers[RerankerType.CROSS_ENCODER] = CrossEncoderReranker()\n        elif self.config.reranker_type == RerankerType.LLM_BASED:\n            self._rerankers[RerankerType.LLM_BASED] = LLMReranker()\n\n    async def rerank(\n        self,\n        query: str,\n        results: List[Dict[str, Any]],\n        content_key: str = \"content\",\n    ) -> List[RerankedResult]:\n        \"\"\"\n        Re-rank search results.\n\n        Args:\n            query: Original search query\n            results: List of search results with content\n            content_key: Key to extract content from results\n\n        Returns:\n            Re-ranked results with scores\n        \"\"\"\n        if not results:\n            return []\n\n        if self.config.reranker_type == RerankerType.NONE:\n            # No re-ranking - return original results\n            return [\n                RerankedResult(\n                    chunk_id=r.get(\"chunk_id\", str(i)),\n                    document_id=r.get(\"document_id\", \"unknown\"),\n                    content=r.get(content_key, \"\"),\n                    original_score=r.get(\"score\", 0.0),\n                    rerank_score=r.get(\"score\", 0.0),\n                    final_score=r.get(\"score\", 0.0),\n                    metadata=r.get(\"metadata\", {}),\n                )\n                for i, r in enumerate(results)\n            ]\n\n        # Check cache\n        cache_key = generate_cache_key(\n            \"rerank\",\n            query,\n            len(results),\n            self.config.reranker_type.value,\n        )\n        cached = await cache_service.get(cache_key)\n        if cached is not None:\n            logger.debug(\"Using cached rerank results\")\n            return [RerankedResult(**r) for r in cached]\n\n        # Extract documents for re-ranking\n        documents = [r.get(content_key, \"\") for r in results]\n\n        # Get reranker\n        reranker = self._rerankers.get(self.config.reranker_type)\n        if not reranker:\n            logger.warning(f\"Reranker {self.config.reranker_type} not initialized\")\n            return self._create_results_without_reranking(results, content_key)\n\n        try:\n            # Perform re-ranking\n            rerank_scores = await reranker.rerank(\n                query=query,\n                documents=documents,\n                top_n=self.config.top_n,\n            )\n\n            # Build reranked results\n            reranked = []\n            for original_idx, rerank_score in rerank_scores:\n                if original_idx >= len(results):\n                    continue\n\n                r = results[original_idx]\n                original_score = r.get(\"score\", 0.0)\n\n                # Combine scores\n                final_score = self.config.score_weight * rerank_score + (1 - self.config.score_weight) * original_score\n\n                if final_score < self.config.min_relevance_score:\n                    continue\n\n                reranked.append(\n                    RerankedResult(\n                        chunk_id=r.get(\"chunk_id\", str(original_idx)),\n                        document_id=r.get(\"document_id\", \"unknown\"),\n                        content=r.get(content_key, \"\"),\n                        original_score=original_score,\n                        rerank_score=rerank_score,\n                        final_score=final_score,\n                        metadata=r.get(\"metadata\", {}),\n                    )\n                )\n\n            # Sort by final score\n            reranked.sort(key=lambda x: x.final_score, reverse=True)\n\n            # Cache results\n            await cache_service.set(\n                cache_key,\n                [\n                    {\n                        \"chunk_id\": r.chunk_id,\n                        \"document_id\": r.document_id,\n                        \"content\": r.content,\n                        \"original_score\": r.original_score,\n                        \"rerank_score\": r.rerank_score,\n                        \"final_score\": r.final_score,\n                        \"metadata\": r.metadata,\n                    }\n                    for r in reranked\n                ],\n                ttl=self.config.cache_ttl,\n            )\n\n            return reranked\n\n        except Exception as e:\n            logger.error(f\"Re-ranking failed: {e}\", exc_info=True)\n            return self._create_results_without_reranking(results, content_key)\n\n    def _create_results_without_reranking(\n        self,\n        results: List[Dict[str, Any]],\n        content_key: str,\n    ) -> List[RerankedResult]:\n        \"\"\"Create results without re-ranking (fallback).\"\"\"\n        return [\n            RerankedResult(\n                chunk_id=r.get(\"chunk_id\", str(i)),\n                document_id=r.get(\"document_id\", \"unknown\"),\n                content=r.get(content_key, \"\"),\n                original_score=r.get(\"score\", 0.0),\n                rerank_score=r.get(\"score\", 0.0),\n                final_score=r.get(\"score\", 0.0),\n                metadata=r.get(\"metadata\", {}),\n            )\n            for i, r in enumerate(results[: self.config.top_n])\n        ]\n\n    async def rerank_with_diversity(\n        self,\n        query: str,\n        results: List[Dict[str, Any]],\n        content_key: str = \"content\",\n        diversity_threshold: float = 0.8,\n    ) -> List[RerankedResult]:\n        \"\"\"\n        Re-rank with diversity - avoid returning too similar documents.\n\n        Uses Maximal Marginal Relevance (MMR) style diversity.\n        \"\"\"\n        # First, get standard reranked results\n        reranked = await self.rerank(query, results, content_key)\n\n        if len(reranked) <= 1:\n            return reranked\n\n        # Apply diversity filter\n        selected = [reranked[0]]\n        candidates = reranked[1:]\n\n        while candidates and len(selected) < self.config.top_n:\n            best_candidate = None\n            best_score = -float(\"inf\")\n\n            for candidate in candidates:\n                # Check similarity to already selected\n                max_similarity = 0.0\n                for selected_doc in selected:\n                    similarity = self._text_similarity(candidate.content, selected_doc.content)\n                    max_similarity = max(max_similarity, similarity)\n\n                # MMR-style score: relevance - lambda * max_similarity\n                mmr_score = candidate.final_score - diversity_threshold * max_similarity\n\n                if mmr_score > best_score:\n                    best_score = mmr_score\n                    best_candidate = candidate\n\n            if best_candidate:\n                selected.append(best_candidate)\n                candidates.remove(best_candidate)\n            else:\n                break\n\n        return selected\n\n    def _text_similarity(self, text1: str, text2: str) -> float:\n        \"\"\"Simple text similarity using Jaccard index.\"\"\"\n        words1 = set(text1.lower().split())\n        words2 = set(text2.lower().split())\n\n        if not words1 or not words2:\n            return 0.0\n\n        intersection = len(words1 & words2)\n        union = len(words1 | words2)\n\n        return intersection / union if union > 0 else 0.0\n"
}
