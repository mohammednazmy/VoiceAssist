{
  "path": "services/api-gateway/app/api/health.py",
  "language": "python",
  "size": 17198,
  "last_modified": "2025-12-04T11:26:48.000Z",
  "lines": 537,
  "content": "\"\"\"\nHealth check endpoints\n\"\"\"\n\nimport time\nfrom typing import Dict\n\nfrom app.core.config import settings\nfrom app.core.database import (\n    check_postgres_connection,\n    check_qdrant_connection,\n    check_redis_connection,\n    engine,\n    redis_client,\n)\nfrom app.core.logging import get_logger\nfrom app.core.resilience import get_circuit_breaker_status\nfrom app.services.nextcloud import check_nextcloud_connection\nfrom fastapi import APIRouter, Request, status\nfrom fastapi.responses import JSONResponse\nfrom pydantic import BaseModel\nfrom slowapi import Limiter\nfrom slowapi.util import get_remote_address\n\nrouter = APIRouter()\nlogger = get_logger(__name__)\nlimiter = Limiter(key_func=get_remote_address)\n\n\nclass HealthResponse(BaseModel):\n    \"\"\"Health check response model\"\"\"\n\n    status: str\n    version: str\n    timestamp: float\n\n\nclass ReadinessResponse(BaseModel):\n    \"\"\"Readiness check response model\"\"\"\n\n    status: str\n    checks: Dict[str, bool]\n    timestamp: float\n\n\n@router.get(\"/health\", response_model=HealthResponse)\n@limiter.limit(\"100/minute\")\nasync def health_check(request: Request):\n    \"\"\"\n    Basic health check endpoint\n    Returns 200 if the service is running\n\n    Rate limit: 100 requests per minute\n    \"\"\"\n    logger.debug(\"health_check_requested\")\n    return HealthResponse(\n        status=\"healthy\",\n        version=settings.APP_VERSION,\n        timestamp=time.time(),\n    )\n\n\n@router.get(\"/ready\", response_class=JSONResponse)\n@limiter.limit(\"100/minute\")\nasync def readiness_check(request: Request):\n    \"\"\"\n    Readiness check endpoint\n    Verifies connectivity to all dependencies (Postgres, Redis, Qdrant, Nextcloud)\n    Returns 200 if all dependencies are accessible, 503 otherwise\n\n    Rate limit: 100 requests per minute\n    \"\"\"\n    logger.debug(\"readiness_check_requested\")\n\n    checks = {\n        \"postgres\": check_postgres_connection(),\n        \"redis\": check_redis_connection(),\n        \"qdrant\": await check_qdrant_connection(),\n        \"nextcloud\": await check_nextcloud_connection(),\n    }\n\n    all_ready = all(checks.values())\n    response_status = status.HTTP_200_OK if all_ready else status.HTTP_503_SERVICE_UNAVAILABLE\n\n    if not all_ready:\n        logger.warning(\"readiness_check_failed\", checks=checks)\n    else:\n        logger.debug(\"readiness_check_passed\")\n\n    return JSONResponse(\n        status_code=response_status,\n        content={\n            \"status\": \"ready\" if all_ready else \"not_ready\",\n            \"checks\": checks,\n            \"timestamp\": time.time(),\n        },\n    )\n\n\n# Metrics endpoint removed - now handled by app/api/metrics.py (Phase 7 - P2.1, P2.5, P3.3)\n# Proper Prometheus metrics with business KPIs are available at /metrics\n\n\n@router.get(\"/health/openai\", response_class=JSONResponse)\n@limiter.limit(\"10/minute\")\nasync def openai_health_check(request: Request):\n    \"\"\"\n    OpenAI API connectivity health check.\n\n    Tests that the configured OPENAI_API_KEY is valid and can connect\n    to OpenAI's API by making a cheap models.list() call.\n\n    Rate limit: 10 requests per minute (to avoid wasting API quota)\n\n    Returns:\n        200: OpenAI API is accessible and key is valid\n        503: OpenAI API is not accessible or key is invalid/missing\n\n    Response body:\n        - status: \"ok\" or \"error\"\n        - configured: whether OPENAI_API_KEY is set\n        - accessible: whether API call succeeded (only if configured)\n        - latency_ms: API call latency in milliseconds (only if accessible)\n        - error: error message (only if failed)\n        - timestamp: Unix timestamp\n\n    Note: This endpoint does NOT reveal the API key value.\n    \"\"\"\n    logger.debug(\"openai_health_check_requested\")\n\n    result = {\n        \"status\": \"error\",\n        \"configured\": False,\n        \"accessible\": False,\n        \"timestamp\": time.time(),\n    }\n\n    # Check if key is configured\n    api_key = settings.OPENAI_API_KEY\n    if not api_key:\n        result[\"error\"] = \"OPENAI_API_KEY not configured\"\n        logger.warning(\"openai_health_check_failed: key not configured\")\n        return JSONResponse(\n            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,\n            content=result,\n        )\n\n    result[\"configured\"] = True\n\n    # Test API connectivity\n    try:\n        from openai import AsyncOpenAI\n\n        client = AsyncOpenAI(api_key=api_key, timeout=10.0)\n\n        start = time.time()\n        models = await client.models.list()\n        latency_ms = (time.time() - start) * 1000\n\n        result[\"status\"] = \"ok\"\n        result[\"accessible\"] = True\n        result[\"latency_ms\"] = round(latency_ms, 2)\n        result[\"models_accessible\"] = len(models.data) if models.data else 0\n\n        logger.info(\n            \"openai_health_check_passed\",\n            latency_ms=round(latency_ms, 2),\n            models_count=result[\"models_accessible\"],\n        )\n\n        return JSONResponse(\n            status_code=status.HTTP_200_OK,\n            content=result,\n        )\n\n    except Exception as e:\n        error_type = type(e).__name__\n        error_msg = str(e)\n\n        # Sanitize error message to avoid leaking sensitive info\n        if \"invalid_api_key\" in error_msg.lower() or \"incorrect api key\" in error_msg.lower():\n            result[\"error\"] = \"API key invalid or expired\"\n        elif \"rate_limit\" in error_msg.lower():\n            result[\"error\"] = \"Rate limited by OpenAI\"\n        elif \"timeout\" in error_msg.lower():\n            result[\"error\"] = \"OpenAI API timeout\"\n        elif \"connection\" in error_msg.lower():\n            result[\"error\"] = f\"Connection error: {error_type}\"\n        else:\n            result[\"error\"] = f\"{error_type}: {error_msg[:100]}\"\n\n        logger.error(\n            \"openai_health_check_failed\",\n            error=result[\"error\"],\n            exc_info=True,\n        )\n\n        return JSONResponse(\n            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,\n            content=result,\n        )\n\n\n@router.get(\"/health/detailed\", response_class=JSONResponse)\n@limiter.limit(\"50/minute\")\nasync def detailed_health_check(request: Request):\n    \"\"\"\n    Detailed health check endpoint\n    Returns comprehensive health information about all components\n\n    Rate limit: 50 requests per minute\n    \"\"\"\n    logger.debug(\"detailed_health_check_requested\")\n\n    # Measure database latency\n    start = time.time()\n    postgres_healthy = check_postgres_connection()\n    postgres_latency = (time.time() - start) * 1000\n\n    # Measure Redis latency\n    start = time.time()\n    redis_healthy = check_redis_connection()\n    redis_latency = (time.time() - start) * 1000\n\n    # Get Redis memory usage\n    try:\n        redis_info = redis_client.info(\"memory\")\n        redis_memory_mb = int(redis_info.get(\"used_memory\", 0)) / 1024 / 1024\n    except Exception:\n        redis_memory_mb = 0\n\n    # Measure Qdrant latency (skip if disabled)\n    qdrant_enabled = settings.QDRANT_ENABLED if hasattr(settings, \"QDRANT_ENABLED\") else True\n    if qdrant_enabled:\n        start = time.time()\n        qdrant_healthy = await check_qdrant_connection()\n        qdrant_latency = (time.time() - start) * 1000\n    else:\n        qdrant_healthy = True  # Report as healthy when disabled\n        qdrant_latency = 0\n\n    # Get database pool stats\n    pool_size = engine.pool.size()\n    pool_checked_out = engine.pool.checkedout()\n\n    # Calculate overall status\n    all_healthy = postgres_healthy and redis_healthy and qdrant_healthy\n    overall_status = \"healthy\" if all_healthy else \"degraded\"\n\n    response = {\n        \"status\": overall_status,\n        \"components\": {\n            \"postgres\": {\n                \"status\": \"up\" if postgres_healthy else \"down\",\n                \"latency_ms\": round(postgres_latency, 2),\n                \"connections\": {\n                    \"active\": pool_checked_out,\n                    \"pool_size\": pool_size,\n                    \"pool_max\": 20,\n                    \"max_overflow\": 40,\n                },\n            },\n            \"redis\": {\n                \"status\": \"up\" if redis_healthy else \"down\",\n                \"latency_ms\": round(redis_latency, 2),\n                \"memory_used_mb\": round(redis_memory_mb, 2),\n            },\n            \"qdrant\": {\n                \"status\": (\"disabled\" if not qdrant_enabled else (\"up\" if qdrant_healthy else \"down\")),\n                \"latency_ms\": round(qdrant_latency, 2),\n                \"enabled\": qdrant_enabled,\n            },\n        },\n        \"version\": settings.APP_VERSION,\n        \"environment\": settings.ENVIRONMENT,\n        \"timestamp\": time.time(),\n    }\n\n    logger.info(\n        \"detailed_health_check_completed\",\n        status=overall_status,\n        latencies={\n            \"postgres_ms\": round(postgres_latency, 2),\n            \"redis_ms\": round(redis_latency, 2),\n            \"qdrant_ms\": round(qdrant_latency, 2),\n        },\n    )\n\n    response_status = status.HTTP_200_OK if all_healthy else status.HTTP_503_SERVICE_UNAVAILABLE\n    return JSONResponse(status_code=response_status, content=response)\n\n\n@router.get(\"/health/circuit-breakers\", response_class=JSONResponse)\n@limiter.limit(\"100/minute\")\nasync def circuit_breaker_status(request: Request):\n    \"\"\"\n    Circuit breaker status endpoint.\n\n    Returns the current state of all circuit breakers for monitoring\n    and alerting purposes.\n\n    Rate limit: 100 requests per minute\n\n    Response body:\n        - status: \"ok\" if all breakers are closed, \"degraded\" if any are open\n        - breakers: Dict of breaker name -> status info\n        - timestamp: Unix timestamp\n\n    Circuit Breaker States:\n        - closed: Normal operation, requests pass through\n        - open: Service is failing, requests fail immediately\n        - half-open: Testing if service recovered\n    \"\"\"\n    logger.debug(\"circuit_breaker_status_requested\")\n\n    breakers = get_circuit_breaker_status()\n\n    # Check if any breakers are open\n    any_open = any(b[\"state\"] == \"open\" for b in breakers.values())\n    overall_status = \"degraded\" if any_open else \"ok\"\n\n    response = {\n        \"status\": overall_status,\n        \"breakers\": breakers,\n        \"timestamp\": time.time(),\n    }\n\n    if any_open:\n        logger.warning(\n            \"circuit_breakers_degraded\",\n            open_breakers=[name for name, b in breakers.items() if b[\"state\"] == \"open\"],\n        )\n\n    return JSONResponse(\n        status_code=status.HTTP_200_OK,\n        content=response,\n    )\n\n\n@router.get(\"/health/voice\", response_class=JSONResponse)\n@limiter.limit(\"50/minute\")\nasync def voice_health_check(request: Request):\n    \"\"\"\n    Voice subsystem health check endpoint.\n\n    Returns comprehensive health status of the voice pipeline including:\n    - Overall status (healthy/degraded/unhealthy)\n    - Provider connectivity (OpenAI, ElevenLabs, Deepgram)\n    - Redis session store health\n    - Recent error rate\n    - TTFA SLO compliance\n    - Active session count\n\n    Rate limit: 50 requests per minute\n    \"\"\"\n    logger.debug(\"voice_health_check_requested\")\n\n    result = {\n        \"status\": \"healthy\",\n        \"timestamp\": time.time(),\n        \"providers\": {},\n        \"session_store\": {\"status\": \"unknown\"},\n        \"metrics\": {\n            \"active_sessions\": 0,\n            \"error_rate_5m\": None,\n            \"ttfa_p95_ms\": None,\n            \"ttfa_slo_compliant\": None,\n        },\n        \"slo\": {\n            \"ttfa_target_ms\": 200,\n            \"error_rate_target\": 0.01,  # 1%\n        },\n    }\n\n    issues = []\n\n    # Check Redis session store health\n    try:\n        redis_start = time.time()\n        redis_healthy = check_redis_connection()\n        redis_latency = (time.time() - redis_start) * 1000\n\n        # Check active voice sessions in Redis\n        active_sessions = 0\n        try:\n            keys = redis_client.keys(\"voice:session:*\")\n            active_sessions = len(keys) if keys else 0\n        except Exception:\n            pass\n\n        result[\"session_store\"] = {\n            \"status\": \"up\" if redis_healthy else \"down\",\n            \"latency_ms\": round(redis_latency, 2),\n            \"active_sessions\": active_sessions,\n        }\n        result[\"metrics\"][\"active_sessions\"] = active_sessions\n\n        if not redis_healthy:\n            issues.append(\"Redis session store unavailable\")\n    except Exception as e:\n        result[\"session_store\"] = {\"status\": \"error\", \"error\": str(e)}\n        issues.append(f\"Redis error: {str(e)[:50]}\")\n\n    # Check OpenAI provider\n    try:\n        from openai import AsyncOpenAI\n\n        api_key = settings.OPENAI_API_KEY\n        if api_key:\n            client = AsyncOpenAI(api_key=api_key, timeout=5.0)\n            start = time.time()\n            await client.models.list()\n            latency = (time.time() - start) * 1000\n\n            result[\"providers\"][\"openai\"] = {\n                \"status\": \"up\",\n                \"latency_ms\": round(latency, 2),\n            }\n        else:\n            result[\"providers\"][\"openai\"] = {\"status\": \"not_configured\"}\n            issues.append(\"OpenAI not configured\")\n    except Exception as e:\n        result[\"providers\"][\"openai\"] = {\n            \"status\": \"down\",\n            \"error\": str(e)[:100],\n        }\n        issues.append(\"OpenAI unavailable\")\n\n    # Check ElevenLabs provider\n    try:\n        elevenlabs_key = getattr(settings, \"ELEVENLABS_API_KEY\", None)\n        if elevenlabs_key:\n            import httpx\n\n            async with httpx.AsyncClient(timeout=5.0) as client:\n                start = time.time()\n                resp = await client.get(\n                    \"https://api.elevenlabs.io/v1/voices\",\n                    headers={\"xi-api-key\": elevenlabs_key},\n                )\n                latency = (time.time() - start) * 1000\n\n                if resp.status_code == 200:\n                    result[\"providers\"][\"elevenlabs\"] = {\n                        \"status\": \"up\",\n                        \"latency_ms\": round(latency, 2),\n                    }\n                else:\n                    result[\"providers\"][\"elevenlabs\"] = {\n                        \"status\": \"degraded\",\n                        \"status_code\": resp.status_code,\n                    }\n                    issues.append(f\"ElevenLabs returned {resp.status_code}\")\n        else:\n            result[\"providers\"][\"elevenlabs\"] = {\"status\": \"not_configured\"}\n    except Exception as e:\n        result[\"providers\"][\"elevenlabs\"] = {\n            \"status\": \"down\",\n            \"error\": str(e)[:100],\n        }\n        issues.append(\"ElevenLabs unavailable\")\n\n    # Check Deepgram STT provider\n    try:\n        deepgram_key = getattr(settings, \"DEEPGRAM_API_KEY\", None)\n        if deepgram_key:\n            import httpx\n\n            async with httpx.AsyncClient(timeout=5.0) as client:\n                start = time.time()\n                resp = await client.get(\n                    \"https://api.deepgram.com/v1/projects\",\n                    headers={\"Authorization\": f\"Token {deepgram_key}\"},\n                )\n                latency = (time.time() - start) * 1000\n\n                if resp.status_code == 200:\n                    result[\"providers\"][\"deepgram\"] = {\n                        \"status\": \"up\",\n                        \"latency_ms\": round(latency, 2),\n                    }\n                else:\n                    result[\"providers\"][\"deepgram\"] = {\n                        \"status\": \"degraded\",\n                        \"status_code\": resp.status_code,\n                    }\n                    issues.append(f\"Deepgram returned {resp.status_code}\")\n        else:\n            result[\"providers\"][\"deepgram\"] = {\"status\": \"not_configured\"}\n    except Exception as e:\n        result[\"providers\"][\"deepgram\"] = {\n            \"status\": \"down\",\n            \"error\": str(e)[:100],\n        }\n        issues.append(\"Deepgram unavailable\")\n\n    # Get circuit breaker status for voice-related breakers\n    breakers = get_circuit_breaker_status()\n    voice_breakers = {\n        k: v for k, v in breakers.items() if \"voice\" in k.lower() or \"tts\" in k.lower() or \"stt\" in k.lower()\n    }\n    if voice_breakers:\n        result[\"circuit_breakers\"] = voice_breakers\n        for name, breaker in voice_breakers.items():\n            if breaker.get(\"state\") == \"open\":\n                issues.append(f\"Circuit breaker open: {name}\")\n\n    # Determine overall status\n    critical_issues = [i for i in issues if \"unavailable\" in i.lower() or \"down\" in i.lower()]\n    if critical_issues:\n        result[\"status\"] = \"unhealthy\"\n    elif issues:\n        result[\"status\"] = \"degraded\"\n    else:\n        result[\"status\"] = \"healthy\"\n\n    result[\"issues\"] = issues if issues else None\n\n    # Log result\n    logger.info(\n        \"voice_health_check_completed\",\n        status=result[\"status\"],\n        providers={k: v.get(\"status\") for k, v in result[\"providers\"].items()},\n        active_sessions=result[\"metrics\"][\"active_sessions\"],\n        issue_count=len(issues),\n    )\n\n    response_status = (\n        status.HTTP_200_OK\n        if result[\"status\"] == \"healthy\"\n        else (\n            status.HTTP_503_SERVICE_UNAVAILABLE if result[\"status\"] == \"unhealthy\" else status.HTTP_200_OK\n        )  # Return 200 for degraded (still functional)\n    )\n\n    return JSONResponse(status_code=response_status, content=result)\n"
}
