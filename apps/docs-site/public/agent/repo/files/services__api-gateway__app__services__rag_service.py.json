{
  "path": "services/api-gateway/app/services/rag_service.py",
  "language": "python",
  "size": 27385,
  "last_modified": "2025-12-04T11:27:00.015Z",
  "lines": 694,
  "content": "\"\"\"Query Orchestrator / RAG Service (Phase 5 Enhanced).\n\nThis module implements the QueryOrchestrator described in\ndocs/ORCHESTRATION_DESIGN.md. Phase 5 adds full RAG integration\nwith semantic search and citation tracking.\n\nPhase 5 Enhancements:\n- Integrated SearchAggregator for semantic search\n- RAG-enhanced prompts with retrieved context\n- Citation extraction and formatting\n- Configurable RAG behavior (enable/disable, top-K, score threshold)\n\nFuture enhancements:\n- PHI detection and routing\n- Intent classification\n- Multi-hop reasoning\n- External evidence integration (OpenEvidence, PubMed)\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport json\nimport logging\nfrom datetime import datetime, timezone\nfrom typing import Awaitable, Callable, List, Optional\n\nfrom app.core.config import settings\nfrom app.services.intent_classifier import IntentClassifier\nfrom app.services.llm_client import LLMClient, LLMRequest, LLMResponse, ToolCall\nfrom app.services.model_adapters import ModelAdapter, ModelAdapterRegistry\nfrom app.services.openai_realtime_client import OpenAIRealtimeClient\nfrom app.services.phi_detector import PHIDetector\nfrom app.services.prompt_service import prompt_service\nfrom app.services.query_expansion import QueryExpansionConfig, QueryExpansionService\nfrom app.services.realtime_voice_service import realtime_voice_service\nfrom app.services.search_aggregator import SearchAggregator\nfrom pydantic import BaseModel, Field\n\n# Import tool service for function calling support\ntry:\n    from app.services.tools import tool_service\n    from app.services.tools.tool_service import ToolExecutionContext\n\n    TOOLS_AVAILABLE = True\nexcept ImportError:\n    TOOLS_AVAILABLE = False\n    logging.warning(\"Tool service not available for chat mode\")\n\n# Default values when prompt service is unavailable\nDEFAULT_TEMPERATURE = 0.7\nDEFAULT_MAX_TOKENS = 1024\n\n\nclass Citation(BaseModel):\n    \"\"\"Structured citation used in QueryResponse.\n\n    Enhanced with additional metadata fields for proper citation formatting.\n    The database model (MessageCitation) provides persistent storage.\n    \"\"\"\n\n    id: str\n    source_id: str\n    source_type: str = Field(..., description=\"textbook|journal|guideline|note\")\n    title: str\n    url: Optional[str] = None\n    authors: Optional[List[str]] = None\n    publication_date: Optional[str] = None\n    journal: Optional[str] = None\n    volume: Optional[str] = None\n    issue: Optional[str] = None\n    pages: Optional[str] = None\n    doi: Optional[str] = None\n    pmid: Optional[str] = None\n    relevance_score: Optional[float] = None\n    quoted_text: Optional[str] = None\n    context: Optional[dict] = None\n\n\nclass QueryRequest(BaseModel):\n    \"\"\"Top-level request into the QueryOrchestrator.\"\"\"\n\n    session_id: Optional[str] = None\n    query: str\n    clinical_context_id: Optional[str] = None\n\n\nclass QueryResponse(BaseModel):\n    \"\"\"Response from the QueryOrchestrator.\"\"\"\n\n    session_id: str\n    message_id: str\n    answer: str\n    created_at: datetime\n    citations: List[Citation] = Field(default_factory=list)\n    tokens: Optional[int] = None\n    model: Optional[str] = None\n    model_provider: Optional[str] = None\n    model_confidence: Optional[float] = None\n    retrieval_confidence: Optional[float] = None\n    reasoning_path: List[dict] = Field(default_factory=list)\n    finish_reason: Optional[str] = None\n\n\nclass QueryOrchestrator:\n    \"\"\"High-level orchestrator entrypoint with RAG integration (Phase 5).\n\n    Implements the full RAG pipeline:\n    1. Query analysis\n    2. Semantic search over knowledge base\n    3. Context assembly\n    4. LLM synthesis with retrieved context\n    5. Citation extraction\n    \"\"\"\n\n    def __init__(\n        self,\n        enable_rag: bool = True,\n        rag_top_k: int = 5,\n        rag_score_threshold: float = 0.7,\n        enable_query_decomposition: bool | None = None,\n        enable_multi_hop: bool | None = None,\n        search_aggregator: SearchAggregator | None = None,\n        query_expansion_service: QueryExpansionService | None = None,\n        model_registry: ModelAdapterRegistry | None = None,\n        enable_tools: bool = True,\n        max_tool_iterations: int = 5,\n    ):\n        \"\"\"\n        Initialize QueryOrchestrator with RAG support.\n\n        Args:\n            enable_rag: Whether to use RAG (can be disabled for testing)\n            rag_top_k: Number of top results to retrieve\n            rag_score_threshold: Minimum similarity score for results\n            enable_tools: Whether to enable function calling/tools\n            max_tool_iterations: Maximum number of tool execution loops\n        \"\"\"\n        self.llm_client = LLMClient(\n            cloud_model=\"gpt-4o\",\n            openai_api_key=settings.OPENAI_API_KEY,\n            openai_timeout_sec=settings.OPENAI_TIMEOUT_SEC,\n            local_api_url=settings.LOCAL_LLM_URL,\n            local_api_key=settings.LOCAL_LLM_API_KEY,\n            local_timeout_sec=settings.LOCAL_LLM_TIMEOUT_SEC,\n            local_model=settings.LOCAL_LLM_MODEL or \"local-clinical-llm\",\n        )\n        self.search_aggregator = search_aggregator or (SearchAggregator() if enable_rag else None)\n        self.phi_detector = PHIDetector()\n        self.intent_classifier = IntentClassifier()\n        self.realtime_client = OpenAIRealtimeClient()\n        self.enable_rag = enable_rag\n        self.rag_top_k = rag_top_k\n        self.rag_score_threshold = rag_score_threshold\n        self.enable_query_decomposition = (\n            settings.ENABLE_QUERY_DECOMPOSITION if enable_query_decomposition is None else enable_query_decomposition\n        )\n        self.enable_multi_hop = settings.ENABLE_MULTI_HOP_RETRIEVAL if enable_multi_hop is None else enable_multi_hop\n        self.query_expander = query_expansion_service or QueryExpansionService(QueryExpansionConfig(enable_llm=False))\n        self.model_registry = model_registry or ModelAdapterRegistry()\n        self.enable_tools = enable_tools and TOOLS_AVAILABLE\n        self.max_tool_iterations = max_tool_iterations\n\n    async def _execute_tool_call(\n        self,\n        tool_call: ToolCall,\n        user_id: str,\n        session_id: Optional[str],\n        trace_id: Optional[str],\n    ) -> dict:\n        \"\"\"\n        Execute a single tool call and return the result.\n\n        Args:\n            tool_call: The tool call to execute\n            user_id: User identifier\n            session_id: Session identifier\n            trace_id: Trace ID for logging\n\n        Returns:\n            Dict with tool result to send back to LLM\n        \"\"\"\n        if not TOOLS_AVAILABLE:\n            return {\"error\": \"Tools not available\"}\n\n        try:\n            # Parse arguments\n            arguments = json.loads(tool_call.arguments)\n\n            # Create execution context\n            context = ToolExecutionContext(\n                user_id=user_id,\n                session_id=session_id,\n                mode=\"chat\",\n                trace_id=trace_id,\n            )\n\n            # Execute the tool\n            result = await tool_service.execute(tool_call.name, arguments, context)\n\n            # Format result for LLM\n            if result.success:\n                output = result.data\n                if result.message and isinstance(output, dict):\n                    output[\"_message\"] = result.message\n                return output\n            else:\n                return {\n                    \"error\": result.error,\n                    \"needs_clarification\": result.needs_clarification,\n                    \"needs_connection\": result.needs_connection,\n                    \"_message\": result.message,\n                }\n\n        except json.JSONDecodeError as e:\n            logging.error(f\"Failed to parse tool arguments: {e}\")\n            return {\"error\": f\"Invalid arguments: {e}\"}\n        except Exception as e:\n            logging.exception(f\"Error executing tool {tool_call.name}: {e}\")\n            return {\"error\": str(e)}\n\n    async def _run_retrieval(self, query: str) -> tuple[list, str, List[dict], float]:\n        \"\"\"Run single or multi-hop retrieval with optional synthesis.\"\"\"\n\n        if not self.enable_rag or not self.search_aggregator:\n            return [], \"\", [], 0.0\n\n        sub_queries = [query]\n        if self.enable_multi_hop and self.enable_query_decomposition:\n            try:\n                sub_queries = await self.query_expander.decompose(query)\n            except Exception:\n                sub_queries = [query]\n\n        hop_traces: List[dict] = []\n        aggregated_results = []\n\n        for hop_idx, sub_query in enumerate(sub_queries, start=1):\n            hop_results = await self.search_aggregator.search(\n                query=sub_query,\n                top_k=self.rag_top_k,\n                score_threshold=self.rag_score_threshold,\n            )\n            hop_traces.append({\"hop\": hop_idx, \"query\": sub_query, \"results\": len(hop_results)})\n            aggregated_results.extend(hop_results)\n\n        synthesized_context = \"\"\n        if aggregated_results:\n            synthesis = self.search_aggregator.synthesize_across_documents(aggregated_results)\n            synthesized_context = synthesis.get(\"context\", \"\") or self.search_aggregator.format_context_for_rag(\n                aggregated_results\n            )\n        return (\n            aggregated_results,\n            synthesized_context,\n            hop_traces,\n            self.search_aggregator.confidence_score(aggregated_results),\n        )\n\n    async def _prepare_llm_request(\n        self,\n        request: QueryRequest,\n        clinical_context: Optional[dict],\n        trace_id: Optional[str],\n    ) -> tuple[LLMRequest, list, str, bool, str]:\n        \"\"\"Prepare prompt, RAG context, and LLM request for streaming or non-streaming paths.\"\"\"\n        search_results, context, reasoning_path, retrieval_confidence = await self._run_retrieval(request.query)\n\n        # Get dynamic RAG instructions from prompt service\n        try:\n            rag_instructions = await prompt_service.get_rag_instructions()\n        except Exception as e:\n            logging.warning(f\"Failed to get RAG instructions, using default: {e}\")\n            rag_instructions = prompt_service._get_default_rag_instructions()\n\n        # Step 3: Build prompt with context and clinical context\n        prompt_parts = [rag_instructions]\n\n        # Add clinical context if provided\n        if clinical_context:\n            clinical_info = []\n            if clinical_context.get(\"age\"):\n                clinical_info.append(f\"Age: {clinical_context['age']}\")\n            if clinical_context.get(\"gender\"):\n                clinical_info.append(f\"Gender: {clinical_context['gender']}\")\n            if clinical_context.get(\"chief_complaint\"):\n                clinical_info.append(f\"Chief Complaint: {clinical_context['chief_complaint']}\")\n            if clinical_context.get(\"problems\"):\n                problems = \", \".join(clinical_context[\"problems\"])\n                clinical_info.append(f\"Problems: {problems}\")\n            if clinical_context.get(\"medications\"):\n                meds = \", \".join(clinical_context[\"medications\"])\n                clinical_info.append(f\"Medications: {meds}\")\n            if clinical_context.get(\"allergies\"):\n                allergies = \", \".join(clinical_context[\"allergies\"])\n                clinical_info.append(f\"Allergies: {allergies}\")\n\n            if clinical_info:\n                prompt_parts.append(\"\\nPatient Context:\")\n                prompt_parts.append(\"\\n\".join(f\"- {info}\" for info in clinical_info))\n\n        # Add knowledge base context if available\n        if context:\n            prompt_parts.append(\"\\nUse the following context from medical literature to answer the query:\")\n            prompt_parts.append(f\"\\nContext:\\n{context}\")\n\n        # Add query\n        prompt_parts.append(f\"\\nQuery: {request.query}\")\n\n        prompt = \"\\n\".join(prompt_parts)\n\n        # Step 4: PHI Detection\n        phi_result = self.phi_detector.detect(text=request.query, clinical_context=clinical_context)\n\n        # Decide prompt and routing: if PHI and no local model, sanitize and send to cloud\n        sanitized_prompt = prompt\n        llm_phi_flag = phi_result.contains_phi and self.llm_client.has_local_model\n\n        if phi_result.contains_phi:\n            logging.warning(\n                f\"PHI detected in query: types={phi_result.phi_types}, \"\n                f\"confidence={phi_result.confidence}, trace_id={trace_id}\"\n            )\n            if not self.llm_client.has_local_model:\n                sanitized_prompt = self.phi_detector.sanitize(prompt)\n                logging.warning(\n                    \"No local model configured; PHI redacted before routing to cloud. trace_id=%s\",\n                    trace_id,\n                )\n\n        # Step 5: Intent Classification\n        intent = self.intent_classifier.classify(query=request.query, clinical_context=clinical_context)\n        adapter: ModelAdapter | None = None\n        try:\n            adapter = self.model_registry.select_for_intent(intent)\n        except Exception:\n            adapter = None\n\n        if adapter and adapter.provider != \"openai\" and not self.llm_client.has_local_model:\n            logging.warning(\n                \"Local adapter %s selected but no local LLM configured; \"\n                \"falling back to default cloud model. trace_id=%s\",\n                adapter.key,\n                trace_id,\n            )\n            adapter = self.model_registry.get(\"default\") or None\n\n        # Get per-prompt temperature and max_tokens from prompt service\n        temperature = DEFAULT_TEMPERATURE\n        max_tokens = DEFAULT_MAX_TOKENS\n        model_override = adapter.model_id if adapter else None\n\n        try:\n            prompt_name = f\"intent:{intent}\"\n            prompt_settings = await prompt_service.get_prompt_with_settings(prompt_name)\n            if prompt_settings:\n                if prompt_settings.get(\"temperature\") is not None:\n                    temperature = prompt_settings[\"temperature\"]\n                if prompt_settings.get(\"max_tokens\") is not None:\n                    max_tokens = prompt_settings[\"max_tokens\"]\n                if prompt_settings.get(\"model_name\") and not model_override:\n                    model_override = prompt_settings[\"model_name\"]\n        except Exception as e:\n            logging.warning(f\"Failed to get prompt settings for {intent}, using defaults: {e}\")\n\n        # Get tools if enabled\n        tools = None\n        if self.enable_tools and TOOLS_AVAILABLE:\n            try:\n                tools = tool_service.get_openai_tools()\n            except Exception as e:\n                logging.warning(f\"Failed to get tools: {e}\")\n\n        llm_request = LLMRequest(\n            prompt=sanitized_prompt,\n            intent=intent,\n            temperature=temperature,\n            max_tokens=max_tokens,\n            phi_present=llm_phi_flag,\n            trace_id=trace_id,\n            model_override=model_override,\n            model_provider=adapter.provider if adapter else None,\n            tools=tools,\n            tool_choice=\"auto\" if tools else None,\n        )\n\n        return (\n            llm_request,\n            search_results,\n            prompt,\n            phi_result.contains_phi,\n            intent,\n            adapter,\n            reasoning_path,\n            retrieval_confidence,\n        )\n\n    @staticmethod\n    def _resolve_model_provider(\n        adapter: ModelAdapter | None,\n        llm_request: LLMRequest,\n        llm_response: LLMResponse,\n    ) -> str | None:\n        \"\"\"Return a user-facing provider label for response metadata.\"\"\"\n\n        if adapter and adapter.provider:\n            return adapter.provider\n        if llm_request.model_provider:\n            return llm_request.model_provider\n        return \"openai\" if llm_response.model_family == \"cloud\" else \"local\"\n\n    async def handle_query(\n        self,\n        request: QueryRequest,\n        clinical_context: Optional[dict] = None,\n        trace_id: Optional[str] = None,\n        user_id: Optional[str] = None,\n    ) -> QueryResponse:\n        \"\"\"Handle a clinician query with full RAG pipeline and tool support.\n\n        Pipeline:\n        1. Load clinical context (if provided)\n        2. Semantic search over KB (if RAG enabled)\n        3. Assemble context from search results\n        4. Generate LLM response with context + clinical context\n        5. Execute tool calls if any (loop until done or max iterations)\n        6. Extract citations\n\n        Args:\n            request: Query request with query text and optional context\n            clinical_context: Optional clinical context dict with patient info\n            trace_id: Trace ID for logging\n            user_id: User identifier for tool execution context\n\n        Returns:\n            QueryResponse with answer and citations\n        \"\"\"\n        now = datetime.now(timezone.utc)\n        message_id = f\"msg-{int(now.timestamp())}\"\n\n        (\n            llm_request,\n            search_results,\n            _prompt,\n            _phi_present,\n            intent,\n            adapter,\n            reasoning_path,\n            retrieval_confidence,\n        ) = await self._prepare_llm_request(request=request, clinical_context=clinical_context, trace_id=trace_id)\n\n        logging.info(f\"Query classified as intent='{intent}', trace_id={trace_id}\")\n\n        # Initial LLM call\n        llm_response: LLMResponse = await self.llm_client.generate(llm_request)\n\n        # Tool execution loop - handle tool calls until LLM returns final response\n        iteration = 0\n        total_tokens = llm_response.used_tokens or 0\n        conversation_messages = [\n            {\"role\": \"user\", \"content\": llm_request.prompt},\n            {\n                \"role\": \"assistant\",\n                \"content\": llm_response.text or \"\",\n                \"tool_calls\": None,\n            },\n        ]\n\n        # Update the assistant message with tool_calls if present\n        if llm_response.tool_calls:\n            conversation_messages[-1][\"tool_calls\"] = [\n                {\n                    \"id\": tc.id,\n                    \"type\": \"function\",\n                    \"function\": {\"name\": tc.name, \"arguments\": tc.arguments},\n                }\n                for tc in llm_response.tool_calls\n            ]\n\n        while llm_response.tool_calls and self.enable_tools and iteration < self.max_tool_iterations:\n            iteration += 1\n            logging.info(\n                f\"Tool execution iteration {iteration}/{self.max_tool_iterations}, \"\n                f\"{len(llm_response.tool_calls)} tool calls, trace_id={trace_id}\"\n            )\n\n            # Execute each tool call\n            for tool_call in llm_response.tool_calls:\n                logging.info(f\"Executing tool: {tool_call.name}, trace_id={trace_id}\")\n\n                tool_result = await self._execute_tool_call(\n                    tool_call=tool_call,\n                    user_id=user_id or \"anonymous\",\n                    session_id=request.session_id,\n                    trace_id=trace_id,\n                )\n\n                # Add tool result to conversation\n                conversation_messages.append(\n                    {\n                        \"role\": \"tool\",\n                        \"tool_call_id\": tool_call.id,\n                        \"content\": json.dumps(tool_result),\n                    }\n                )\n\n                # Add to reasoning path for transparency\n                reasoning_path.append(\n                    {\n                        \"type\": \"tool_call\",\n                        \"iteration\": iteration,\n                        \"tool\": tool_call.name,\n                        \"result_keys\": (list(tool_result.keys()) if isinstance(tool_result, dict) else None),\n                    }\n                )\n\n            # Continue conversation with tool results\n            followup_request = LLMRequest(\n                prompt=None,  # Use messages instead\n                messages=conversation_messages,\n                intent=intent,\n                temperature=llm_request.temperature,\n                max_tokens=llm_request.max_tokens,\n                phi_present=llm_request.phi_present,\n                trace_id=trace_id,\n                model_override=llm_request.model_override,\n                model_provider=llm_request.model_provider,\n                tools=llm_request.tools,\n                tool_choice=\"auto\",\n            )\n\n            llm_response = await self.llm_client.generate(followup_request)\n            total_tokens += llm_response.used_tokens or 0\n\n            # Update conversation with new assistant response\n            assistant_msg = {\"role\": \"assistant\", \"content\": llm_response.text or \"\"}\n            if llm_response.tool_calls:\n                assistant_msg[\"tool_calls\"] = [\n                    {\n                        \"id\": tc.id,\n                        \"type\": \"function\",\n                        \"function\": {\"name\": tc.name, \"arguments\": tc.arguments},\n                    }\n                    for tc in llm_response.tool_calls\n                ]\n            conversation_messages.append(assistant_msg)\n\n        if iteration >= self.max_tool_iterations and llm_response.tool_calls:\n            logging.warning(\n                f\"Tool execution reached max iterations ({self.max_tool_iterations}), \" f\"trace_id={trace_id}\"\n            )\n\n        # Extract citations from search results\n        citations = []\n        if search_results:\n            citation_dicts = self.search_aggregator.extract_citations(search_results)\n            for cite_dict in citation_dicts:\n                citations.append(\n                    Citation(\n                        id=cite_dict.get(\"id\", \"\"),\n                        source_id=cite_dict.get(\"source_id\", cite_dict.get(\"id\", \"\")),\n                        source_type=cite_dict.get(\"source_type\", \"textbook\"),\n                        title=cite_dict.get(\"title\", \"Untitled\"),\n                        url=cite_dict.get(\"url\"),\n                        authors=cite_dict.get(\"authors\"),\n                        publication_date=cite_dict.get(\"publication_date\"),\n                        journal=cite_dict.get(\"journal\"),\n                        volume=cite_dict.get(\"volume\"),\n                        issue=cite_dict.get(\"issue\"),\n                        pages=cite_dict.get(\"pages\"),\n                        doi=cite_dict.get(\"doi\"),\n                        pmid=cite_dict.get(\"pmid\"),\n                        relevance_score=cite_dict.get(\"relevance_score\"),\n                        quoted_text=cite_dict.get(\"quoted_text\"),\n                        context=cite_dict.get(\"context\"),\n                    )\n                )\n\n        return QueryResponse(\n            session_id=request.session_id or \"session-stub\",\n            message_id=message_id,\n            answer=llm_response.text,\n            created_at=now,\n            citations=citations,\n            tokens=total_tokens,\n            model=llm_response.model_name,\n            model_provider=self._resolve_model_provider(adapter, llm_request, llm_response),\n            model_confidence=(adapter.confidence if adapter else None),\n            retrieval_confidence=retrieval_confidence if retrieval_confidence else None,\n            reasoning_path=reasoning_path,\n            finish_reason=llm_response.finish_reason,\n        )\n\n    async def prepare_realtime_session(\n        self,\n        user_id: str,\n        conversation_id: str | None = None,\n        voice: str | None = None,\n        language: str | None = None,\n        vad_sensitivity: int | None = None,\n    ) -> dict:\n        \"\"\"Expose realtime session config for voice-first clients.\"\"\"\n\n        if not self.realtime_client.is_enabled():\n            raise ValueError(\"Realtime client is not enabled\")\n\n        return await realtime_voice_service.generate_session_config(\n            user_id=user_id,\n            conversation_id=conversation_id,\n            voice=voice,\n            language=language,\n            vad_sensitivity=vad_sensitivity,\n        )\n\n    async def stream_query(\n        self,\n        request: QueryRequest,\n        clinical_context: Optional[dict] = None,\n        trace_id: Optional[str] = None,\n        on_chunk: Optional[Callable[[str], Awaitable[None]]] = None,\n    ) -> QueryResponse:\n        \"\"\"Stream a clinician query with RAG support.\n\n        Args:\n            request: Query request with query text and optional context\n            clinical_context: Optional clinical context dict with patient info\n            trace_id: Trace ID for logging\n            on_chunk: Callback invoked per text delta\n\n        Returns:\n            QueryResponse with full answer and citations\n        \"\"\"\n        now = datetime.now(timezone.utc)\n        message_id = f\"msg-{int(now.timestamp())}\"\n\n        (\n            llm_request,\n            search_results,\n            _prompt,\n            _phi_present,\n            intent,\n            adapter,\n            reasoning_path,\n            retrieval_confidence,\n        ) = await self._prepare_llm_request(request=request, clinical_context=clinical_context, trace_id=trace_id)\n\n        async def _emit_chunk(text: str):\n            if on_chunk:\n                result = on_chunk(text)\n                if asyncio.iscoroutine(result):\n                    await result\n\n        llm_response: LLMResponse = await self.llm_client.stream_generate(llm_request, on_chunk=_emit_chunk)\n\n        citations: List[Citation] = []\n        if search_results:\n            citation_dicts = self.search_aggregator.extract_citations(search_results)\n            for cite_dict in citation_dicts:\n                citations.append(\n                    Citation(\n                        id=cite_dict.get(\"id\", \"\"),\n                        source_id=cite_dict.get(\"source_id\", cite_dict.get(\"id\", \"\")),\n                        source_type=cite_dict.get(\"source_type\", \"textbook\"),\n                        title=cite_dict.get(\"title\", \"Untitled\"),\n                        url=cite_dict.get(\"url\"),\n                        authors=cite_dict.get(\"authors\"),\n                        publication_date=cite_dict.get(\"publication_date\"),\n                        journal=cite_dict.get(\"journal\"),\n                        volume=cite_dict.get(\"volume\"),\n                        issue=cite_dict.get(\"issue\"),\n                        pages=cite_dict.get(\"pages\"),\n                        doi=cite_dict.get(\"doi\"),\n                        pmid=cite_dict.get(\"pmid\"),\n                        relevance_score=cite_dict.get(\"relevance_score\"),\n                        quoted_text=cite_dict.get(\"quoted_text\"),\n                        context=cite_dict.get(\"context\"),\n                    )\n                )\n\n        return QueryResponse(\n            session_id=request.session_id or \"session-stub\",\n            message_id=message_id,\n            answer=llm_response.text,\n            created_at=now,\n            citations=citations,\n            tokens=llm_response.used_tokens,\n            model=llm_response.model_name,\n            model_provider=self._resolve_model_provider(adapter, llm_request, llm_response),\n            model_confidence=(adapter.confidence if adapter else None),\n            retrieval_confidence=retrieval_confidence if retrieval_confidence else None,\n            reasoning_path=reasoning_path,\n            finish_reason=llm_response.finish_reason,\n        )\n"
}
