{
  "path": "apps/web-app/src/components/voice/VoiceModePanel.tsx",
  "language": "typescript",
  "size": 33228,
  "last_modified": "2025-12-04T00:32:48.619Z",
  "lines": 945,
  "content": "/**\n * LEGACY - OpenAI Realtime API Voice Panel\n *\n * @deprecated This component uses OpenAI Realtime API which has been REPLACED\n * by the Thinker/Talker pipeline (Deepgram STT + GPT-4o + ElevenLabs TTS).\n *\n * USE INSTEAD: ThinkerTalkerVoicePanel\n *\n * This component is maintained for backwards compatibility only.\n *\n * Original description:\n * Voice Mode Panel\n * Integrates OpenAI Realtime API for full-duplex voice conversations\n *\n * Features:\n * - Real-time bidirectional audio streaming\n * - Live transcript display\n * - Waveform visualization\n * - Connection status indicator\n * - Seamless integration with Chat UI\n */\n\nimport { useCallback, useEffect, useRef, useState } from \"react\";\nimport {\n  useRealtimeVoiceSession,\n  type VoiceMetrics,\n} from \"../../hooks/useRealtimeVoiceSession\";\nimport { useOfflineVoiceCapture } from \"../../hooks/useOfflineVoiceCapture\";\nimport { VoiceModeSettings } from \"./VoiceModeSettings\";\nimport { VoiceMetricsDisplay } from \"./VoiceMetricsDisplay\";\nimport { VoiceTranscriptPreview } from \"./VoiceTranscriptPreview\";\nimport { PendingRecordingsPanel } from \"./PendingRecordingsPanel\";\nimport {\n  VoiceBargeInIndicator,\n  type BargeInEvent,\n} from \"./VoiceBargeInIndicator\";\nimport { ConnectionStatusIndicator } from \"./ConnectionStatusIndicator\";\nimport { VoiceActivityIndicator } from \"./VoiceActivityIndicator\";\nimport { VoiceMicControl, type VoiceStatus } from \"./VoiceMicControl\";\nimport {\n  useVoiceSettingsStore,\n  VOICE_OPTIONS,\n} from \"../../stores/voiceSettingsStore\";\nimport { useAuth } from \"../../hooks/useAuth\";\nimport { useWebRTCClient } from \"../../hooks/useWebRTCClient\";\nimport { useStreamingAudio } from \"../../hooks/useStreamingAudio\";\n\n// Note: LANGUAGE_OPTIONS is defined in voiceSettingsStore but not currently used in this component\n\nexport interface VoiceModePanelProps {\n  conversationId?: string;\n  onClose?: () => void;\n  onTranscriptReceived?: (text: string, isFinal: boolean) => void;\n  /** Called when a final user transcript is ready to be added to chat */\n  onUserMessage?: (content: string) => void;\n  /** Called when a final assistant response is ready to be added to chat */\n  onAssistantMessage?: (content: string) => void;\n  /** Called when voice metrics are updated (for observability/export) */\n  onMetricsUpdate?: (metrics: VoiceMetrics) => void;\n}\n\nexport function VoiceModePanel({\n  conversationId,\n  onClose,\n  onTranscriptReceived,\n  onUserMessage,\n  onAssistantMessage,\n  onMetricsUpdate,\n}: VoiceModePanelProps) {\n  const audioContextRef = useRef<AudioContext | null>(null);\n  const audioQueueRef = useRef<ArrayBuffer[]>([]);\n  const isPlayingRef = useRef(false);\n\n  // Track currently playing Audio element for stopping on barge-in\n  const currentAudioRef = useRef<HTMLAudioElement | null>(null);\n  // Track if we're currently processing a response to prevent overlaps\n  const isProcessingResponseRef = useRef(false);\n  // Response ID to track current response and ignore stale ones\n  const currentResponseIdRef = useRef<number>(0);\n\n  const [userTranscript, setUserTranscript] = useState(\"\");\n  const [aiTranscript, setAiTranscript] = useState(\"\");\n  const [showSettings, setShowSettings] = useState(false);\n  const [showPendingRecordings, setShowPendingRecordings] = useState(false);\n  const [isSyncing, setIsSyncing] = useState(false);\n  const [isSynthesizing, setIsSynthesizing] = useState(false);\n  const [currentBargeInEvent, setCurrentBargeInEvent] =\n    useState<BargeInEvent | null>(null);\n\n  // Track pending final transcripts to add to chat\n  const pendingAiMessageRef = useRef<string | null>(null);\n\n  // Voice settings from store\n  const {\n    voice,\n    language,\n    showStatusHints,\n    ttsProvider,\n    elevenlabsVoiceId,\n    stability,\n    similarityBoost,\n    style,\n  } = useVoiceSettingsStore();\n  const { apiClient, tokens } = useAuth();\n\n  // Initialize streaming audio hook for low-latency TTS\n  const {\n    playStream,\n    stop: stopStreamingAudio,\n    state: _streamingState,\n  } = useStreamingAudio({\n    onFirstAudio: (ttfaMs) => {\n      console.log(`[VoiceModePanel] Streaming TTFA: ${ttfaMs}ms`);\n    },\n    onEnd: () => {\n      console.log(\"[VoiceModePanel] Streaming playback ended\");\n      isProcessingResponseRef.current = false;\n    },\n    onError: (error) => {\n      console.error(\"[VoiceModePanel] Streaming playback error:\", error);\n      isProcessingResponseRef.current = false;\n    },\n  });\n\n  /**\n   * Stop any currently playing audio (for barge-in)\n   */\n  const stopCurrentAudio = useCallback(() => {\n    // Stop standard Audio element playback\n    if (currentAudioRef.current) {\n      currentAudioRef.current.pause();\n      currentAudioRef.current.currentTime = 0;\n      // Revoke the object URL to free memory\n      if (currentAudioRef.current.src.startsWith(\"blob:\")) {\n        URL.revokeObjectURL(currentAudioRef.current.src);\n      }\n      currentAudioRef.current = null;\n      console.log(\"[VoiceModePanel] Stopped current audio playback\");\n    }\n    // Stop streaming audio playback\n    stopStreamingAudio();\n    // Clear the audio queue as well\n    audioQueueRef.current = [];\n    isPlayingRef.current = false;\n    // Increment response ID to invalidate any pending responses\n    currentResponseIdRef.current++;\n    isProcessingResponseRef.current = false;\n    setIsSynthesizing(false);\n  }, [stopStreamingAudio]);\n\n  const {\n    state: _webRTCState,\n    vadState: _vadState,\n    noiseSuppressionEnabled: _noiseSuppressionEnabled,\n    connect: connectWebRTC,\n    disconnect: disconnectWebRTC,\n    bargeIn: bargeInWebRTC,\n  } = useWebRTCClient({\n    sessionId: conversationId || \"voice-webrtc\",\n    token: tokens?.accessToken,\n  });\n\n  /**\n   * Combined barge-in: stop audio + cancel response + reconnect WebRTC + log event\n   */\n  const bargeIn = useCallback(() => {\n    console.log(\"[VoiceModePanel] Barge-in triggered\");\n\n    // Capture interrupted content before stopping\n    const interruptedContent = aiTranscript || pendingAiMessageRef.current;\n\n    stopCurrentAudio();\n    bargeInWebRTC();\n\n    // Create and show barge-in event\n    const bargeInEvent: BargeInEvent = {\n      id: `barge-${Date.now()}`,\n      timestamp: Date.now(),\n      interruptedContent: interruptedContent || undefined,\n      // Estimate completion based on whether we had partial content\n      completionPercentage: interruptedContent\n        ? Math.min(90, interruptedContent.length / 5)\n        : undefined,\n    };\n\n    setCurrentBargeInEvent(bargeInEvent);\n\n    // Log event to backend for analytics (fire-and-forget)\n    if (apiClient) {\n      apiClient\n        .logVoiceEvent({\n          conversation_id: conversationId || null,\n          event_type: \"barge_in\",\n          timestamp: bargeInEvent.timestamp,\n          metadata: {\n            interrupted_content: bargeInEvent.interruptedContent,\n            completion_percentage: bargeInEvent.completionPercentage,\n          },\n        })\n        .catch((err) => {\n          console.warn(\"[VoiceModePanel] Failed to log barge-in event:\", err);\n        });\n    }\n  }, [\n    stopCurrentAudio,\n    bargeInWebRTC,\n    aiTranscript,\n    apiClient,\n    conversationId,\n  ]);\n\n  // Initialize offline voice capture hook\n  const {\n    isRecording: isOfflineRecording,\n    isOfflineMode,\n    recordingDuration,\n    pendingCount,\n    startRecording: startOfflineRecording,\n    stopRecording: stopOfflineRecording,\n    cancelRecording: cancelOfflineRecording,\n    syncPendingRecordings,\n    getPendingRecordings,\n    deleteRecording,\n  } = useOfflineVoiceCapture({\n    conversationId: conversationId || \"default\",\n    apiClient: apiClient\n      ? {\n          transcribeAudio: async (audio: Blob, _filename?: string) => {\n            // Use the API client to transcribe audio\n            const result = await apiClient.transcribeAudio(audio);\n            return result;\n          },\n        }\n      : undefined,\n    onRecordingComplete: (recording) => {\n      console.log(\n        `[VoiceModePanel] Offline recording complete: ${recording.id}`,\n      );\n    },\n    onUploadComplete: (recording, transcribedText) => {\n      console.log(\n        `[VoiceModePanel] Recording uploaded: ${recording.id}, text: ${transcribedText}`,\n      );\n      // Add transcribed text to chat\n      if (transcribedText.trim()) {\n        onUserMessage?.(transcribedText);\n      }\n    },\n    onError: (error) => {\n      console.error(\"[VoiceModePanel] Offline recording error:\", error);\n    },\n  });\n\n  // Initialize Realtime voice session\n  const {\n    status,\n    error,\n    transcript,\n    partialTranscript,\n    isSpeaking,\n    connect,\n    disconnect,\n    isConnected,\n    isConnecting: _isConnecting,\n    isMicPermissionDenied,\n    resetFatalError,\n    metrics,\n  } = useRealtimeVoiceSession({\n    conversation_id: conversationId,\n    onTranscript: (transcriptData) => {\n      // Update local transcript display (AI responses)\n      if (transcriptData.text) {\n        if (transcriptData.is_final) {\n          // Final AI response - set full transcript and add to chat\n          setAiTranscript(transcriptData.text);\n          pendingAiMessageRef.current = transcriptData.text;\n          // Fire callback to add AI message to chat timeline\n          if (transcriptData.text.trim()) {\n            onAssistantMessage?.(transcriptData.text);\n          }\n        } else {\n          // Partial - just append to display\n          setAiTranscript((prev) => prev + transcriptData.text);\n        }\n        onTranscriptReceived?.(transcriptData.text, transcriptData.is_final);\n      }\n    },\n    onAudioChunk: (chunk) => {\n      // Queue audio chunks for playback\n      audioQueueRef.current.push(chunk.audio);\n      if (!isPlayingRef.current) {\n        playAudioQueue();\n      }\n    },\n    onError: (err) => {\n      console.error(\"[VoiceModePanel] Error:\", err);\n    },\n    onConnectionChange: (newStatus) => {\n      console.log(\"[VoiceModePanel] Status changed:\", newStatus);\n    },\n    onMetricsUpdate: (metrics) => {\n      // Log key metrics for observability (no secrets or PHI)\n      if (metrics.connectionTimeMs !== null) {\n        console.log(\n          `[VoiceModePanel] voice_session_connect_ms=${metrics.connectionTimeMs}`,\n        );\n      }\n      if (metrics.lastSttLatencyMs !== null) {\n        console.log(\n          `[VoiceModePanel] voice_stt_latency_ms=${metrics.lastSttLatencyMs}`,\n        );\n      }\n      if (metrics.lastResponseLatencyMs !== null) {\n        console.log(\n          `[VoiceModePanel] voice_first_reply_ms=${metrics.lastResponseLatencyMs}`,\n        );\n      }\n      if (metrics.sessionDurationMs !== null) {\n        console.log(\n          `[VoiceModePanel] voice_session_duration_ms=${metrics.sessionDurationMs}`,\n        );\n      }\n      // Forward to parent for backend export\n      onMetricsUpdate?.(metrics);\n    },\n    onRelayResult: async ({ answer }) => {\n      // Backend RAG answer: show and synthesize audio\n      if (answer) {\n        // Prevent overlapping responses - only process if not already processing\n        if (isProcessingResponseRef.current) {\n          console.log(\n            \"[VoiceModePanel] Skipping response - already processing another\",\n          );\n          return;\n        }\n\n        // Stop any currently playing audio first (before incrementing response ID)\n        stopCurrentAudio();\n\n        // Capture current response ID to check for staleness later\n        const responseId = ++currentResponseIdRef.current;\n        isProcessingResponseRef.current = true;\n\n        setAiTranscript(answer);\n        pendingAiMessageRef.current = answer;\n        onAssistantMessage?.(answer);\n\n        try {\n          setIsSynthesizing(true);\n\n          // Use streaming for ElevenLabs (lower latency), blob for OpenAI\n          const useStreaming = ttsProvider === \"elevenlabs\";\n          const effectiveVoice =\n            ttsProvider === \"elevenlabs\"\n              ? elevenlabsVoiceId || \"Rachel\"\n              : voice;\n\n          if (useStreaming) {\n            // Streaming playback for ElevenLabs - lower TTFA\n            console.log(\"[VoiceModePanel] Using streaming TTS (ElevenLabs)\");\n            const response = await apiClient.synthesizeSpeechStream(answer, {\n              voiceId: effectiveVoice,\n              provider: ttsProvider,\n              stability,\n              similarityBoost,\n              style,\n              language,\n            });\n\n            // Check if this response is still current (not cancelled by barge-in)\n            if (responseId !== currentResponseIdRef.current) {\n              console.log(\n                \"[VoiceModePanel] Response cancelled - skipping streaming playback\",\n              );\n              return;\n            }\n\n            // Play using streaming audio hook (isProcessingResponseRef is managed by onEnd/onError callbacks)\n            await playStream(response);\n          } else {\n            // Standard blob playback for OpenAI\n            console.log(\"[VoiceModePanel] Using standard TTS (OpenAI HD)\");\n            const audioBlob = await apiClient.synthesizeSpeech(\n              answer,\n              effectiveVoice,\n            );\n\n            // Check if this response is still current (not cancelled by barge-in)\n            if (responseId !== currentResponseIdRef.current) {\n              console.log(\n                \"[VoiceModePanel] Response cancelled - skipping playback\",\n              );\n              URL.revokeObjectURL(URL.createObjectURL(audioBlob));\n              return;\n            }\n\n            const url = URL.createObjectURL(audioBlob);\n            const audio = new Audio(url);\n            currentAudioRef.current = audio;\n\n            // Clean up when audio ends\n            audio.onended = () => {\n              if (currentAudioRef.current === audio) {\n                currentAudioRef.current = null;\n                isProcessingResponseRef.current = false;\n              }\n              URL.revokeObjectURL(url);\n            };\n\n            audio.onerror = () => {\n              if (currentAudioRef.current === audio) {\n                currentAudioRef.current = null;\n                isProcessingResponseRef.current = false;\n              }\n              URL.revokeObjectURL(url);\n            };\n\n            await audio.play().catch((err) => {\n              console.error(\"[VoiceModePanel] Audio play failed:\", err);\n              isProcessingResponseRef.current = false;\n            });\n          }\n        } catch (err) {\n          console.error(\"[VoiceModePanel] Failed to synthesize speech\", err);\n          isProcessingResponseRef.current = false;\n        } finally {\n          setIsSynthesizing(false);\n        }\n      }\n    },\n    onRelayPersist: ({ user_message_id, assistant_message_id }) => {\n      console.debug(\n        \"[VoiceModePanel] Persisted voice exchange\",\n        user_message_id,\n        assistant_message_id,\n      );\n    },\n    onSpeechStarted: () => {\n      // User started speaking - stop any playing audio (barge-in)\n      console.log(\"[VoiceModePanel] Speech started - stopping audio\");\n      stopCurrentAudio();\n    },\n    autoConnect: false, // Manual connect\n  });\n\n  // Use refs to avoid dependency array issues causing repeated connect/disconnect\n  const connectWebRTCRef = useRef(connectWebRTC);\n  const disconnectWebRTCRef = useRef(disconnectWebRTC);\n  connectWebRTCRef.current = connectWebRTC;\n  disconnectWebRTCRef.current = disconnectWebRTC;\n\n  useEffect(() => {\n    if (isConnected) {\n      void connectWebRTCRef.current();\n    } else {\n      disconnectWebRTCRef.current();\n    }\n  }, [isConnected]);\n\n  /**\n   * Play queued audio chunks\n   */\n  const playAudioQueue = async () => {\n    if (isPlayingRef.current || audioQueueRef.current.length === 0) return;\n\n    isPlayingRef.current = true;\n\n    try {\n      // Initialize audio context if needed\n      if (!audioContextRef.current) {\n        audioContextRef.current = new AudioContext({ sampleRate: 24000 });\n      }\n\n      const audioContext = audioContextRef.current;\n\n      // Process all queued audio chunks\n      while (audioQueueRef.current.length > 0) {\n        const audioData = audioQueueRef.current.shift();\n        if (!audioData) continue;\n\n        // Convert PCM16 to AudioBuffer\n        const audioBuffer = audioContext.createBuffer(\n          1, // mono\n          audioData.byteLength / 2, // 16-bit = 2 bytes per sample\n          24000,\n        );\n\n        const channelData = audioBuffer.getChannelData(0);\n        const view = new Int16Array(audioData);\n\n        // Convert Int16 to Float32 (-1.0 to 1.0)\n        for (let i = 0; i < view.length; i++) {\n          channelData[i] = view[i] / 32768.0;\n        }\n\n        // Play audio\n        const source = audioContext.createBufferSource();\n        source.buffer = audioBuffer;\n        source.connect(audioContext.destination);\n        source.start();\n\n        // Wait for audio to finish\n        await new Promise((resolve) => {\n          source.onended = resolve;\n        });\n      }\n    } catch (err) {\n      console.error(\"[VoiceModePanel] Audio playback error:\", err);\n    } finally {\n      isPlayingRef.current = false;\n    }\n  };\n\n  // Ref to hold the latest disconnect function for cleanup\n  const disconnectFnRef = useRef(disconnect);\n  disconnectFnRef.current = disconnect;\n\n  /**\n   * Cleanup on unmount only\n   */\n  useEffect(() => {\n    return () => {\n      disconnectFnRef.current();\n      if (audioContextRef.current) {\n        audioContextRef.current.close();\n        audioContextRef.current = null;\n      }\n      audioQueueRef.current = [];\n    };\n  }, []);\n\n  /**\n   * Update user transcript from the hook and add to chat\n   * The hook's transcript state contains the user's speech transcription\n   */\n  useEffect(() => {\n    if (transcript && transcript.trim()) {\n      setUserTranscript(transcript);\n      // Fire callback to add user message to chat timeline\n      onUserMessage?.(transcript);\n    }\n  }, [transcript, onUserMessage]);\n\n  /**\n   * Handle connect - resets fatal errors first to allow retry\n   */\n  const handleConnect = async () => {\n    try {\n      // Reset any fatal error state before attempting to connect\n      // This allows users to retry after fixing mic permissions\n      resetFatalError();\n      await connect();\n    } catch (err) {\n      // Error handling is done in the hook - it will set isMicPermissionDenied\n      // and update the error state appropriately\n      console.error(\"[VoiceModePanel] Failed to connect:\", err);\n    }\n  };\n\n  /**\n   * Handle disconnect - clears local state\n   */\n  const handleDisconnect = () => {\n    disconnect();\n    setUserTranscript(\"\");\n    setAiTranscript(\"\");\n  };\n\n  /**\n   * Handle sync with loading state\n   */\n  const handleSync = async () => {\n    setIsSyncing(true);\n    try {\n      await syncPendingRecordings();\n    } finally {\n      setIsSyncing(false);\n    }\n  };\n\n  return (\n    <div\n      className=\"bg-white border-2 border-primary-500 rounded-lg shadow-xl p-4 sm:p-6 space-y-3 sm:space-y-4\"\n      data-testid=\"voice-mode-panel\"\n    >\n      {/* Header - Simplified with connection status indicator */}\n      <div className=\"flex items-center justify-between\">\n        <div className=\"flex items-center space-x-3\">\n          <h3 className=\"text-lg font-semibold text-neutral-900\">Voice Mode</h3>\n          <ConnectionStatusIndicator\n            status={status}\n            isOfflineMode={isOfflineMode}\n            isOfflineRecording={isOfflineRecording}\n            reconnectAttempts={metrics.reconnectCount}\n          />\n        </div>\n\n        <div className=\"flex items-center space-x-2\">\n          {/* Pending recordings badge */}\n          {pendingCount > 0 && (\n            <button\n              type=\"button\"\n              onClick={() => setShowPendingRecordings(true)}\n              className=\"flex items-center gap-1 px-2 py-1 text-xs font-medium text-amber-700 bg-amber-100 hover:bg-amber-200 rounded-full transition-colors\"\n              aria-label={`${pendingCount} pending recording${pendingCount > 1 ? \"s\" : \"\"}`}\n              data-testid=\"pending-recordings-badge\"\n            >\n              <svg\n                className=\"w-3.5 h-3.5\"\n                fill=\"none\"\n                viewBox=\"0 0 24 24\"\n                stroke=\"currentColor\"\n                strokeWidth={2}\n              >\n                <path\n                  strokeLinecap=\"round\"\n                  strokeLinejoin=\"round\"\n                  d=\"M12 16.5V9.75m0 0l3 3m-3-3l-3 3M6.75 19.5a4.5 4.5 0 01-1.41-8.775 5.25 5.25 0 0110.233-2.33 3 3 0 013.758 3.848A3.752 3.752 0 0118 19.5H6.75z\"\n                />\n              </svg>\n              <span>{pendingCount}</span>\n            </button>\n          )}\n\n          {/* Voice/Language info */}\n          <span className=\"hidden sm:inline text-xs text-neutral-500\">\n            {VOICE_OPTIONS.find((v) => v.value === voice)?.label}\n          </span>\n\n          {/* Settings */}\n          <button\n            type=\"button\"\n            onClick={() => setShowSettings(true)}\n            className=\"p-1.5 text-neutral-400 hover:text-neutral-600 hover:bg-neutral-100 rounded-lg transition-colors\"\n            aria-label=\"Voice settings\"\n            data-testid=\"voice-settings-button\"\n          >\n            <svg\n              className=\"w-5 h-5\"\n              fill=\"none\"\n              viewBox=\"0 0 24 24\"\n              stroke=\"currentColor\"\n              strokeWidth={1.5}\n            >\n              <path\n                strokeLinecap=\"round\"\n                strokeLinejoin=\"round\"\n                d=\"M9.594 3.94c.09-.542.56-.94 1.11-.94h2.593c.55 0 1.02.398 1.11.94l.213 1.281c.063.374.313.686.645.87.074.04.147.083.22.127.324.196.72.257 1.075.124l1.217-.456a1.125 1.125 0 011.37.49l1.296 2.247a1.125 1.125 0 01-.26 1.431l-1.003.827c-.293.24-.438.613-.431.992a6.759 6.759 0 010 .255c-.007.378.138.75.43.99l1.005.828c.424.35.534.954.26 1.43l-1.298 2.247a1.125 1.125 0 01-1.369.491l-1.217-.456c-.355-.133-.75-.072-1.076.124a6.57 6.57 0 01-.22.128c-.331.183-.581.495-.644.869l-.213 1.28c-.09.543-.56.941-1.11.941h-2.594c-.55 0-1.02-.398-1.11-.94l-.213-1.281c-.062-.374-.312-.686-.644-.87a6.52 6.52 0 01-.22-.127c-.325-.196-.72-.257-1.076-.124l-1.217.456a1.125 1.125 0 01-1.369-.49l-1.297-2.247a1.125 1.125 0 01.26-1.431l1.004-.827c.292-.24.437-.613.43-.992a6.932 6.932 0 010-.255c.007-.378-.138-.75-.43-.99l-1.004-.828a1.125 1.125 0 01-.26-1.43l1.297-2.247a1.125 1.125 0 011.37-.491l1.216.456c.356.133.751.072 1.076-.124.072-.044.146-.087.22-.128.332-.183.582-.495.644-.869l.214-1.281z\"\n              />\n              <path\n                strokeLinecap=\"round\"\n                strokeLinejoin=\"round\"\n                d=\"M15 12a3 3 0 11-6 0 3 3 0 016 0z\"\n              />\n            </svg>\n          </button>\n\n          {/* Close */}\n          <button\n            type=\"button\"\n            onClick={onClose}\n            className=\"p-1.5 text-neutral-400 hover:text-neutral-600 hover:bg-neutral-100 rounded-lg transition-colors\"\n            aria-label=\"Close voice mode\"\n            data-testid=\"close-voice-mode\"\n          >\n            <svg\n              className=\"w-5 h-5\"\n              fill=\"none\"\n              viewBox=\"0 0 24 24\"\n              stroke=\"currentColor\"\n              strokeWidth={2}\n            >\n              <path\n                strokeLinecap=\"round\"\n                strokeLinejoin=\"round\"\n                d=\"M6 18L18 6M6 6l12 12\"\n              />\n            </svg>\n          </button>\n        </div>\n      </div>\n\n      {/* Central Microphone Control */}\n      <VoiceMicControl\n        status={status as VoiceStatus}\n        isConnected={isConnected}\n        isSpeaking={isSpeaking}\n        isMicPermissionDenied={isMicPermissionDenied}\n        isOfflineMode={isOfflineMode}\n        isOfflineRecording={isOfflineRecording}\n        recordingDuration={recordingDuration}\n        isSynthesizing={isSynthesizing}\n        onConnect={handleConnect}\n        onDisconnect={handleDisconnect}\n        onBargeIn={bargeIn}\n        onStartOfflineRecording={startOfflineRecording}\n        onStopOfflineRecording={() => {\n          stopOfflineRecording().then((recording) => {\n            if (recording)\n              console.log(\n                `[VoiceModePanel] Stopped recording: ${recording.id}`,\n              );\n          });\n        }}\n        onCancelOfflineRecording={cancelOfflineRecording}\n      />\n\n      {/* Microphone Permission Denied - Focused Error Card */}\n      {isMicPermissionDenied && (\n        <div\n          className=\"p-4 bg-red-50 border-2 border-red-300 rounded-lg\"\n          data-testid=\"mic-permission-error\"\n        >\n          <div className=\"flex items-start space-x-3\">\n            <div className=\"flex-shrink-0 w-10 h-10 rounded-full bg-red-100 flex items-center justify-center\">\n              <svg\n                xmlns=\"http://www.w3.org/2000/svg\"\n                fill=\"none\"\n                viewBox=\"0 0 24 24\"\n                strokeWidth={1.5}\n                stroke=\"currentColor\"\n                className=\"w-6 h-6 text-red-600\"\n              >\n                <path\n                  strokeLinecap=\"round\"\n                  strokeLinejoin=\"round\"\n                  d=\"M12 18.75a6 6 0 006-6v-1.5m-6 7.5a6 6 0 01-6-6v-1.5m6 7.5v3.75m-3.75 0h7.5M12 15.75a3 3 0 01-3-3V4.5a3 3 0 116 0v8.25a3 3 0 01-3 3z\"\n                />\n              </svg>\n            </div>\n            <div className=\"flex-1\">\n              <h4 className=\"text-base font-semibold text-red-900\">\n                Microphone Access Blocked\n              </h4>\n              <p className=\"text-sm text-red-700 mt-1\">\n                Voice mode needs microphone access to work. Your browser or\n                system settings are currently blocking it.\n              </p>\n              <div className=\"mt-3 p-3 bg-red-100/50 rounded-md\">\n                <p className=\"text-xs font-medium text-red-800 mb-2\">\n                  To fix this:\n                </p>\n                <ol className=\"text-xs text-red-700 space-y-1 list-decimal list-inside\">\n                  <li>\n                    Click the lock/info icon in your browser&apos;s address bar\n                  </li>\n                  <li>Find &quot;Microphone&quot; in the permissions list</li>\n                  <li>Change it to &quot;Allow&quot;</li>\n                  <li>Click &quot;Re-check Microphone&quot; below</li>\n                </ol>\n              </div>\n              <div className=\"mt-3 flex flex-wrap gap-2\">\n                <button\n                  type=\"button\"\n                  onClick={handleConnect}\n                  className=\"min-h-[44px] px-4 py-2 text-sm font-medium text-white bg-red-600 hover:bg-red-700 rounded-md transition-colors\"\n                  data-testid=\"recheck-mic-button\"\n                >\n                  Re-check Microphone\n                </button>\n                <button\n                  type=\"button\"\n                  onClick={onClose}\n                  className=\"min-h-[44px] px-4 py-2 text-sm font-medium text-red-700 bg-red-100 hover:bg-red-200 rounded-md transition-colors\"\n                  data-testid=\"use-text-only-button\"\n                >\n                  Use text-only mode\n                </button>\n              </div>\n            </div>\n          </div>\n        </div>\n      )}\n\n      {/* General Error Display (non-mic errors) */}\n      {error && !isMicPermissionDenied && (\n        <div\n          className=\"p-3 bg-red-50 border border-red-200 rounded-lg flex items-start space-x-2\"\n          data-testid=\"connection-error\"\n        >\n          <svg\n            xmlns=\"http://www.w3.org/2000/svg\"\n            fill=\"none\"\n            viewBox=\"0 0 24 24\"\n            strokeWidth={1.5}\n            stroke=\"currentColor\"\n            className=\"w-5 h-5 text-red-600 flex-shrink-0 mt-0.5\"\n          >\n            <path\n              strokeLinecap=\"round\"\n              strokeLinejoin=\"round\"\n              d=\"M12 9v3.75m9-.75a9 9 0 11-18 0 9 9 0 0118 0zm-9 3.75h.008v.008H12v-.008z\"\n            />\n          </svg>\n          <div className=\"flex-1\">\n            <p className=\"text-sm font-medium text-red-900\">Connection Error</p>\n            <p className=\"text-sm text-red-700 mt-1\">{error.message}</p>\n            <div className=\"mt-2 flex flex-wrap gap-2\">\n              <button\n                type=\"button\"\n                onClick={handleConnect}\n                className=\"min-h-[44px] px-3 py-2 text-sm font-medium text-red-700 bg-red-100 hover:bg-red-200 rounded transition-colors\"\n              >\n                Try Again\n              </button>\n              <button\n                type=\"button\"\n                onClick={onClose}\n                className=\"min-h-[44px] px-3 py-2 text-sm font-medium text-red-700 bg-red-100 hover:bg-red-200 rounded transition-colors\"\n                data-testid=\"use-text-only-error-button\"\n              >\n                Use text-only mode instead\n              </button>\n            </div>\n          </div>\n        </div>\n      )}\n\n      {/* Offline Mode Info Card */}\n      {isOfflineMode && !isConnected && !isOfflineRecording && (\n        <div\n          className=\"p-3 bg-orange-50 border border-orange-200 rounded-lg flex items-start space-x-2\"\n          data-testid=\"offline-mode-info\"\n        >\n          <svg\n            xmlns=\"http://www.w3.org/2000/svg\"\n            fill=\"none\"\n            viewBox=\"0 0 24 24\"\n            strokeWidth={1.5}\n            stroke=\"currentColor\"\n            className=\"w-5 h-5 text-orange-600 flex-shrink-0 mt-0.5\"\n          >\n            <path\n              strokeLinecap=\"round\"\n              strokeLinejoin=\"round\"\n              d=\"M8.288 15.038a5.25 5.25 0 017.424 0M5.106 11.856c3.807-3.808 9.98-3.808 13.788 0M1.924 8.674c5.565-5.565 14.587-5.565 20.152 0M12.53 18.22l-.53.53-.53-.53a.75.75 0 011.06 0z\"\n            />\n          </svg>\n          <div className=\"flex-1\">\n            <p className=\"text-sm font-medium text-orange-900\">\n              You&apos;re Currently Offline\n            </p>\n            <p className=\"text-sm text-orange-700 mt-1\">\n              Real-time voice isn&apos;t available, but you can record voice\n              messages offline. They&apos;ll be transcribed automatically when\n              you&apos;re back online.\n            </p>\n            {pendingCount > 0 && (\n              <p className=\"text-xs text-orange-600 mt-2\">\n                {pendingCount} recording{pendingCount > 1 ? \"s\" : \"\"} waiting to\n                sync\n              </p>\n            )}\n          </div>\n        </div>\n      )}\n\n      {/* Voice Activity Visualization */}\n      <VoiceActivityIndicator\n        isSpeaking={isSpeaking}\n        isSynthesizing={isSynthesizing}\n        isConnected={isConnected}\n        className=\"py-2\"\n      />\n\n      {/* Live Transcript Preview (while speaking) */}\n      {isConnected && (\n        <VoiceTranscriptPreview\n          partialTranscript={partialTranscript}\n          isSpeaking={isSpeaking}\n        />\n      )}\n\n      {/* Transcript Display */}\n      {isConnected && (\n        <div className=\"space-y-3\">\n          {/* User Transcript */}\n          {userTranscript && (\n            <div className=\"p-3 bg-blue-50 border border-blue-200 rounded-lg\">\n              <p className=\"text-xs font-semibold text-blue-900 mb-1\">You:</p>\n              <p className=\"text-sm text-blue-800\">{userTranscript}</p>\n            </div>\n          )}\n\n          {/* AI Transcript */}\n          {aiTranscript && (\n            <div className=\"p-3 bg-purple-50 border border-purple-200 rounded-lg\">\n              <p className=\"text-xs font-semibold text-purple-900 mb-1\">\n                AI Assistant:\n              </p>\n              <p className=\"text-sm text-purple-800\">{aiTranscript}</p>\n            </div>\n          )}\n        </div>\n      )}\n\n      {/* Voice Metrics Display */}\n      <VoiceMetricsDisplay metrics={metrics} isConnected={isConnected} />\n\n      {/* Instructions - only show when showStatusHints is enabled */}\n      {!isConnected && !error && showStatusHints && (\n        <div className=\"p-3 bg-blue-50/70 border border-blue-200/50 rounded-lg text-center\">\n          <p className=\"text-sm text-blue-700\">\n            Tap the microphone to start a voice conversation\n          </p>\n        </div>\n      )}\n\n      {/* Voice Settings Modal */}\n      <VoiceModeSettings\n        isOpen={showSettings}\n        onClose={() => setShowSettings(false)}\n      />\n\n      {/* Pending Recordings Modal */}\n      {showPendingRecordings && (\n        <div\n          className=\"fixed inset-0 z-50 flex items-center justify-center bg-black/50 p-4\"\n          onClick={(e) => {\n            if (e.target === e.currentTarget) {\n              setShowPendingRecordings(false);\n            }\n          }}\n        >\n          <PendingRecordingsPanel\n            getPendingRecordings={getPendingRecordings}\n            deleteRecording={deleteRecording}\n            syncPendingRecordings={handleSync}\n            isSyncing={isSyncing}\n            isOffline={isOfflineMode}\n            onClose={() => setShowPendingRecordings(false)}\n          />\n        </div>\n      )}\n\n      {/* Voice Barge-in Indicator */}\n      <VoiceBargeInIndicator\n        event={currentBargeInEvent}\n        onDismiss={() => setCurrentBargeInEvent(null)}\n        autoDismissMs={4000}\n      />\n    </div>\n  );\n}\n"
}
