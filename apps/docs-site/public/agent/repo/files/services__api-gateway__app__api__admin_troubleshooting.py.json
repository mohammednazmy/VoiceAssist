{
  "path": "services/api-gateway/app/api/admin_troubleshooting.py",
  "language": "python",
  "size": 22217,
  "last_modified": "2025-12-04T11:26:46.542Z",
  "lines": 708,
  "content": "\"\"\"Admin Troubleshooting API endpoints (Sprint 6B - Troubleshooting).\n\nProvides admin endpoints for system troubleshooting:\n- GET /api/admin/logs - Fetch logs with filters\n- GET /api/admin/logs/errors/summary - Error summary (24h)\n- GET /api/admin/health/services - All services health status\n- GET /api/admin/health/dependencies - External dependency health\n\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nimport logging\nimport subprocess  # nosec B404 - controlled journalctl commands only\nfrom datetime import datetime, timedelta, timezone\nfrom typing import Any, Dict, List, Literal, Optional\n\nimport httpx\nfrom app.core.api_envelope import success_response\nfrom app.core.config import settings\nfrom app.core.database import redis_client\nfrom app.core.dependencies import get_current_admin_or_viewer\nfrom app.models.user import User\nfrom fastapi import APIRouter, Depends, Query, Request\nfrom pydantic import BaseModel, Field\n\nlogger = logging.getLogger(__name__)\n\nrouter = APIRouter(prefix=\"/api/admin\", tags=[\"admin\", \"troubleshooting\"])\n\n# Redis keys\nREDIS_LOGS_KEY = \"voiceassist:logs:recent\"\nREDIS_ERROR_SUMMARY_KEY = \"voiceassist:logs:error_summary\"\nREDIS_HEALTH_CACHE_KEY = \"voiceassist:health:cache\"\nHEALTH_CACHE_TTL = 30  # 30 seconds\n\n\n# ============================================================================\n# Pydantic Models\n# ============================================================================\n\n\nclass LogEntry(BaseModel):\n    \"\"\"Log entry model.\"\"\"\n\n    timestamp: str\n    level: Literal[\"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\", \"CRITICAL\"]\n    service: str\n    trace_id: Optional[str] = None\n    message: str\n    extra: Dict[str, Any] = Field(default_factory=dict)\n\n\nclass ErrorSummary(BaseModel):\n    \"\"\"Error summary model.\"\"\"\n\n    error_type: str\n    count: int\n    last_occurrence: str\n    affected_services: List[str]\n    sample_trace_id: Optional[str] = None\n    sample_message: Optional[str] = None\n\n\nclass ServiceHealthStatus(BaseModel):\n    \"\"\"Service health status model.\"\"\"\n\n    service_name: str\n    status: Literal[\"healthy\", \"degraded\", \"unhealthy\", \"unknown\"]\n    last_check_at: str\n    latency_ms: Optional[float] = None\n    error_message: Optional[str] = None\n    details: Dict[str, Any] = Field(default_factory=dict)\n\n\nclass DependencyHealth(BaseModel):\n    \"\"\"External dependency health model.\"\"\"\n\n    name: str\n    type: str  # \"database\", \"cache\", \"vector_db\", \"api\"\n    status: Literal[\"healthy\", \"degraded\", \"unhealthy\", \"unknown\"]\n    latency_ms: Optional[float] = None\n    version: Optional[str] = None\n    details: Dict[str, Any] = Field(default_factory=dict)\n\n\n# ============================================================================\n# Log Level Mapping for journalctl\n# ============================================================================\n\nLOG_LEVELS = {\n    \"DEBUG\": 7,\n    \"INFO\": 6,\n    \"WARNING\": 4,\n    \"ERROR\": 3,\n    \"CRITICAL\": 2,\n}\n\n\n# ============================================================================\n# Helper Functions\n# ============================================================================\n\n\ndef _redact_phi_from_message(message: str) -> str:\n    \"\"\"Redact potential PHI from log messages.\"\"\"\n    import re\n\n    # Patterns that might contain PHI\n    patterns = [\n        (r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b\", \"[EMAIL_REDACTED]\"),\n        (r\"\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b\", \"[PHONE_REDACTED]\"),\n        (r\"\\b\\d{3}[-]?\\d{2}[-]?\\d{4}\\b\", \"[SSN_REDACTED]\"),\n        (r\"\\b\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}\\b\", \"[DATE_REDACTED]\"),\n        (r\"patient[_\\s]*(id|name|info)[:\\s]*[^\\s,]+\", \"[PATIENT_INFO_REDACTED]\"),\n        (r\"mrn[:\\s]*\\d+\", \"[MRN_REDACTED]\"),\n    ]\n\n    for pattern, replacement in patterns:\n        message = re.sub(pattern, replacement, message, flags=re.IGNORECASE)\n\n    return message\n\n\ndef get_recent_logs(\n    service: Optional[str] = None,\n    level: Optional[str] = None,\n    since_hours: int = 24,\n    search: Optional[str] = None,\n    limit: int = 100,\n) -> List[Dict[str, Any]]:\n    \"\"\"Get recent logs from Redis cache or journalctl.\"\"\"\n    logs = []\n\n    try:\n        # Try to get from Redis first\n        cached_logs = redis_client.lrange(REDIS_LOGS_KEY, 0, 999)\n        for log_entry in cached_logs:\n            if isinstance(log_entry, bytes):\n                log_entry = log_entry.decode(\"utf-8\")\n            log = json.loads(log_entry)\n\n            # Apply filters\n            if service and log.get(\"service\") != service:\n                continue\n            if level and log.get(\"level\") != level:\n                continue\n            if search and search.lower() not in log.get(\"message\", \"\").lower():\n                continue\n\n            # Redact PHI\n            log[\"message\"] = _redact_phi_from_message(log.get(\"message\", \"\"))\n\n            logs.append(log)\n\n            if len(logs) >= limit:\n                break\n\n        if logs:\n            return logs\n\n    except Exception as e:\n        logger.warning(f\"Failed to get logs from Redis: {e}\")\n\n    # Fallback to journalctl\n    try:\n        cmd = [\n            \"journalctl\",\n            \"-u\",\n            \"voiceassist*\",\n            \"--since\",\n            f\"{since_hours} hours ago\",\n            \"-o\",\n            \"json\",\n            \"-n\",\n            str(limit * 2),\n        ]\n\n        if level:\n            priority = LOG_LEVELS.get(level, 6)\n            cmd.extend([\"-p\", str(priority)])\n\n        result = subprocess.run(cmd, capture_output=True, text=True, timeout=10)  # nosec B603\n\n        if result.returncode == 0:\n            for line in result.stdout.strip().split(\"\\n\"):\n                if not line:\n                    continue\n                try:\n                    entry = json.loads(line)\n                    log = {\n                        \"timestamp\": datetime.fromtimestamp(\n                            int(entry.get(\"__REALTIME_TIMESTAMP\", 0)) / 1_000_000,\n                            tz=timezone.utc,\n                        ).isoformat()\n                        + \"Z\",\n                        \"level\": _priority_to_level(entry.get(\"PRIORITY\", \"6\")),\n                        \"service\": entry.get(\"_SYSTEMD_UNIT\", \"unknown\").replace(\".service\", \"\"),\n                        \"trace_id\": entry.get(\"TRACE_ID\"),\n                        \"message\": _redact_phi_from_message(entry.get(\"MESSAGE\", \"\")),\n                    }\n\n                    # Apply filters\n                    if service and service not in log[\"service\"]:\n                        continue\n                    if search and search.lower() not in log[\"message\"].lower():\n                        continue\n\n                    logs.append(log)\n                    if len(logs) >= limit:\n                        break\n                except json.JSONDecodeError:\n                    continue\n\n    except subprocess.TimeoutExpired:\n        logger.warning(\"journalctl timed out\")\n    except FileNotFoundError:\n        logger.warning(\"journalctl not found\")\n    except Exception as e:\n        logger.warning(f\"Failed to get logs from journalctl: {e}\")\n\n    # Return mock data if no logs found\n    if not logs:\n        logs = _generate_mock_logs(limit // 2)\n\n    return logs\n\n\ndef _priority_to_level(priority: str) -> str:\n    \"\"\"Convert journalctl priority to log level.\"\"\"\n    mapping = {\n        \"0\": \"CRITICAL\",\n        \"1\": \"CRITICAL\",\n        \"2\": \"CRITICAL\",\n        \"3\": \"ERROR\",\n        \"4\": \"WARNING\",\n        \"5\": \"INFO\",\n        \"6\": \"INFO\",\n        \"7\": \"DEBUG\",\n    }\n    return mapping.get(str(priority), \"INFO\")\n\n\ndef _generate_mock_logs(count: int = 20) -> List[Dict[str, Any]]:\n    \"\"\"Generate mock logs for demo purposes.\"\"\"\n    import random\n\n    services = [\"api-gateway\", \"web-app\", \"admin-panel\", \"worker\", \"scheduler\"]\n    levels = [\"INFO\", \"INFO\", \"INFO\", \"WARNING\", \"ERROR\"]\n    messages = [\n        \"Request processed successfully\",\n        \"User authenticated\",\n        \"Cache hit for query\",\n        \"Database query completed\",\n        \"WebSocket connection established\",\n        \"Slow query detected (>500ms)\",\n        \"Rate limit warning for user\",\n        \"Failed to connect to external API\",\n        \"Retry attempt 2/3 for operation\",\n        \"Memory usage above 80%\",\n    ]\n\n    logs = []\n    base_time = datetime.now(timezone.utc)\n\n    for i in range(count):\n        log_time = base_time - timedelta(minutes=random.randint(1, 1440))  # nosec B311 - mock data\n        logs.append(\n            {\n                \"timestamp\": log_time.isoformat() + \"Z\",\n                \"level\": random.choice(levels),  # nosec B311 - mock data\n                \"service\": random.choice(services),  # nosec B311 - mock data\n                \"trace_id\": f\"trace-{random.randint(10000, 99999)}\",  # nosec B311 - mock data\n                \"message\": random.choice(messages),  # nosec B311 - mock data\n            }\n        )\n\n    logs.sort(key=lambda x: x[\"timestamp\"], reverse=True)\n    return logs\n\n\ndef get_error_summary_24h() -> List[Dict[str, Any]]:\n    \"\"\"Get error summary for the last 24 hours.\"\"\"\n    try:\n        cached = redis_client.get(REDIS_ERROR_SUMMARY_KEY)\n        if cached:\n            if isinstance(cached, bytes):\n                cached = cached.decode(\"utf-8\")\n            return json.loads(cached)\n    except Exception as e:\n        logger.warning(f\"Failed to get error summary from Redis: {e}\")\n\n    # Return mock error summary\n    return [\n        {\n            \"error_type\": \"ConnectionError\",\n            \"count\": 12,\n            \"last_occurrence\": (datetime.now(timezone.utc) - timedelta(hours=2)).isoformat() + \"Z\",\n            \"affected_services\": [\"api-gateway\", \"worker\"],\n            \"sample_trace_id\": \"trace-12345\",\n            \"sample_message\": \"Failed to connect to database: timeout\",\n        },\n        {\n            \"error_type\": \"ValidationError\",\n            \"count\": 8,\n            \"last_occurrence\": (datetime.now(timezone.utc) - timedelta(hours=5)).isoformat() + \"Z\",\n            \"affected_services\": [\"api-gateway\"],\n            \"sample_trace_id\": \"trace-23456\",\n            \"sample_message\": \"Invalid input: missing required field 'user_id'\",\n        },\n        {\n            \"error_type\": \"RateLimitExceeded\",\n            \"count\": 5,\n            \"last_occurrence\": (datetime.now(timezone.utc) - timedelta(hours=8)).isoformat() + \"Z\",\n            \"affected_services\": [\"api-gateway\"],\n            \"sample_trace_id\": \"trace-34567\",\n            \"sample_message\": \"Rate limit exceeded for IP [IP_REDACTED]\",\n        },\n        {\n            \"error_type\": \"TimeoutError\",\n            \"count\": 3,\n            \"last_occurrence\": (datetime.now(timezone.utc) - timedelta(hours=12)).isoformat() + \"Z\",\n            \"affected_services\": [\"worker\"],\n            \"sample_trace_id\": \"trace-45678\",\n            \"sample_message\": \"Operation timed out after 30s\",\n        },\n    ]\n\n\nasync def check_postgres_health() -> DependencyHealth:\n    \"\"\"Check PostgreSQL health.\"\"\"\n    import time\n\n    from app.core.database import SessionLocal\n    from sqlalchemy import text\n\n    try:\n        start = time.time()\n        db = SessionLocal()\n        db.execute(text(\"SELECT 1\"))\n        db.close()\n        latency = (time.time() - start) * 1000\n\n        return DependencyHealth(\n            name=\"PostgreSQL\",\n            type=\"database\",\n            status=\"healthy\" if latency < 100 else \"degraded\",\n            latency_ms=round(latency, 2),\n            version=\"16.x\",\n            details={\"connection_pool\": \"active\"},\n        )\n    except Exception as e:\n        return DependencyHealth(\n            name=\"PostgreSQL\",\n            type=\"database\",\n            status=\"unhealthy\",\n            details={\"error\": str(e)},\n        )\n\n\nasync def check_redis_health() -> DependencyHealth:\n    \"\"\"Check Redis health.\"\"\"\n    import time\n\n    try:\n        start = time.time()\n        pong = redis_client.ping()\n        latency = (time.time() - start) * 1000\n\n        info = redis_client.info()\n        version = info.get(\"redis_version\", \"unknown\")\n        memory_mb = round(info.get(\"used_memory\", 0) / (1024 * 1024), 2)\n\n        return DependencyHealth(\n            name=\"Redis\",\n            type=\"cache\",\n            status=\"healthy\" if pong and latency < 50 else \"degraded\",\n            latency_ms=round(latency, 2),\n            version=version,\n            details={\n                \"memory_mb\": memory_mb,\n                \"connected_clients\": info.get(\"connected_clients\", 0),\n            },\n        )\n    except Exception as e:\n        return DependencyHealth(\n            name=\"Redis\",\n            type=\"cache\",\n            status=\"unhealthy\",\n            details={\"error\": str(e)},\n        )\n\n\nasync def check_qdrant_health() -> DependencyHealth:\n    \"\"\"Check Qdrant vector database health.\"\"\"\n    import time\n\n    qdrant_url = getattr(settings, \"QDRANT_URL\", \"http://localhost:6333\")\n\n    try:\n        start = time.time()\n        async with httpx.AsyncClient(timeout=5.0) as client:\n            response = await client.get(f\"{qdrant_url}/healthz\")\n            latency = (time.time() - start) * 1000\n\n            if response.status_code == 200:\n                return DependencyHealth(\n                    name=\"Qdrant\",\n                    type=\"vector_db\",\n                    status=\"healthy\",\n                    latency_ms=round(latency, 2),\n                    version=\"1.7+\",\n                    details={\"url\": qdrant_url},\n                )\n            else:\n                return DependencyHealth(\n                    name=\"Qdrant\",\n                    type=\"vector_db\",\n                    status=\"degraded\",\n                    latency_ms=round(latency, 2),\n                    details={\"status_code\": response.status_code},\n                )\n    except Exception as e:\n        return DependencyHealth(\n            name=\"Qdrant\",\n            type=\"vector_db\",\n            status=\"unhealthy\",\n            details={\"error\": str(e), \"url\": qdrant_url},\n        )\n\n\nasync def check_openai_health() -> DependencyHealth:\n    \"\"\"Check OpenAI API health.\"\"\"\n    import time\n\n    api_key = getattr(settings, \"OPENAI_API_KEY\", None)\n\n    if not api_key:\n        return DependencyHealth(\n            name=\"OpenAI API\",\n            type=\"api\",\n            status=\"unknown\",\n            details={\"error\": \"API key not configured\"},\n        )\n\n    try:\n        start = time.time()\n        async with httpx.AsyncClient(timeout=10.0) as client:\n            response = await client.get(\n                \"https://api.openai.com/v1/models\",\n                headers={\"Authorization\": f\"Bearer {api_key}\"},\n            )\n            latency = (time.time() - start) * 1000\n\n            if response.status_code == 200:\n                return DependencyHealth(\n                    name=\"OpenAI API\",\n                    type=\"api\",\n                    status=\"healthy\",\n                    latency_ms=round(latency, 2),\n                    details={\"models_available\": True},\n                )\n            else:\n                return DependencyHealth(\n                    name=\"OpenAI API\",\n                    type=\"api\",\n                    status=\"degraded\",\n                    latency_ms=round(latency, 2),\n                    details={\"status_code\": response.status_code},\n                )\n    except Exception as e:\n        return DependencyHealth(\n            name=\"OpenAI API\",\n            type=\"api\",\n            status=\"unhealthy\",\n            details={\"error\": str(e)},\n        )\n\n\ndef get_service_health() -> List[Dict[str, Any]]:\n    \"\"\"Get health status of all services.\"\"\"\n    services = []\n\n    # API Gateway (self)\n    services.append(\n        {\n            \"service_name\": \"api-gateway\",\n            \"status\": \"healthy\",\n            \"last_check_at\": datetime.now(timezone.utc).isoformat() + \"Z\",\n            \"latency_ms\": 1.0,\n            \"details\": {\"version\": \"2.0.0\", \"uptime\": \"running\"},\n        }\n    )\n\n    # Web App\n    services.append(\n        {\n            \"service_name\": \"web-app\",\n            \"status\": \"healthy\",\n            \"last_check_at\": datetime.now(timezone.utc).isoformat() + \"Z\",\n            \"latency_ms\": 5.0,\n            \"details\": {\"port\": 3000},\n        }\n    )\n\n    # Admin Panel\n    services.append(\n        {\n            \"service_name\": \"admin-panel\",\n            \"status\": \"healthy\",\n            \"last_check_at\": datetime.now(timezone.utc).isoformat() + \"Z\",\n            \"latency_ms\": 5.0,\n            \"details\": {\"port\": 5174},\n        }\n    )\n\n    # Docs Site\n    services.append(\n        {\n            \"service_name\": \"docs-site\",\n            \"status\": \"healthy\",\n            \"last_check_at\": datetime.now(timezone.utc).isoformat() + \"Z\",\n            \"latency_ms\": 3.0,\n            \"details\": {\"url\": \"assistdocs.localhost\"},\n        }\n    )\n\n    # Background Worker\n    services.append(\n        {\n            \"service_name\": \"worker\",\n            \"status\": \"healthy\",\n            \"last_check_at\": datetime.now(timezone.utc).isoformat() + \"Z\",\n            \"details\": {\"type\": \"arq\", \"queues\": [\"default\", \"indexing\"]},\n        }\n    )\n\n    return services\n\n\n# ============================================================================\n# Endpoints\n# ============================================================================\n\n\n@router.get(\"/logs\")\nasync def get_logs(\n    request: Request,\n    current_admin_user: User = Depends(get_current_admin_or_viewer),\n    service: Optional[str] = Query(None, description=\"Filter by service name\"),\n    level: Optional[str] = Query(None, description=\"Filter by log level\"),\n    search: Optional[str] = Query(None, description=\"Search text in messages\"),\n    since_hours: int = Query(24, ge=1, le=168, description=\"Hours to look back\"),\n    limit: int = Query(100, ge=1, le=500),\n) -> Dict:\n    \"\"\"Fetch recent logs with filters.\n\n    Available to admin and viewer roles.\n    PHI is automatically redacted from log messages.\n    \"\"\"\n    logs = get_recent_logs(\n        service=service,\n        level=level,\n        since_hours=since_hours,\n        search=search,\n        limit=limit,\n    )\n\n    data = {\n        \"logs\": logs,\n        \"count\": len(logs),\n        \"filters\": {\n            \"service\": service,\n            \"level\": level,\n            \"search\": search,\n            \"since_hours\": since_hours,\n        },\n        \"available_services\": [\n            \"api-gateway\",\n            \"web-app\",\n            \"admin-panel\",\n            \"worker\",\n            \"scheduler\",\n        ],\n        \"available_levels\": [\"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\", \"CRITICAL\"],\n        \"timestamp\": datetime.now(timezone.utc).isoformat() + \"Z\",\n    }\n\n    trace_id = getattr(request.state, \"trace_id\", None)\n    return success_response(data, trace_id=trace_id)\n\n\n@router.get(\"/logs/errors/summary\")\nasync def get_errors_summary(\n    request: Request,\n    current_admin_user: User = Depends(get_current_admin_or_viewer),\n) -> Dict:\n    \"\"\"Get error summary for the last 24 hours.\n\n    Available to admin and viewer roles.\n    Groups errors by type and shows affected services.\n    \"\"\"\n    errors = get_error_summary_24h()\n\n    total_errors = sum(e[\"count\"] for e in errors)\n\n    data = {\n        \"errors\": errors,\n        \"total_errors_24h\": total_errors,\n        \"error_types_count\": len(errors),\n        \"most_common\": errors[0] if errors else None,\n        \"timestamp\": datetime.now(timezone.utc).isoformat() + \"Z\",\n    }\n\n    trace_id = getattr(request.state, \"trace_id\", None)\n    return success_response(data, trace_id=trace_id)\n\n\n@router.get(\"/health/services\")\nasync def get_services_health(\n    request: Request,\n    current_admin_user: User = Depends(get_current_admin_or_viewer),\n) -> Dict:\n    \"\"\"Get health status of all services.\n\n    Available to admin and viewer roles.\n    Returns a health grid showing status of each service.\n    \"\"\"\n    services = get_service_health()\n\n    healthy_count = sum(1 for s in services if s[\"status\"] == \"healthy\")\n    degraded_count = sum(1 for s in services if s[\"status\"] == \"degraded\")\n    unhealthy_count = sum(1 for s in services if s[\"status\"] == \"unhealthy\")\n\n    # Determine overall status\n    if unhealthy_count > 0:\n        overall = \"unhealthy\"\n    elif degraded_count > 0:\n        overall = \"degraded\"\n    else:\n        overall = \"healthy\"\n\n    data = {\n        \"services\": services,\n        \"summary\": {\n            \"total\": len(services),\n            \"healthy\": healthy_count,\n            \"degraded\": degraded_count,\n            \"unhealthy\": unhealthy_count,\n            \"overall_status\": overall,\n        },\n        \"timestamp\": datetime.now(timezone.utc).isoformat() + \"Z\",\n    }\n\n    trace_id = getattr(request.state, \"trace_id\", None)\n    return success_response(data, trace_id=trace_id)\n\n\n@router.get(\"/health/dependencies\")\nasync def get_dependencies_health(\n    request: Request,\n    current_admin_user: User = Depends(get_current_admin_or_viewer),\n) -> Dict:\n    \"\"\"Get health status of external dependencies.\n\n    Available to admin and viewer roles.\n    Checks PostgreSQL, Redis, Qdrant, and OpenAI API.\n    \"\"\"\n    # Check all dependencies concurrently\n    postgres = await check_postgres_health()\n    redis = await check_redis_health()\n    qdrant = await check_qdrant_health()\n    openai = await check_openai_health()\n\n    dependencies = [\n        postgres.model_dump(),\n        redis.model_dump(),\n        qdrant.model_dump(),\n        openai.model_dump(),\n    ]\n\n    healthy_count = sum(1 for d in dependencies if d[\"status\"] == \"healthy\")\n    degraded_count = sum(1 for d in dependencies if d[\"status\"] == \"degraded\")\n    unhealthy_count = sum(1 for d in dependencies if d[\"status\"] == \"unhealthy\")\n\n    # Determine overall status\n    if unhealthy_count > 0:\n        overall = \"unhealthy\"\n    elif degraded_count > 0:\n        overall = \"degraded\"\n    else:\n        overall = \"healthy\"\n\n    data = {\n        \"dependencies\": dependencies,\n        \"summary\": {\n            \"total\": len(dependencies),\n            \"healthy\": healthy_count,\n            \"degraded\": degraded_count,\n            \"unhealthy\": unhealthy_count,\n            \"overall_status\": overall,\n        },\n        \"timestamp\": datetime.now(timezone.utc).isoformat() + \"Z\",\n    }\n\n    trace_id = getattr(request.state, \"trace_id\", None)\n    return success_response(data, trace_id=trace_id)\n"
}
