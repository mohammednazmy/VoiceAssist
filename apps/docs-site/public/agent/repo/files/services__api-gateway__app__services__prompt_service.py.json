{
  "path": "services/api-gateway/app/services/prompt_service.py",
  "language": "python",
  "size": 42707,
  "last_modified": "2025-12-04T11:27:00.044Z",
  "lines": 1146,
  "content": "\"\"\"Prompt Service with Multi-Level Caching and Redis Pub/Sub.\n\nProvides dynamic AI prompt management with:\n- L1 Cache: In-memory TTL cache (1-minute TTL) for sub-millisecond access\n- L2 Cache: Redis distributed cache (5-minute TTL) for cross-instance consistency\n- L3 Persistence: PostgreSQL for durability\n- Redis Pub/Sub: Real-time propagation of prompt updates\n\nUsage:\n    from app.services.prompt_service import prompt_service\n\n    # Get prompt for intent (with caching)\n    prompt_text = await prompt_service.get_system_prompt_for_intent(\"diagnosis\")\n\n    # Get voice instructions\n    voice_prompt = await prompt_service.get_voice_instructions()\n\n    # CRUD operations\n    prompt = await prompt_service.create_prompt(data, actor, db)\n    await prompt_service.publish_prompt(prompt_id, actor, db)\n\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nfrom datetime import datetime, timezone\nfrom typing import Any, Dict, List, Optional\nfrom uuid import UUID\n\nfrom app.core.database import SessionLocal, redis_client\nfrom app.core.logging import get_logger\nfrom app.models.prompt import Prompt, PromptStatus, PromptVersion\nfrom app.models.user import User\nfrom cachetools import TTLCache\nfrom sqlalchemy import and_, or_\nfrom sqlalchemy.exc import IntegrityError\nfrom sqlalchemy.orm import Session, joinedload\n\nlogger = get_logger(__name__)\n\n# Cache configuration\nPROMPT_CACHE_PREFIX = \"prompt:\"\nPROMPT_NAME_CACHE_PREFIX = \"prompt_name:\"\nPROMPT_INTENT_CACHE_PREFIX = \"prompt_intent:\"\nPROMPT_CACHE_TTL = 300  # 5 minutes (L2 TTL)\nLOCAL_CACHE_TTL = 60  # 1 minute (L1 TTL)\nLOCAL_CACHE_MAX_SIZE = 500\n\n# Redis Pub/Sub channel\nPROMPT_UPDATES_CHANNEL = \"prompt:updates\"\n\n\nclass PromptService:\n    \"\"\"Service for managing AI prompts with multi-level caching.\n\n    Three-tier caching architecture:\n    - L1: In-memory TTL cache (1-minute TTL) - fastest, process-local\n    - L2: Redis distributed cache (5-minute TTL) - shared across instances\n    - L3: PostgreSQL persistence - source of truth\n\n    Features:\n    - Sub-millisecond prompt lookups via L1 cache\n    - Cross-instance consistency via L2 (Redis)\n    - Real-time propagation via Redis Pub/Sub\n    - Version history with rollback capability\n    - Draft/Published workflow for sandbox testing\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize prompt service with caching.\"\"\"\n        self.logger = get_logger(__name__)\n\n        # L1 Cache: Local in-memory cache with TTL\n        self._local_cache: TTLCache = TTLCache(maxsize=LOCAL_CACHE_MAX_SIZE, ttl=LOCAL_CACHE_TTL)\n        self._cache_stats = {\n            \"l1_hits\": 0,\n            \"l1_misses\": 0,\n            \"l2_hits\": 0,\n            \"l2_misses\": 0,\n            \"l3_hits\": 0,\n            \"l3_misses\": 0,\n        }\n\n        # Default prompts for fallback\n        self._default_prompts = self._load_default_prompts()\n\n    def _load_default_prompts(self) -> Dict[str, str]:\n        \"\"\"Load default prompts for fallback when dynamic lookup fails.\"\"\"\n        return {\n            \"intent:diagnosis\": (\n                \"You are a medical AI assistant specializing in clinical diagnosis. \"\n                \"Provide evidence-based diagnostic insights with appropriate citations.\"\n            ),\n            \"intent:treatment\": (\n                \"You are a medical AI assistant specializing in treatment planning. \"\n                \"Provide evidence-based treatment recommendations with appropriate citations.\"\n            ),\n            \"intent:drug\": (\n                \"You are a medical AI assistant specializing in pharmacology. \"\n                \"Provide evidence-based drug information with appropriate citations.\"\n            ),\n            \"intent:guideline\": (\n                \"You are a medical AI assistant specializing in clinical guidelines. \"\n                \"Provide evidence-based guideline information with appropriate citations.\"\n            ),\n            \"intent:summary\": (\n                \"You are a medical AI assistant specializing in medical summarization. \"\n                \"Provide clear, concise summaries with appropriate citations.\"\n            ),\n            \"intent:other\": (\n                \"You are a helpful medical AI assistant. \"\n                \"Provide accurate, evidence-based information with appropriate citations.\"\n            ),\n            \"voice:default\": \"\"\"You are VoiceAssist, a helpful AI assistant in voice conversation mode.\n\nCRITICAL SPEAKING GUIDELINES (Every response will be read aloud):\n1. SHORT SENTENCES ONLY - Maximum 15-20 words per sentence\n2. NO ABBREVIATIONS - Say \"blood pressure\" not \"BP\", \"heart rate\" not \"HR\"\n3. NO ACRONYMS WITHOUT EXPANSION - Say \"electrocardiogram or ECG\" first time\n4. AVOID LISTS - Convert bullet points to flowing narrative\n5. NO SPECIAL CHARACTERS - Don't use asterisks, hyphens, or formatting\n6. NATURAL PAUSES - Use commas and periods to create breathing room\n7. CONVERSATIONAL CONTRACTIONS - Use \"I'm\", \"you're\", \"it's\" naturally\n8. ACKNOWLEDGE FIRST - Start with brief acknowledgment before answering\n\nRESPONSE STRUCTURE:\n- Start with a brief acknowledgment (1-2 words: \"Sure.\", \"Got it.\", \"Okay.\")\n- Give the core answer in 2-3 short sentences\n- Offer to elaborate if complex (\"Would you like more details on that?\")\n\nEXAMPLE - BAD (written style):\n\"HTN management includes: 1) lifestyle modifications 2) pharmacotherapy with ACE-I or ARBs 3) regular BP monitoring...\"\n\nEXAMPLE - GOOD (spoken style):\n\"High blood pressure is managed in a few ways. First, lifestyle changes like diet and exercise.\nThen medications if needed. Do you want more details?\"\n\nRemember: You're SPEAKING, not writing. Keep it brief and natural.\"\"\",\n        }\n\n    # ==================== Cache Operations ====================\n\n    def _get_cache_key(self, key_type: str, identifier: str) -> str:\n        \"\"\"Generate Redis cache key.\"\"\"\n        prefix_map = {\n            \"id\": PROMPT_CACHE_PREFIX,\n            \"name\": PROMPT_NAME_CACHE_PREFIX,\n            \"intent\": PROMPT_INTENT_CACHE_PREFIX,\n        }\n        return f\"{prefix_map.get(key_type, PROMPT_CACHE_PREFIX)}{identifier}\"\n\n    async def _get_from_local_cache(self, cache_key: str) -> Optional[Dict[str, Any]]:\n        \"\"\"Get from L1 (local in-memory) cache.\"\"\"\n        try:\n            if cache_key in self._local_cache:\n                self._cache_stats[\"l1_hits\"] += 1\n                self.logger.debug(f\"L1 cache hit: {cache_key}\")\n                return self._local_cache[cache_key]\n            self._cache_stats[\"l1_misses\"] += 1\n            return None\n        except Exception as e:\n            self.logger.warning(f\"L1 cache error: {e}\")\n            return None\n\n    async def _set_local_cache(self, cache_key: str, data: Dict[str, Any]) -> None:\n        \"\"\"Set in L1 (local in-memory) cache.\"\"\"\n        try:\n            self._local_cache[cache_key] = data\n            self.logger.debug(f\"Set L1 cache: {cache_key}\")\n        except Exception as e:\n            self.logger.warning(f\"L1 cache set error: {e}\")\n\n    async def _invalidate_local_cache(self, cache_key: str) -> None:\n        \"\"\"Invalidate L1 cache entry.\"\"\"\n        try:\n            if cache_key in self._local_cache:\n                del self._local_cache[cache_key]\n                self.logger.debug(f\"Invalidated L1 cache: {cache_key}\")\n        except Exception as e:\n            self.logger.warning(f\"L1 cache invalidate error: {e}\")\n\n    async def _get_from_redis_cache(self, cache_key: str) -> Optional[Dict[str, Any]]:\n        \"\"\"Get from L2 (Redis) cache.\"\"\"\n        try:\n            cached_value = redis_client.get(cache_key)\n            if cached_value:\n                self._cache_stats[\"l2_hits\"] += 1\n                self.logger.debug(f\"L2 cache hit: {cache_key}\")\n                return json.loads(cached_value)\n            self._cache_stats[\"l2_misses\"] += 1\n            return None\n        except Exception as e:\n            self.logger.warning(f\"L2 cache error: {e}\")\n            return None\n\n    async def _set_cache(self, cache_key: str, data: Dict[str, Any]) -> None:\n        \"\"\"Set in both L1 and L2 caches.\"\"\"\n        # Set in L1\n        await self._set_local_cache(cache_key, data)\n\n        # Set in L2 (Redis)\n        try:\n            redis_client.setex(cache_key, PROMPT_CACHE_TTL, json.dumps(data))\n            self.logger.debug(f\"Set L2 cache: {cache_key}\")\n        except Exception as e:\n            self.logger.warning(f\"L2 cache set error: {e}\")\n\n    async def _invalidate_cache(self, prompt_id: UUID, name: str) -> None:\n        \"\"\"Invalidate all cache entries for a prompt.\"\"\"\n        cache_keys = [\n            self._get_cache_key(\"id\", str(prompt_id)),\n            self._get_cache_key(\"name\", name),\n        ]\n\n        # Also invalidate intent cache if applicable\n        if name.startswith(\"intent:\"):\n            intent = name.split(\":\", 1)[1] if \":\" in name else name\n            cache_keys.append(self._get_cache_key(\"intent\", f\"chat:{intent}\"))\n        elif name.startswith(\"voice:\"):\n            cache_keys.append(self._get_cache_key(\"intent\", \"voice:default\"))\n\n        for cache_key in cache_keys:\n            await self._invalidate_local_cache(cache_key)\n            try:\n                redis_client.delete(cache_key)\n            except Exception as e:\n                self.logger.warning(f\"L2 cache invalidate error for {cache_key}: {e}\")\n\n    async def _publish_update(self, prompt_id: UUID, name: str, action: str, version: int = None) -> None:\n        \"\"\"Publish prompt update event via Redis Pub/Sub.\"\"\"\n        try:\n            message = json.dumps(\n                {\n                    \"event\": action,\n                    \"prompt_id\": str(prompt_id),\n                    \"prompt_name\": name,\n                    \"version\": version,\n                    \"timestamp\": datetime.now(timezone.utc).isoformat(),\n                }\n            )\n            redis_client.publish(PROMPT_UPDATES_CHANNEL, message)\n            self.logger.info(f\"Published prompt update: {action} for {name}\")\n        except Exception as e:\n            self.logger.warning(f\"Failed to publish update: {e}\")\n\n    # ==================== Runtime Lookups ====================\n\n    async def get_system_prompt_for_intent(\n        self, intent: str, prompt_type: str = \"chat\", db: Optional[Session] = None\n    ) -> str:\n        \"\"\"Get the active system prompt for a given intent.\n\n        This is the primary method used by llm_client.py for chat prompts.\n\n        Args:\n            intent: The intent category (diagnosis, treatment, etc.)\n            prompt_type: The prompt type (chat, voice)\n            db: Optional database session\n\n        Returns:\n            The active system prompt text, or fallback default\n        \"\"\"\n        prompt_name = f\"intent:{intent}\"\n        cache_key = self._get_cache_key(\"intent\", f\"{prompt_type}:{intent}\")\n\n        # L1 Cache lookup\n        cached = await self._get_from_local_cache(cache_key)\n        if cached:\n            return cached.get(\"published_content\") or cached.get(\"system_prompt\", \"\")\n\n        # L2 Cache lookup\n        cached = await self._get_from_redis_cache(cache_key)\n        if cached:\n            await self._set_local_cache(cache_key, cached)\n            return cached.get(\"published_content\") or cached.get(\"system_prompt\", \"\")\n\n        # L3 Database lookup\n        should_close_db = False\n        if db is None:\n            db = SessionLocal()\n            should_close_db = True\n\n        try:\n            prompt = (\n                db.query(Prompt)\n                .filter(\n                    and_(\n                        Prompt.name == prompt_name,\n                        Prompt.status == PromptStatus.PUBLISHED.value,\n                        Prompt.is_active is True,\n                    )\n                )\n                .first()\n            )\n\n            if prompt:\n                self._cache_stats[\"l3_hits\"] += 1\n                prompt_data = prompt.to_dict()\n                await self._set_cache(cache_key, prompt_data)\n                return prompt.get_active_content()\n            else:\n                self._cache_stats[\"l3_misses\"] += 1\n                # Return fallback default\n                return self._default_prompts.get(prompt_name, self._default_prompts.get(\"intent:other\", \"\"))\n\n        except Exception as e:\n            self.logger.error(f\"Failed to get prompt for intent '{intent}': {e}\")\n            return self._default_prompts.get(prompt_name, self._default_prompts.get(\"intent:other\", \"\"))\n        finally:\n            if should_close_db:\n                db.close()\n\n    async def get_voice_instructions(\n        self,\n        persona: Optional[str] = None,\n        conversation_id: Optional[str] = None,\n        db: Optional[Session] = None,\n    ) -> str:\n        \"\"\"Get voice mode system instructions.\n\n        This is the primary method used by realtime_voice_service.py.\n\n        Args:\n            persona: Optional persona name to use\n            conversation_id: Optional conversation ID for context\n            db: Optional database session\n\n        Returns:\n            The voice instructions text\n        \"\"\"\n        prompt_name = f\"persona:{persona}\" if persona else \"voice:default\"\n        cache_key = self._get_cache_key(\"name\", prompt_name)\n\n        # L1 Cache lookup\n        cached = await self._get_from_local_cache(cache_key)\n        if cached:\n            instructions = cached.get(\"published_content\") or cached.get(\"system_prompt\", \"\")\n            if conversation_id:\n                instructions += f\"\\nResuming conversation: {conversation_id}\"\n            return instructions\n\n        # L2 Cache lookup\n        cached = await self._get_from_redis_cache(cache_key)\n        if cached:\n            await self._set_local_cache(cache_key, cached)\n            instructions = cached.get(\"published_content\") or cached.get(\"system_prompt\", \"\")\n            if conversation_id:\n                instructions += f\"\\nResuming conversation: {conversation_id}\"\n            return instructions\n\n        # L3 Database lookup\n        should_close_db = False\n        if db is None:\n            db = SessionLocal()\n            should_close_db = True\n\n        try:\n            prompt = (\n                db.query(Prompt)\n                .filter(\n                    and_(\n                        Prompt.name == prompt_name,\n                        Prompt.status == PromptStatus.PUBLISHED.value,\n                        Prompt.is_active is True,\n                    )\n                )\n                .first()\n            )\n\n            if prompt:\n                self._cache_stats[\"l3_hits\"] += 1\n                prompt_data = prompt.to_dict()\n                await self._set_cache(cache_key, prompt_data)\n                instructions = prompt.get_active_content()\n            else:\n                self._cache_stats[\"l3_misses\"] += 1\n                # Try voice:default fallback\n                if persona:\n                    return await self.get_voice_instructions(persona=None, conversation_id=conversation_id, db=db)\n                instructions = self._default_prompts.get(\"voice:default\", \"\")\n\n            if conversation_id:\n                instructions += f\"\\nResuming conversation: {conversation_id}\"\n            return instructions\n\n        except Exception as e:\n            self.logger.error(f\"Failed to get voice instructions: {e}\")\n            return self._default_prompts.get(\"voice:default\", \"\")\n        finally:\n            if should_close_db:\n                db.close()\n\n    async def get_rag_instructions(self, db: Optional[Session] = None) -> str:\n        \"\"\"Get the RAG orchestrator instructions.\n\n        This is used by rag_service.py to get the dynamic instructions\n        for the query orchestrator.\n\n        Returns:\n            The RAG instructions text, or fallback default\n        \"\"\"\n        prompt_name = \"system:rag_instructions\"\n        cache_key = self._get_cache_key(\"name\", prompt_name)\n\n        # L1 Cache lookup\n        cached = await self._get_from_local_cache(cache_key)\n        if cached:\n            return cached.get(\"published_content\") or cached.get(\"system_prompt\", \"\")\n\n        # L2 Cache lookup\n        cached = await self._get_from_redis_cache(cache_key)\n        if cached:\n            await self._set_local_cache(cache_key, cached)\n            return cached.get(\"published_content\") or cached.get(\"system_prompt\", \"\")\n\n        # L3 Database lookup\n        should_close_db = False\n        if db is None:\n            db = SessionLocal()\n            should_close_db = True\n\n        try:\n            prompt = (\n                db.query(Prompt)\n                .filter(\n                    and_(\n                        Prompt.name == prompt_name,\n                        Prompt.status == PromptStatus.PUBLISHED.value,\n                        Prompt.is_active is True,\n                    )\n                )\n                .first()\n            )\n\n            if prompt:\n                self._cache_stats[\"l3_hits\"] += 1\n                prompt_data = prompt.to_dict()\n                await self._set_cache(cache_key, prompt_data)\n                return prompt.get_active_content()\n            else:\n                self._cache_stats[\"l3_misses\"] += 1\n                # Return fallback default\n                return self._get_default_rag_instructions()\n\n        except Exception as e:\n            self.logger.error(f\"Failed to get RAG instructions: {e}\")\n            return self._get_default_rag_instructions()\n        finally:\n            if should_close_db:\n                db.close()\n\n    def _get_default_rag_instructions(self) -> str:\n        \"\"\"Get default RAG instructions for fallback.\"\"\"\n        return \"\"\"You are a knowledgeable and friendly medical assistant helping users understand health information.\n\nGuidelines:\n- Answer questions directly, then offer to explore further\n- Explain medical concepts in plain language\n- Use natural language - contractions are fine\n- Acknowledge when a topic is complex or when concerns are valid\n- If you don't know something, say so warmly\n\nImportant: Always recommend consulting a healthcare provider for specific medical advice.\"\"\"\n\n    async def get_prompt_with_settings(self, name: str, db: Optional[Session] = None) -> Optional[Dict[str, Any]]:\n        \"\"\"Get a prompt with its model settings (temperature, max_tokens).\n\n        Returns the full prompt data including temperature and max_tokens\n        for use by rag_service.py and llm_client.py.\n\n        Args:\n            name: The prompt name (e.g., \"intent:diagnosis\")\n            db: Optional database session\n\n        Returns:\n            Dict with prompt data including temperature, max_tokens, model_name\n        \"\"\"\n        cache_key = self._get_cache_key(\"name\", name)\n\n        # L1 Cache lookup\n        cached = await self._get_from_local_cache(cache_key)\n        if cached:\n            return cached\n\n        # L2 Cache lookup\n        cached = await self._get_from_redis_cache(cache_key)\n        if cached:\n            await self._set_local_cache(cache_key, cached)\n            return cached\n\n        # L3 Database lookup\n        should_close_db = False\n        if db is None:\n            db = SessionLocal()\n            should_close_db = True\n\n        try:\n            prompt = db.query(Prompt).filter(and_(Prompt.name == name, Prompt.is_active is True)).first()\n\n            if prompt:\n                self._cache_stats[\"l3_hits\"] += 1\n                prompt_data = prompt.to_dict()\n                await self._set_cache(cache_key, prompt_data)\n                return prompt_data\n            else:\n                self._cache_stats[\"l3_misses\"] += 1\n                return None\n\n        except Exception as e:\n            self.logger.error(f\"Failed to get prompt with settings '{name}': {e}\")\n            return None\n        finally:\n            if should_close_db:\n                db.close()\n\n    # ==================== CRUD Operations ====================\n\n    async def get_prompt(self, prompt_id: UUID, db: Session) -> Optional[Prompt]:\n        \"\"\"Get a prompt by ID.\"\"\"\n        try:\n            return (\n                db.query(Prompt)\n                .options(joinedload(Prompt.created_by), joinedload(Prompt.updated_by))\n                .filter(Prompt.id == prompt_id)\n                .first()\n            )\n        except Exception as e:\n            self.logger.error(f\"Failed to get prompt {prompt_id}: {e}\")\n            return None\n\n    async def get_prompt_by_name(self, name: str, db: Session) -> Optional[Prompt]:\n        \"\"\"Get a prompt by name.\"\"\"\n        try:\n            return (\n                db.query(Prompt)\n                .options(joinedload(Prompt.created_by), joinedload(Prompt.updated_by))\n                .filter(Prompt.name == name)\n                .first()\n            )\n        except Exception as e:\n            self.logger.error(f\"Failed to get prompt by name '{name}': {e}\")\n            return None\n\n    async def list_prompts(\n        self,\n        db: Session,\n        prompt_type: Optional[str] = None,\n        status: Optional[str] = None,\n        intent_category: Optional[str] = None,\n        is_active: Optional[bool] = None,\n        search: Optional[str] = None,\n        page: int = 1,\n        page_size: int = 20,\n        sort_by: str = \"updated_at\",\n        sort_order: str = \"desc\",\n    ) -> tuple[List[Prompt], int]:\n        \"\"\"List prompts with filtering and pagination.\"\"\"\n        try:\n            query = db.query(Prompt).options(joinedload(Prompt.created_by), joinedload(Prompt.updated_by))\n\n            # Apply filters\n            if prompt_type:\n                query = query.filter(Prompt.prompt_type == prompt_type)\n            if status:\n                query = query.filter(Prompt.status == status)\n            if intent_category:\n                query = query.filter(Prompt.intent_category == intent_category)\n            if is_active is not None:\n                query = query.filter(Prompt.is_active == is_active)\n            if search:\n                search_term = f\"%{search}%\"\n                query = query.filter(\n                    or_(\n                        Prompt.name.ilike(search_term),\n                        Prompt.display_name.ilike(search_term),\n                        Prompt.description.ilike(search_term),\n                    )\n                )\n\n            # Get total count\n            total = query.count()\n\n            # Apply sorting\n            sort_column = getattr(Prompt, sort_by, Prompt.updated_at)\n            if sort_order == \"desc\":\n                query = query.order_by(sort_column.desc())\n            else:\n                query = query.order_by(sort_column.asc())\n\n            # Apply pagination\n            offset = (page - 1) * page_size\n            prompts = query.offset(offset).limit(page_size).all()\n\n            return prompts, total\n\n        except Exception as e:\n            self.logger.error(f\"Failed to list prompts: {e}\")\n            return [], 0\n\n    async def create_prompt(\n        self,\n        name: str,\n        display_name: str,\n        system_prompt: str,\n        prompt_type: str = \"chat\",\n        description: Optional[str] = None,\n        intent_category: Optional[str] = None,\n        temperature: Optional[float] = None,\n        max_tokens: Optional[int] = None,\n        model_name: Optional[str] = None,\n        metadata: Optional[Dict[str, Any]] = None,\n        actor: Optional[User] = None,\n        db: Session = None,\n    ) -> Optional[Prompt]:\n        \"\"\"Create a new prompt.\"\"\"\n        should_close_db = False\n        if db is None:\n            db = SessionLocal()\n            should_close_db = True\n\n        try:\n            prompt = Prompt(\n                name=name,\n                display_name=display_name,\n                description=description,\n                prompt_type=prompt_type,\n                intent_category=intent_category,\n                system_prompt=system_prompt,\n                temperature=temperature,\n                max_tokens=max_tokens,\n                model_name=model_name,\n                status=PromptStatus.DRAFT.value,\n                is_active=True,\n                current_version=1,\n                prompt_metadata=metadata,\n                created_by_id=actor.id if actor else None,\n                updated_by_id=actor.id if actor else None,\n            )\n\n            db.add(prompt)\n            db.commit()\n            db.refresh(prompt)\n\n            # Create initial version\n            version = PromptVersion(\n                prompt_id=prompt.id,\n                version_number=1,\n                system_prompt=system_prompt,\n                prompt_type=prompt_type,\n                intent_category=intent_category,\n                version_metadata=metadata,\n                change_summary=\"Initial version\",\n                changed_by_id=actor.id if actor else None,\n                changed_by_email=actor.email if actor else None,\n                status=PromptStatus.DRAFT.value,\n            )\n            db.add(version)\n            db.commit()\n\n            self.logger.info(f\"Created prompt: {name}\")\n            return prompt\n\n        except IntegrityError:\n            db.rollback()\n            self.logger.warning(f\"Prompt already exists: {name}\")\n            return None\n        except Exception as e:\n            db.rollback()\n            self.logger.error(f\"Failed to create prompt: {e}\")\n            return None\n        finally:\n            if should_close_db:\n                db.close()\n\n    async def update_prompt(\n        self,\n        prompt_id: UUID,\n        display_name: Optional[str] = None,\n        description: Optional[str] = None,\n        system_prompt: Optional[str] = None,\n        intent_category: Optional[str] = None,\n        temperature: Optional[float] = None,\n        max_tokens: Optional[int] = None,\n        model_name: Optional[str] = None,\n        metadata: Optional[Dict[str, Any]] = None,\n        is_active: Optional[bool] = None,\n        change_summary: Optional[str] = None,\n        actor: Optional[User] = None,\n        db: Session = None,\n    ) -> Optional[Prompt]:\n        \"\"\"Update a prompt (creates new version if content changes).\"\"\"\n        should_close_db = False\n        if db is None:\n            db = SessionLocal()\n            should_close_db = True\n\n        try:\n            prompt = await self.get_prompt(prompt_id, db)\n            if not prompt:\n                self.logger.warning(f\"Prompt not found: {prompt_id}\")\n                return None\n\n            content_changed = False\n\n            # Update fields\n            if display_name is not None:\n                prompt.display_name = display_name\n            if description is not None:\n                prompt.description = description\n            if system_prompt is not None and system_prompt != prompt.system_prompt:\n                prompt.system_prompt = system_prompt\n                content_changed = True\n            if intent_category is not None:\n                prompt.intent_category = intent_category\n            if temperature is not None:\n                prompt.temperature = temperature\n            if max_tokens is not None:\n                prompt.max_tokens = max_tokens\n            if model_name is not None:\n                prompt.model_name = model_name\n            if metadata is not None:\n                prompt.prompt_metadata = metadata\n            if is_active is not None:\n                prompt.is_active = is_active\n\n            prompt.updated_at = datetime.now(timezone.utc)\n            prompt.updated_by_id = actor.id if actor else None\n\n            # Create new version if content changed\n            if content_changed:\n                prompt.current_version += 1\n                version = PromptVersion(\n                    prompt_id=prompt.id,\n                    version_number=prompt.current_version,\n                    system_prompt=system_prompt,\n                    prompt_type=prompt.prompt_type,\n                    intent_category=prompt.intent_category,\n                    version_metadata=prompt.prompt_metadata,\n                    change_summary=change_summary or \"Content updated\",\n                    changed_by_id=actor.id if actor else None,\n                    changed_by_email=actor.email if actor else None,\n                    status=prompt.status,\n                )\n                db.add(version)\n\n            db.commit()\n            db.refresh(prompt)\n\n            # Invalidate cache\n            await self._invalidate_cache(prompt.id, prompt.name)\n\n            # Publish update event\n            await self._publish_update(prompt.id, prompt.name, \"prompt_updated\", prompt.current_version)\n\n            self.logger.info(f\"Updated prompt: {prompt.name}\")\n            return prompt\n\n        except Exception as e:\n            db.rollback()\n            self.logger.error(f\"Failed to update prompt: {e}\")\n            return None\n        finally:\n            if should_close_db:\n                db.close()\n\n    async def publish_prompt(\n        self,\n        prompt_id: UUID,\n        change_summary: Optional[str] = None,\n        actor: Optional[User] = None,\n        db: Session = None,\n    ) -> Optional[Prompt]:\n        \"\"\"Publish a prompt (make draft content live).\"\"\"\n        should_close_db = False\n        if db is None:\n            db = SessionLocal()\n            should_close_db = True\n\n        try:\n            prompt = await self.get_prompt(prompt_id, db)\n            if not prompt:\n                self.logger.warning(f\"Prompt not found: {prompt_id}\")\n                return None\n\n            # Copy draft to published\n            prompt.published_content = prompt.system_prompt\n            prompt.status = PromptStatus.PUBLISHED.value\n            prompt.published_at = datetime.now(timezone.utc)\n            prompt.updated_at = datetime.now(timezone.utc)\n            prompt.updated_by_id = actor.id if actor else None\n\n            # Update or create version with published status\n            latest_version = (\n                db.query(PromptVersion)\n                .filter(\n                    and_(\n                        PromptVersion.prompt_id == prompt.id,\n                        PromptVersion.version_number == prompt.current_version,\n                    )\n                )\n                .first()\n            )\n\n            if latest_version:\n                latest_version.status = PromptStatus.PUBLISHED.value\n                if change_summary:\n                    latest_version.change_summary = change_summary\n            else:\n                # Create new published version\n                prompt.current_version += 1\n                version = PromptVersion(\n                    prompt_id=prompt.id,\n                    version_number=prompt.current_version,\n                    system_prompt=prompt.system_prompt,\n                    prompt_type=prompt.prompt_type,\n                    intent_category=prompt.intent_category,\n                    version_metadata=prompt.prompt_metadata,\n                    change_summary=change_summary or \"Published\",\n                    changed_by_id=actor.id if actor else None,\n                    changed_by_email=actor.email if actor else None,\n                    status=PromptStatus.PUBLISHED.value,\n                )\n                db.add(version)\n\n            db.commit()\n            db.refresh(prompt)\n\n            # Invalidate and update cache\n            await self._invalidate_cache(prompt.id, prompt.name)\n            await self._set_cache(self._get_cache_key(\"name\", prompt.name), prompt.to_dict())\n\n            # Publish update event\n            await self._publish_update(prompt.id, prompt.name, \"prompt_published\", prompt.current_version)\n\n            self.logger.info(f\"Published prompt: {prompt.name}\")\n            return prompt\n\n        except Exception as e:\n            db.rollback()\n            self.logger.error(f\"Failed to publish prompt: {e}\")\n            return None\n        finally:\n            if should_close_db:\n                db.close()\n\n    async def rollback_to_version(\n        self,\n        prompt_id: UUID,\n        version_number: int,\n        reason: Optional[str] = None,\n        actor: Optional[User] = None,\n        db: Session = None,\n    ) -> Optional[Prompt]:\n        \"\"\"Rollback prompt to a previous version.\"\"\"\n        should_close_db = False\n        if db is None:\n            db = SessionLocal()\n            should_close_db = True\n\n        try:\n            prompt = await self.get_prompt(prompt_id, db)\n            if not prompt:\n                self.logger.warning(f\"Prompt not found: {prompt_id}\")\n                return None\n\n            # Get the target version\n            target_version = (\n                db.query(PromptVersion)\n                .filter(\n                    and_(\n                        PromptVersion.prompt_id == prompt_id,\n                        PromptVersion.version_number == version_number,\n                    )\n                )\n                .first()\n            )\n\n            if not target_version:\n                self.logger.warning(f\"Version {version_number} not found for prompt {prompt_id}\")\n                return None\n\n            # Create new version with rollback content\n            prompt.current_version += 1\n            prompt.system_prompt = target_version.system_prompt\n            prompt.published_content = target_version.system_prompt\n            prompt.prompt_metadata = target_version.version_metadata\n            prompt.updated_at = datetime.now(timezone.utc)\n            prompt.updated_by_id = actor.id if actor else None\n\n            rollback_version = PromptVersion(\n                prompt_id=prompt.id,\n                version_number=prompt.current_version,\n                system_prompt=target_version.system_prompt,\n                prompt_type=prompt.prompt_type,\n                intent_category=prompt.intent_category,\n                version_metadata=target_version.version_metadata,\n                change_summary=reason or f\"Rolled back to version {version_number}\",\n                changed_by_id=actor.id if actor else None,\n                changed_by_email=actor.email if actor else None,\n                status=prompt.status,\n            )\n            db.add(rollback_version)\n\n            db.commit()\n            db.refresh(prompt)\n\n            # Invalidate cache\n            await self._invalidate_cache(prompt.id, prompt.name)\n\n            # Publish update event\n            await self._publish_update(prompt.id, prompt.name, \"prompt_rolled_back\", prompt.current_version)\n\n            self.logger.info(f\"Rolled back prompt {prompt.name} to version {version_number}\")\n            return prompt\n\n        except Exception as e:\n            db.rollback()\n            self.logger.error(f\"Failed to rollback prompt: {e}\")\n            return None\n        finally:\n            if should_close_db:\n                db.close()\n\n    async def delete_prompt(self, prompt_id: UUID, actor: Optional[User] = None, db: Session = None) -> bool:\n        \"\"\"Archive (soft delete) a prompt.\"\"\"\n        should_close_db = False\n        if db is None:\n            db = SessionLocal()\n            should_close_db = True\n\n        try:\n            prompt = await self.get_prompt(prompt_id, db)\n            if not prompt:\n                self.logger.warning(f\"Prompt not found: {prompt_id}\")\n                return False\n\n            prompt.status = PromptStatus.ARCHIVED.value\n            prompt.is_active = False\n            prompt.updated_at = datetime.now(timezone.utc)\n            prompt.updated_by_id = actor.id if actor else None\n\n            db.commit()\n\n            # Invalidate cache\n            await self._invalidate_cache(prompt.id, prompt.name)\n\n            # Publish update event\n            await self._publish_update(prompt.id, prompt.name, \"prompt_deleted\")\n\n            self.logger.info(f\"Archived prompt: {prompt.name}\")\n            return True\n\n        except Exception as e:\n            db.rollback()\n            self.logger.error(f\"Failed to delete prompt: {e}\")\n            return False\n        finally:\n            if should_close_db:\n                db.close()\n\n    async def archive_prompt(self, prompt_id: UUID, actor: Optional[User] = None, db: Session = None) -> bool:\n        \"\"\"Archive a prompt (sets status to archived).\n\n        This is an explicit archive operation that preserves the prompt\n        for reference but removes it from active use.\n        \"\"\"\n        should_close_db = False\n        if db is None:\n            db = SessionLocal()\n            should_close_db = True\n\n        try:\n            prompt = await self.get_prompt(prompt_id, db)\n            if not prompt:\n                self.logger.warning(f\"Prompt not found: {prompt_id}\")\n                return False\n\n            prompt.status = PromptStatus.ARCHIVED.value\n            prompt.is_active = False\n            prompt.updated_at = datetime.now(timezone.utc)\n            prompt.updated_by_id = actor.id if actor else None\n\n            db.commit()\n\n            # Invalidate cache\n            await self._invalidate_cache(prompt.id, prompt.name)\n\n            # Publish update event\n            await self._publish_update(prompt.id, prompt.name, \"prompt_archived\")\n\n            self.logger.info(f\"Archived prompt: {prompt.name}\")\n            return True\n\n        except Exception as e:\n            db.rollback()\n            self.logger.error(f\"Failed to archive prompt: {e}\")\n            return False\n        finally:\n            if should_close_db:\n                db.close()\n\n    async def get_versions(self, prompt_id: UUID, db: Session) -> List[PromptVersion]:\n        \"\"\"Get all versions for a prompt.\"\"\"\n        try:\n            return (\n                db.query(PromptVersion)\n                .filter(PromptVersion.prompt_id == prompt_id)\n                .order_by(PromptVersion.version_number.desc())\n                .all()\n            )\n        except Exception as e:\n            self.logger.error(f\"Failed to get versions for prompt {prompt_id}: {e}\")\n            return []\n\n    async def get_version(self, prompt_id: UUID, version_number: int, db: Session) -> Optional[PromptVersion]:\n        \"\"\"Get a specific version.\"\"\"\n        try:\n            return (\n                db.query(PromptVersion)\n                .filter(\n                    and_(\n                        PromptVersion.prompt_id == prompt_id,\n                        PromptVersion.version_number == version_number,\n                    )\n                )\n                .first()\n            )\n        except Exception as e:\n            self.logger.error(f\"Failed to get version {version_number}: {e}\")\n            return None\n\n    async def duplicate_prompt(\n        self,\n        prompt_id: UUID,\n        new_name: str,\n        new_display_name: Optional[str] = None,\n        actor: Optional[User] = None,\n        db: Session = None,\n    ) -> Optional[Prompt]:\n        \"\"\"Duplicate a prompt with a new name.\"\"\"\n        should_close_db = False\n        if db is None:\n            db = SessionLocal()\n            should_close_db = True\n\n        try:\n            original = await self.get_prompt(prompt_id, db)\n            if not original:\n                self.logger.warning(f\"Original prompt not found: {prompt_id}\")\n                return None\n\n            return await self.create_prompt(\n                name=new_name,\n                display_name=new_display_name or f\"{original.display_name} (Copy)\",\n                description=original.description,\n                prompt_type=original.prompt_type,\n                intent_category=original.intent_category,\n                system_prompt=original.system_prompt,\n                temperature=original.temperature,\n                max_tokens=original.max_tokens,\n                model_name=original.model_name,\n                metadata=original.prompt_metadata,\n                actor=actor,\n                db=db,\n            )\n\n        except Exception as e:\n            self.logger.error(f\"Failed to duplicate prompt: {e}\")\n            return None\n        finally:\n            if should_close_db:\n                db.close()\n\n    # ==================== Cache Management ====================\n\n    async def warm_cache(self, db: Optional[Session] = None) -> int:\n        \"\"\"Warm cache with all published prompts on startup.\"\"\"\n        should_close_db = False\n        if db is None:\n            db = SessionLocal()\n            should_close_db = True\n\n        try:\n            prompts = (\n                db.query(Prompt)\n                .filter(\n                    and_(\n                        Prompt.status == PromptStatus.PUBLISHED.value,\n                        Prompt.is_active is True,\n                    )\n                )\n                .all()\n            )\n\n            for prompt in prompts:\n                prompt_data = prompt.to_dict()\n                await self._set_cache(self._get_cache_key(\"name\", prompt.name), prompt_data)\n                await self._set_cache(self._get_cache_key(\"id\", str(prompt.id)), prompt_data)\n\n                # Also cache by intent for quick lookup\n                if prompt.intent_category:\n                    await self._set_cache(\n                        self._get_cache_key(\"intent\", f\"{prompt.prompt_type}:{prompt.intent_category}\"),\n                        prompt_data,\n                    )\n\n            self.logger.info(f\"Prompt cache warmed: {len(prompts)} prompts cached\")\n            return len(prompts)\n\n        except Exception as e:\n            self.logger.error(f\"Failed to warm prompt cache: {e}\")\n            return 0\n        finally:\n            if should_close_db:\n                db.close()\n\n    def get_cache_stats(self) -> Dict[str, Any]:\n        \"\"\"Get cache statistics for monitoring.\"\"\"\n        total_l1 = self._cache_stats[\"l1_hits\"] + self._cache_stats[\"l1_misses\"]\n        total_l2 = self._cache_stats[\"l2_hits\"] + self._cache_stats[\"l2_misses\"]\n\n        return {\n            \"l1_cache\": {\n                \"hits\": self._cache_stats[\"l1_hits\"],\n                \"misses\": self._cache_stats[\"l1_misses\"],\n                \"hit_rate\": ((self._cache_stats[\"l1_hits\"] / total_l1 * 100) if total_l1 > 0 else 0),\n                \"size\": len(self._local_cache),\n                \"max_size\": LOCAL_CACHE_MAX_SIZE,\n                \"ttl_seconds\": LOCAL_CACHE_TTL,\n            },\n            \"l2_cache\": {\n                \"hits\": self._cache_stats[\"l2_hits\"],\n                \"misses\": self._cache_stats[\"l2_misses\"],\n                \"hit_rate\": ((self._cache_stats[\"l2_hits\"] / total_l2 * 100) if total_l2 > 0 else 0),\n                \"ttl_seconds\": PROMPT_CACHE_TTL,\n            },\n            \"l3_database\": {\n                \"hits\": self._cache_stats[\"l3_hits\"],\n                \"misses\": self._cache_stats[\"l3_misses\"],\n            },\n        }\n\n\n# Global singleton instance\nprompt_service = PromptService()\n"
}
