{
  "path": "services/api-gateway/app/api/voice.py",
  "language": "python",
  "size": 66722,
  "last_modified": "2025-12-04T11:26:50.900Z",
  "lines": 1904,
  "content": "\"\"\"\nVoice API endpoints\nHandles audio transcription, speech synthesis, and Realtime API sessions\n\nProviders:\n- OpenAI Whisper/TTS (default)\n- OpenAI Realtime API (WebSocket-based voice mode)\n- Thinker/Talker Pipeline (Deepgram + GPT-4o + ElevenLabs)\n- Stubs for future providers (Azure/GCP) using config\n\nNote: Pydantic schemas are now defined in app/api/voice/schemas.py\n\"\"\"\n\nimport asyncio\nimport time\nimport uuid\n\nimport httpx\nfrom app.api.voice_schemas.schemas import (\n    RealtimeSessionRequest,\n    RealtimeSessionResponse,\n    SynthesizeRequest,\n    TranscribeResponse,\n    VADProfileResponse,\n    VADSessionMetrics,\n    VoiceAuthCompleteResponse,\n    VoiceAuthSampleResponse,\n    VoiceAuthStartResponse,\n    VoiceAuthStatusResponse,\n    VoiceAuthVerifyResponse,\n    VoiceInfo,\n    VoiceListResponse,\n    VoiceMetricsPayload,\n    VoiceMetricsResponse,\n    VoicePreferencesRequest,\n    VoicePreferencesResponse,\n    VoiceRelayRequest,\n    VoiceRelayResponse,\n    VoiceStylePresetsListResponse,\n)\nfrom app.core.config import settings\nfrom app.core.database import get_db\nfrom app.core.dependencies import get_current_user\nfrom app.core.logging import get_logger\nfrom app.core.metrics import (\n    external_api_duration_seconds,\n    external_api_requests_total,\n    voice_connection_time_seconds,\n    voice_first_audio_latency_seconds,\n    voice_proxy_ttfb_seconds,\n    voice_reconnects_total,\n    voice_relay_latency_seconds,\n    voice_response_latency_seconds,\n    voice_session_duration_seconds,\n    voice_sessions_total,\n    voice_slo_violations_total,\n    voice_stt_latency_seconds,\n    voice_transcripts_total,\n)\nfrom app.core.middleware import rate_limit\nfrom app.core.security import verify_token\nfrom app.core.security import verify_token as verify_token_func\nfrom app.core.sentry import capture_slo_violation\nfrom app.core.slo import check_slo_violations, log_slo_violations\nfrom app.models.message import Message\nfrom app.models.session import Session as ChatSession\nfrom app.models.user import User\nfrom app.models.user_voice_preferences import UserVoicePreferences\nfrom app.services.elevenlabs_service import elevenlabs_service\nfrom app.services.rag_service import QueryOrchestrator, QueryRequest\nfrom app.services.realtime_voice_service import adaptive_vad_manager, realtime_voice_service\nfrom app.services.thinker_talker_websocket_handler import TTSessionConfig, thinker_talker_session_manager\nfrom app.services.voice_authentication import voice_auth_service\nfrom app.services.voice_pipeline_service import voice_pipeline_service\nfrom app.services.voice_style_detector import voice_style_detector\nfrom fastapi import APIRouter, Depends, File, HTTPException, Request, UploadFile, WebSocket, WebSocketDisconnect, status\nfrom fastapi.responses import Response, StreamingResponse\nfrom sqlalchemy.orm import Session\n\nlogger = get_logger(__name__)\n\nrouter = APIRouter(prefix=\"/voice\", tags=[\"voice\"])\n\n# Shared orchestrator instance for voice relay\nvoice_query_orchestrator = QueryOrchestrator()\n\n\n@router.post(\n    \"/transcribe\",\n    response_model=TranscribeResponse,\n    summary=\"Transcribe audio to text\",\n    description=\"Convert audio file to text using OpenAI Whisper API\",\n)\nasync def transcribe_audio(\n    audio: UploadFile = File(..., description=\"Audio file to transcribe\"),\n    current_user: User = Depends(get_current_user),\n):\n    \"\"\"\n    Transcribe audio to text using OpenAI Whisper.\n\n    Args:\n        audio: Audio file (supported formats: mp3, mp4, mpeg, mpga, m4a, wav, webm)\n        current_user: Authenticated user\n\n    Returns:\n        TranscribeResponse with transcribed text\n    \"\"\"\n    logger.info(\n        f\"Transcribing audio for user {current_user.id}\",\n        extra={\"user_id\": current_user.id, \"filename\": audio.filename},\n    )\n\n    # Validate file\n    if not audio.content_type or not audio.content_type.startswith(\"audio/\"):\n        raise HTTPException(\n            status_code=status.HTTP_400_BAD_REQUEST,\n            detail=\"File must be an audio file\",\n        )\n\n    # Check file size (max 25MB for Whisper API)\n    contents = await audio.read()\n    if len(contents) > 25 * 1024 * 1024:\n        raise HTTPException(\n            status_code=status.HTTP_400_BAD_REQUEST,\n            detail=\"Audio file too large (max 25MB)\",\n        )\n\n    try:\n        # Use OpenAI Whisper API for transcription\n        openai_api_key = settings.OPENAI_API_KEY\n        if not openai_api_key:\n            raise HTTPException(\n                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n                detail=\"OpenAI API key not configured\",\n            )\n\n        # Prepare file for OpenAI API\n        files = {\n            \"file\": (audio.filename or \"audio.webm\", contents, audio.content_type),\n            \"model\": (None, \"whisper-1\"),\n        }\n\n        async with httpx.AsyncClient(timeout=settings.OPENAI_TIMEOUT_SEC) as client:\n            response = await client.post(\n                \"https://api.openai.com/v1/audio/transcriptions\",\n                headers={\"Authorization\": f\"Bearer {openai_api_key}\"},\n                files=files,\n            )\n\n            if response.status_code != 200:\n                logger.error(\n                    f\"OpenAI transcription failed: {response.text}\",\n                    extra={\n                        \"status_code\": response.status_code,\n                        \"user_id\": current_user.id,\n                    },\n                )\n                raise HTTPException(\n                    status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n                    detail=\"Transcription failed\",\n                )\n\n            result = response.json()\n            transcribed_text = result.get(\"text\", \"\")\n\n            logger.info(\n                f\"Transcription successful for user {current_user.id}\",\n                extra={\n                    \"user_id\": current_user.id,\n                    \"text_length\": len(transcribed_text),\n                },\n            )\n\n            return TranscribeResponse(text=transcribed_text)\n\n    except httpx.TimeoutException:\n        logger.error(\n            \"Transcription timeout\",\n            extra={\"user_id\": current_user.id},\n        )\n        raise HTTPException(\n            status_code=status.HTTP_504_GATEWAY_TIMEOUT,\n            detail=\"Transcription request timed out\",\n        )\n    except Exception as e:\n        logger.error(\n            f\"Transcription error: {str(e)}\",\n            extra={\"user_id\": current_user.id, \"error\": str(e)},\n        )\n        raise HTTPException(\n            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n            detail=f\"Transcription failed: {str(e)}\",\n        )\n\n\n@router.post(\n    \"/synthesize\",\n    summary=\"Synthesize speech from text\",\n    description=\"Convert text to speech using OpenAI or ElevenLabs TTS API\",\n    response_class=Response,  # Return raw audio\n)\nasync def synthesize_speech(\n    request: SynthesizeRequest,\n    current_user: User = Depends(get_current_user),\n    db: Session = Depends(get_db),\n):\n    \"\"\"\n    Synthesize speech from text using OpenAI or ElevenLabs TTS.\n\n    Phase 11: Added provider selection with automatic fallback.\n    Voice Mode Overhaul: Added user preferences and context-aware style detection.\n    - provider: \"openai\" | \"elevenlabs\" | None (uses user preference or admin default)\n    - Automatic fallback to OpenAI if ElevenLabs fails\n    - Context-aware style detection adjusts TTS parameters based on content\n    - X-TTS-Provider response header indicates which provider was used\n    - X-TTS-Style response header indicates detected style (if context-aware enabled)\n\n    Args:\n        request: SynthesizeRequest with text, voiceId, and optional provider settings\n        current_user: Authenticated user\n        db: Database session\n\n    Returns:\n        Audio file (mp3 format) with X-TTS-Provider and X-TTS-Style headers\n    \"\"\"\n    # Load user's voice preferences\n    user_prefs = db.query(UserVoicePreferences).filter(UserVoicePreferences.user_id == current_user.id).first()\n\n    # Use defaults if no preferences exist\n    if not user_prefs:\n        user_prefs = UserVoicePreferences.get_default_preferences(current_user.id)\n\n    # Determine effective settings: request > user_prefs > system defaults\n    effective_provider = request.provider or user_prefs.tts_provider or settings.TTS_PROVIDER or \"openai\"\n    effective_voice = (\n        request.voiceId\n        or (user_prefs.elevenlabs_voice_id if effective_provider == \"elevenlabs\" else user_prefs.openai_voice_id)\n        or settings.TTS_VOICE\n        or \"alloy\"\n    )\n    effective_stability = request.stability if request.stability is not None else user_prefs.stability\n    effective_similarity = (\n        request.similarity_boost if request.similarity_boost is not None else user_prefs.similarity_boost\n    )\n    effective_style = request.style if request.style is not None else user_prefs.style\n    effective_speech_rate = user_prefs.speech_rate\n\n    # Apply context-aware style detection if enabled\n    detected_style = None\n    if user_prefs.context_aware_style:\n        style_params = voice_style_detector.apply_style_to_synthesis(\n            text=request.text,\n            base_stability=effective_stability,\n            base_similarity_boost=effective_similarity,\n            base_style=effective_style,\n            base_speech_rate=effective_speech_rate,\n            auto_detect=True,\n        )\n        effective_stability = style_params[\"stability\"]\n        effective_similarity = style_params[\"similarity_boost\"]\n        effective_style = style_params[\"style\"]\n        effective_speech_rate = style_params[\"speech_rate\"]\n        detected_style = style_params[\"detected_style\"]\n\n        logger.debug(\n            f\"Applied context-aware style: {detected_style}\",\n            extra={\n                \"user_id\": current_user.id,\n                \"detected_style\": detected_style,\n                \"stability\": effective_stability,\n                \"speech_rate\": effective_speech_rate,\n            },\n        )\n\n    logger.info(\n        f\"Synthesizing speech for user {current_user.id}\",\n        extra={\n            \"user_id\": current_user.id,\n            \"text_length\": len(request.text),\n            \"voice_id\": effective_voice,\n            \"provider\": effective_provider,\n            \"detected_style\": detected_style,\n        },\n    )\n\n    # Validate text length\n    max_length = 5000 if effective_provider == \"elevenlabs\" else 4096\n    if len(request.text) > max_length:\n        raise HTTPException(\n            status_code=status.HTTP_400_BAD_REQUEST,\n            detail=f\"Text too long (max {max_length} characters)\",\n        )\n\n    if not request.text.strip():\n        raise HTTPException(\n            status_code=status.HTTP_400_BAD_REQUEST,\n            detail=\"Text cannot be empty\",\n        )\n\n    used_provider = effective_provider\n    fallback_used = False\n\n    async def synthesize_with_openai() -> tuple[bytes, str]:\n        \"\"\"Synthesize with OpenAI TTS.\"\"\"\n        openai_api_key = settings.OPENAI_API_KEY\n        if not openai_api_key:\n            raise ValueError(\"OpenAI API key not configured\")\n\n        async with httpx.AsyncClient(timeout=settings.OPENAI_TIMEOUT_SEC) as client:\n            external_api_requests_total.labels(service=\"openai\", endpoint=\"audio/speech\", status_code=\"pending\").inc()\n            tts_start = time.monotonic()\n            response = await client.post(\n                \"https://api.openai.com/v1/audio/speech\",\n                headers={\n                    \"Authorization\": f\"Bearer {openai_api_key}\",\n                    \"Content-Type\": \"application/json\",\n                },\n                json={\n                    \"model\": \"tts-1-hd\",  # HD model for higher quality voice\n                    \"input\": request.text,\n                    \"voice\": effective_voice,\n                    \"response_format\": \"mp3\",\n                    \"speed\": effective_speech_rate,\n                },\n            )\n\n            latency = time.monotonic() - tts_start\n            external_api_requests_total.labels(\n                service=\"openai\",\n                endpoint=\"audio/speech\",\n                status_code=str(response.status_code),\n            ).inc()\n            external_api_duration_seconds.labels(service=\"openai\", endpoint=\"audio/speech\").observe(latency)\n\n            if response.status_code != 200:\n                logger.error(f\"OpenAI TTS failed: {response.text}\")\n                raise ValueError(f\"OpenAI TTS failed: {response.status_code}\")\n\n            return response.content, \"audio/mpeg\"\n\n    async def synthesize_with_elevenlabs() -> tuple[bytes, str]:\n        \"\"\"Synthesize with ElevenLabs TTS.\"\"\"\n        if not elevenlabs_service.is_enabled():\n            raise ValueError(\"ElevenLabs TTS is not enabled\")\n\n        external_api_requests_total.labels(service=\"elevenlabs\", endpoint=\"text-to-speech\", status_code=\"pending\").inc()\n        tts_start = time.monotonic()\n\n        result = await elevenlabs_service.synthesize(\n            text=request.text,\n            voice_id=effective_voice,\n            model_id=request.model_id,\n            stability=effective_stability,\n            similarity_boost=effective_similarity,\n            style=effective_style,\n        )\n\n        latency = time.monotonic() - tts_start\n        external_api_requests_total.labels(service=\"elevenlabs\", endpoint=\"text-to-speech\", status_code=\"200\").inc()\n        external_api_duration_seconds.labels(service=\"elevenlabs\", endpoint=\"text-to-speech\").observe(latency)\n\n        return result.audio_data, result.content_type\n\n    try:\n        audio_content: bytes\n        content_type: str\n\n        if effective_provider == \"elevenlabs\":\n            try:\n                audio_content, content_type = await synthesize_with_elevenlabs()\n                used_provider = \"elevenlabs\"\n            except Exception as e:\n                # Fallback to OpenAI on ElevenLabs failure\n                logger.warning(\n                    f\"ElevenLabs TTS failed, falling back to OpenAI: {str(e)}\",\n                    extra={\"user_id\": current_user.id, \"error\": str(e)},\n                )\n                audio_content, content_type = await synthesize_with_openai()\n                used_provider = \"openai\"\n                fallback_used = True\n        else:\n            # Default: OpenAI\n            audio_content, content_type = await synthesize_with_openai()\n            used_provider = \"openai\"\n\n        logger.info(\n            f\"Speech synthesis successful for user {current_user.id}\",\n            extra={\n                \"user_id\": current_user.id,\n                \"audio_size\": len(audio_content),\n                \"voice\": effective_voice,\n                \"provider\": used_provider,\n                \"fallback_used\": fallback_used,\n                \"detected_style\": detected_style,\n            },\n        )\n\n        # Build response headers\n        headers = {\n            \"Content-Disposition\": \"attachment; filename=speech.mp3\",\n            \"X-TTS-Provider\": used_provider,\n            \"X-TTS-Fallback\": \"true\" if fallback_used else \"false\",\n        }\n        if detected_style:\n            headers[\"X-TTS-Style\"] = detected_style\n\n        return Response(\n            content=audio_content,\n            media_type=content_type,\n            headers=headers,\n        )\n\n    except httpx.TimeoutException:\n        logger.error(\n            \"Speech synthesis timeout\",\n            extra={\"user_id\": current_user.id, \"provider\": effective_provider},\n        )\n        raise HTTPException(\n            status_code=status.HTTP_504_GATEWAY_TIMEOUT,\n            detail=\"Speech synthesis request timed out\",\n        )\n    except ValueError as e:\n        logger.error(\n            f\"Speech synthesis error: {str(e)}\",\n            extra={\n                \"user_id\": current_user.id,\n                \"provider\": effective_provider,\n                \"error\": str(e),\n            },\n        )\n        raise HTTPException(\n            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n            detail=f\"Speech synthesis failed: {str(e)}\",\n        )\n    except Exception as e:\n        logger.error(\n            f\"Speech synthesis error: {str(e)}\",\n            extra={\n                \"user_id\": current_user.id,\n                \"provider\": effective_provider,\n                \"error\": str(e),\n            },\n        )\n        raise HTTPException(\n            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n            detail=f\"Speech synthesis failed: {str(e)}\",\n        )\n\n\n# ==============================================================================\n# Streaming TTS Endpoint (Low-Latency Audio)\n# ==============================================================================\n\n\n@router.post(\n    \"/synthesize/stream\",\n    summary=\"Stream TTS audio (low latency)\",\n    description=(\n        \"Stream TTS audio chunks as they're generated for immediate playback. \" \"Uses ElevenLabs for true streaming.\"\n    ),\n)\nasync def synthesize_speech_stream(\n    request: SynthesizeRequest,\n    current_user: User = Depends(get_current_user),\n    db: Session = Depends(get_db),\n):\n    \"\"\"\n    Stream TTS audio for low-latency playback.\n\n    Audio chunks are streamed as they become available, allowing playback\n    to start before the full synthesis completes. This can reduce time-to-first-audio\n    from ~500ms to ~100-200ms.\n\n    Note: True streaming is only available with ElevenLabs. OpenAI requests\n    will fall back to buffered response.\n    \"\"\"\n    import time\n\n    start_time = time.monotonic()\n\n    # Load user preferences\n    user_prefs = db.query(UserVoicePreferences).filter(UserVoicePreferences.user_id == current_user.id).first()\n\n    # Determine effective settings\n    effective_provider = request.provider or (user_prefs.tts_provider if user_prefs else \"openai\")\n    effective_voice = request.voice_id or (user_prefs.elevenlabs_voice_id if user_prefs else None)\n    effective_stability = (\n        request.stability if request.stability is not None else (user_prefs.stability if user_prefs else 0.7)\n    )\n    effective_similarity = (\n        request.similarity_boost\n        if request.similarity_boost is not None\n        else (user_prefs.similarity_boost if user_prefs else 0.8)\n    )\n    effective_style = request.style if request.style is not None else (user_prefs.style if user_prefs else 0.15)\n\n    # Apply context-aware style if enabled\n    detected_style = None\n    if user_prefs and user_prefs.context_aware_style:\n        style_params = voice_style_detector.apply_style_to_synthesis(\n            text=request.text,\n            base_stability=effective_stability,\n            base_similarity_boost=effective_similarity,\n            base_style=effective_style,\n            base_speech_rate=1.0,\n            auto_detect=True,\n        )\n        effective_stability = style_params[\"stability\"]\n        effective_similarity = style_params[\"similarity_boost\"]\n        effective_style = style_params[\"style\"]\n        detected_style = style_params.get(\"detected_style\")\n\n    # Only ElevenLabs supports true streaming\n    if effective_provider == \"elevenlabs\" and elevenlabs_service.is_enabled():\n        try:\n\n            async def audio_generator():\n                \"\"\"Generate audio chunks from ElevenLabs streaming API.\"\"\"\n                ttfb_logged = False\n                async for chunk in elevenlabs_service.synthesize_stream(\n                    text=request.text,\n                    voice_id=effective_voice,\n                    model_id=\"eleven_turbo_v2\",  # Use turbo model for lowest latency\n                    stability=effective_stability,\n                    similarity_boost=effective_similarity,\n                    style=effective_style,\n                    use_speaker_boost=True,\n                    chunk_size=1024,  # 1KB chunks for smooth streaming\n                ):\n                    if not ttfb_logged:\n                        ttfb_ms = (time.monotonic() - start_time) * 1000\n                        logger.info(\n                            f\"Streaming TTS TTFB: {ttfb_ms:.0f}ms\",\n                            extra={\n                                \"user_id\": str(current_user.id),\n                                \"ttfb_ms\": ttfb_ms,\n                                \"provider\": \"elevenlabs\",\n                                \"streaming\": True,\n                            },\n                        )\n                        ttfb_logged = True\n                    yield chunk\n\n            headers = {\n                \"X-TTS-Provider\": \"elevenlabs\",\n                \"X-TTS-Streaming\": \"true\",\n                \"X-TTS-Model\": \"eleven_turbo_v2\",\n                \"Cache-Control\": \"no-cache\",\n            }\n            if detected_style:\n                headers[\"X-TTS-Style\"] = detected_style\n\n            return StreamingResponse(\n                audio_generator(),\n                media_type=\"audio/mpeg\",\n                headers=headers,\n            )\n\n        except Exception as e:\n            logger.error(\n                f\"Streaming TTS error: {str(e)}\",\n                extra={\"user_id\": str(current_user.id)},\n            )\n            # Fall through to non-streaming fallback\n\n    # Fallback: Use non-streaming synthesize (OpenAI or ElevenLabs fallback)\n    logger.info(\n        \"Streaming not available, using buffered synthesis\",\n        extra={\"user_id\": str(current_user.id), \"provider\": effective_provider},\n    )\n\n    # Call the regular synthesize endpoint\n    return await synthesize_speech(request, current_user, db)\n\n\n# ==============================================================================\n# Phase 11: Voice Listing Endpoint\n# ==============================================================================\n\n\n@router.get(\n    \"/voices\",\n    response_model=VoiceListResponse,\n    summary=\"Get available TTS voices\",\n    description=\"List all available TTS voices from configured providers\",\n)\nasync def get_available_voices(\n    provider: str | None = None,\n    current_user: User = Depends(get_current_user),\n):\n    \"\"\"\n    Get available TTS voices from all configured providers.\n\n    Phase 11: Combined voice listing for OpenAI and ElevenLabs.\n\n    Args:\n        provider: Optional filter by provider (\"openai\" | \"elevenlabs\")\n        current_user: Authenticated user\n\n    Returns:\n        VoiceListResponse with available voices and defaults\n    \"\"\"\n    voices: list[VoiceInfo] = []\n\n    # OpenAI voices (always available if API key configured)\n    if provider is None or provider == \"openai\":\n        openai_voices = [\n            VoiceInfo(\n                voice_id=\"alloy\",\n                name=\"Alloy\",\n                provider=\"openai\",\n                category=\"neural\",\n                description=\"Neutral and balanced voice\",\n            ),\n            VoiceInfo(\n                voice_id=\"echo\",\n                name=\"Echo\",\n                provider=\"openai\",\n                category=\"neural\",\n                description=\"Warm and smooth voice\",\n            ),\n            VoiceInfo(\n                voice_id=\"fable\",\n                name=\"Fable\",\n                provider=\"openai\",\n                category=\"neural\",\n                description=\"Expressive and dynamic voice\",\n            ),\n            VoiceInfo(\n                voice_id=\"onyx\",\n                name=\"Onyx\",\n                provider=\"openai\",\n                category=\"neural\",\n                description=\"Deep and authoritative voice\",\n            ),\n            VoiceInfo(\n                voice_id=\"nova\",\n                name=\"Nova\",\n                provider=\"openai\",\n                category=\"neural\",\n                description=\"Bright and energetic voice\",\n            ),\n            VoiceInfo(\n                voice_id=\"shimmer\",\n                name=\"Shimmer\",\n                provider=\"openai\",\n                category=\"neural\",\n                description=\"Soft and gentle voice\",\n            ),\n        ]\n        voices.extend(openai_voices)\n\n    # ElevenLabs voices (if enabled)\n    if (provider is None or provider == \"elevenlabs\") and elevenlabs_service.is_enabled():\n        try:\n            elevenlabs_voices = await elevenlabs_service.get_voices()\n            for ev in elevenlabs_voices:\n                voices.append(\n                    VoiceInfo(\n                        voice_id=ev.voice_id,\n                        name=ev.name,\n                        provider=\"elevenlabs\",\n                        category=ev.category,\n                        preview_url=ev.preview_url,\n                        description=ev.description,\n                        labels=ev.labels,\n                    )\n                )\n        except Exception as e:\n            logger.warning(f\"Failed to fetch ElevenLabs voices: {str(e)}\")\n            # Continue without ElevenLabs voices\n\n    # Determine defaults\n    default_provider = settings.TTS_PROVIDER or \"openai\"\n    default_voice_id = settings.TTS_VOICE or \"alloy\"\n\n    logger.info(\n        f\"Listed {len(voices)} TTS voices for user {current_user.id}\",\n        extra={\n            \"user_id\": current_user.id,\n            \"voice_count\": len(voices),\n            \"providers\": list(set(v.provider for v in voices)),\n        },\n    )\n\n    return VoiceListResponse(\n        voices=voices,\n        default_voice_id=default_voice_id,\n        default_provider=default_provider,\n    )\n\n\n@router.post(\n    \"/realtime-session\",\n    response_model=RealtimeSessionResponse,\n    summary=\"Create Realtime API session\",\n    description=\"Generate session config for OpenAI Realtime API voice mode\",\n)\nasync def create_realtime_session(\n    request: RealtimeSessionRequest,\n    current_user: User = Depends(get_current_user),\n):\n    \"\"\"\n    Create a Realtime API session for voice mode.\n\n    This endpoint generates ephemeral session configuration that the frontend\n    uses to establish a WebSocket connection to OpenAI's Realtime API.\n\n    SECURITY: This endpoint returns an HMAC-signed ephemeral token instead of\n    the raw OpenAI API key. The token is valid for 5 minutes and is tied to\n    a specific user and session.\n\n    Args:\n        request: RealtimeSessionRequest with optional conversation_id\n        current_user: Authenticated user\n\n    Returns:\n        RealtimeSessionResponse with session configuration including:\n        - WebSocket URL\n        - Model name\n        - Session ID\n        - Expiry timestamp\n        - Auth: Ephemeral token (NOT the raw API key)\n        - Voice configuration (voice, modalities, VAD settings)\n\n    Raises:\n        HTTPException: If Realtime API is not enabled or configured\n    \"\"\"\n    start_time = time.monotonic()\n    logger.info(\n        f\"Creating Realtime session for user {current_user.id}\",\n        extra={\n            \"user_id\": current_user.id,\n            \"conversation_id\": request.conversation_id,\n            \"voice\": request.voice,\n            \"language\": request.language,\n            \"vad_sensitivity\": request.vad_sensitivity,\n        },\n    )\n\n    try:\n        # Check if Realtime API is enabled\n        if not realtime_voice_service.is_enabled():\n            raise HTTPException(\n                status_code=status.HTTP_503_SERVICE_UNAVAILABLE,\n                detail=\"Realtime API is not enabled or not configured\",\n            )\n\n        # Generate session configuration\n        # This creates a real ephemeral session with OpenAI\n        config = await realtime_voice_service.generate_session_config(\n            user_id=str(current_user.id),\n            conversation_id=request.conversation_id,\n            voice=request.voice,\n            language=request.language,\n            vad_sensitivity=request.vad_sensitivity,\n            silence_duration_ms=request.silence_duration_ms,\n            adaptive_vad=request.adaptive_vad,\n        )\n\n        duration_ms = int((time.monotonic() - start_time) * 1000)\n        logger.info(\n            f\"Realtime session created for user {current_user.id}\",\n            extra={\n                \"user_id\": current_user.id,\n                \"session_id\": config[\"session_id\"],\n                \"expires_at\": config[\"expires_at\"],\n                \"voice\": config.get(\"voice_config\", {}).get(\"voice\"),\n                \"duration_ms\": duration_ms,\n            },\n        )\n\n        return RealtimeSessionResponse(**config)\n\n    except ValueError as e:\n        duration_ms = int((time.monotonic() - start_time) * 1000)\n        logger.error(\n            f\"Failed to create Realtime session: {str(e)}\",\n            extra={\n                \"user_id\": current_user.id,\n                \"error\": str(e),\n                \"duration_ms\": duration_ms,\n            },\n        )\n        raise HTTPException(\n            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,\n            detail=str(e),\n        )\n    except Exception as e:\n        duration_ms = int((time.monotonic() - start_time) * 1000)\n        logger.error(\n            f\"Realtime session error: {str(e)}\",\n            extra={\n                \"user_id\": current_user.id,\n                \"error\": str(e),\n                \"duration_ms\": duration_ms,\n            },\n        )\n        raise HTTPException(\n            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n            detail=f\"Failed to create Realtime session: {str(e)}\",\n        )\n\n\n@router.post(\n    \"/metrics\",\n    response_model=VoiceMetricsResponse,\n    summary=\"Submit voice session metrics\",\n    description=\"Submit timing and count metrics from a voice session\",\n)\nasync def post_voice_metrics(\n    payload: VoiceMetricsPayload,\n    request: Request,\n    db: Session = Depends(get_db),\n) -> VoiceMetricsResponse:\n    \"\"\"\n    Submit voice session metrics for observability.\n\n    This endpoint receives timing and count metrics from frontend voice sessions.\n    No transcript content or PHI is sent - only timing and counts.\n\n    Authentication is optional to support sendBeacon (which cannot send headers).\n    If authenticated, user_id is included in metrics. Otherwise, metrics are\n    recorded anonymously.\n\n    Metrics flow:\n    1. Logged with structured logging\n    2. Recorded to Prometheus histograms/counters\n    3. Checked against SLO thresholds\n    4. SLO violations logged and sent to Sentry\n\n    Args:\n        payload: VoiceMetricsPayload with timing and count metrics\n        request: HTTP request (for optional auth header extraction)\n        db: Database session\n\n    Returns:\n        VoiceMetricsResponse with status \"ok\"\n    \"\"\"\n    # Optional authentication - supports sendBeacon which cannot send headers\n    user_id: str | None = None\n    auth_header = request.headers.get(\"Authorization\")\n    if auth_header and auth_header.startswith(\"Bearer \"):\n        token = auth_header[7:]\n        try:\n            payload_data = verify_token_func(token, token_type=\"access\")\n            if payload_data:\n                user_id = payload_data.get(\"sub\")\n        except Exception:\n            pass  # Auth is optional, continue without user_id\n\n    # Use anonymous if no auth\n    if not user_id:\n        user_id = \"anonymous\"\n\n    # 1. Log metrics\n    logger.info(\n        \"VoiceMetrics received\",\n        extra={\n            \"user_id\": user_id,\n            \"conversation_id\": payload.conversation_id,\n            \"connection_time_ms\": payload.connection_time_ms,\n            \"time_to_first_transcript_ms\": payload.time_to_first_transcript_ms,\n            \"last_stt_latency_ms\": payload.last_stt_latency_ms,\n            \"last_response_latency_ms\": payload.last_response_latency_ms,\n            \"session_duration_ms\": payload.session_duration_ms,\n            \"user_transcript_count\": payload.user_transcript_count,\n            \"ai_response_count\": payload.ai_response_count,\n            \"reconnect_count\": payload.reconnect_count,\n            \"session_started_at\": payload.session_started_at,\n        },\n    )\n\n    # 2. Record Prometheus metrics\n    if payload.connection_time_ms is not None:\n        voice_connection_time_seconds.observe(payload.connection_time_ms / 1000.0)\n\n    if payload.last_stt_latency_ms is not None:\n        voice_stt_latency_seconds.observe(payload.last_stt_latency_ms / 1000.0)\n\n    if payload.last_response_latency_ms is not None:\n        voice_response_latency_seconds.observe(payload.last_response_latency_ms / 1000.0)\n\n    if payload.session_duration_ms is not None:\n        voice_session_duration_seconds.observe(payload.session_duration_ms / 1000.0)\n        # Mark session as completed (we only receive metrics for completed sessions)\n        voice_sessions_total.labels(status=\"completed\").inc()\n\n    if payload.user_transcript_count > 0:\n        voice_transcripts_total.labels(direction=\"user\").inc(payload.user_transcript_count)\n\n    if payload.ai_response_count > 0:\n        voice_transcripts_total.labels(direction=\"ai\").inc(payload.ai_response_count)\n\n    if payload.reconnect_count > 0:\n        voice_reconnects_total.inc(payload.reconnect_count)\n\n    # 3. Check SLO thresholds\n    violations = check_slo_violations(\n        connection_time_ms=(float(payload.connection_time_ms) if payload.connection_time_ms else None),\n        stt_latency_ms=(float(payload.last_stt_latency_ms) if payload.last_stt_latency_ms else None),\n        response_latency_ms=(float(payload.last_response_latency_ms) if payload.last_response_latency_ms else None),\n        time_to_first_transcript_ms=(\n            float(payload.time_to_first_transcript_ms) if payload.time_to_first_transcript_ms else None\n        ),\n    )\n\n    # 4. Handle SLO violations\n    if violations:\n        # Log violations\n        log_slo_violations(violations, user_id=user_id, conversation_id=payload.conversation_id)\n\n        # Record to Prometheus\n        for violation in violations:\n            voice_slo_violations_total.labels(metric=violation.metric.value, severity=violation.severity).inc()\n\n            # Report critical violations to Sentry\n            if violation.severity == \"critical\":\n                capture_slo_violation(\n                    metric_name=violation.metric.value,\n                    actual_value=violation.actual_ms,\n                    threshold=violation.threshold_ms,\n                    user_id=user_id,\n                    conversation_id=payload.conversation_id,\n                )\n\n    return VoiceMetricsResponse(status=\"ok\")\n\n\n@router.post(\n    \"/vad-profile/update\",\n    response_model=VADProfileResponse,\n    summary=\"Update adaptive VAD profile\",\n    description=\"Submit session timing metrics to update user's adaptive VAD profile\",\n)\nasync def update_vad_profile(\n    payload: VADSessionMetrics,\n    current_user: User = Depends(get_current_user),\n) -> VADProfileResponse:\n    \"\"\"\n    Update the user's adaptive VAD profile with session metrics.\n\n    Called at end of voice session to learn user's speech patterns\n    and optimize future turn detection timing.\n    \"\"\"\n    user_id = str(current_user.id)\n\n    # Update the adaptive VAD profile\n    await adaptive_vad_manager.update_user_profile(\n        user_id=user_id,\n        pause_durations_ms=payload.pause_durations_ms,\n        utterance_durations_ms=payload.utterance_durations_ms,\n    )\n\n    # Get updated profile\n    profile = await adaptive_vad_manager.get_user_profile(user_id)\n    optimal_silence = profile.optimal_silence_duration_ms if profile else 500\n\n    logger.info(\n        f\"Updated VAD profile for user {user_id}\",\n        extra={\n            \"user_id\": user_id,\n            \"optimal_silence_ms\": optimal_silence,\n            \"pause_count\": len(payload.pause_durations_ms),\n            \"utterance_count\": len(payload.utterance_durations_ms),\n        },\n    )\n\n    return VADProfileResponse(\n        status=\"ok\",\n        optimal_silence_ms=optimal_silence,\n        is_adaptive=True,\n    )\n\n\n@router.get(\n    \"/vad-profile\",\n    response_model=VADProfileResponse,\n    summary=\"Get adaptive VAD profile\",\n    description=\"Get user's current adaptive VAD settings\",\n)\nasync def get_vad_profile(\n    current_user: User = Depends(get_current_user),\n) -> VADProfileResponse:\n    \"\"\"\n    Get the user's current adaptive VAD profile.\n\n    Returns the optimal silence duration learned from previous sessions.\n    \"\"\"\n    user_id = str(current_user.id)\n    profile = await adaptive_vad_manager.get_user_profile(user_id)\n\n    if profile:\n        return VADProfileResponse(\n            status=\"ok\",\n            optimal_silence_ms=profile.optimal_silence_duration_ms,\n            is_adaptive=True,\n        )\n    else:\n        return VADProfileResponse(\n            status=\"ok\",\n            optimal_silence_ms=500,  # Default\n            is_adaptive=False,\n        )\n\n\n@router.post(\n    \"/relay\",\n    response_model=VoiceRelayResponse,\n    summary=\"Relay final voice transcript to RAG\",\n    description=\"Persists the user transcript, runs RAG, persists assistant reply, and returns the answer.\",\n)\n@rate_limit(calls=30, period=60, key_prefix=\"voice_relay\")  # 30 calls per minute per user\nasync def relay_voice_transcript(\n    request: Request,  # Required for rate limiting\n    payload: VoiceRelayRequest,\n    current_user: User = Depends(get_current_user),\n    db: Session = Depends(get_db),\n):\n    \"\"\"\n    Take a final voice transcript, run the medical RAG pipeline, and persist both sides of the exchange.\n\n    This is intended to be called after the frontend receives a final transcription event from the Realtime\n    voice session. It provides a low-latency path to get a clinical answer while keeping conversation history\n    consistent between voice and text modes.\n    \"\"\"\n    start_time = time.monotonic()\n\n    # Persist user message first (idempotency handled at DB level if needed)\n    try:\n        session_uuid = uuid.UUID(payload.conversation_id)\n    except ValueError:\n        raise HTTPException(\n            status_code=status.HTTP_400_BAD_REQUEST,\n            detail=\"Invalid conversation_id\",\n        )\n\n    from app.api.conversations import get_session_or_404  # lazy import to avoid cycles\n\n    session = get_session_or_404(db, session_uuid, current_user)\n\n    user_message = Message(\n        session_id=session.id,\n        role=\"user\",\n        content=payload.transcript,\n        message_metadata={\n            \"source\": \"voice_relay\",\n            \"clinical_context_id\": payload.clinical_context_id,\n        },\n    )\n    db.add(user_message)\n    session.message_count = (session.message_count or 0) + 1\n    db.commit()\n    db.refresh(user_message)\n\n    # Run RAG pipeline with streaming disabled here (latency-focused)\n    query_request = QueryRequest(\n        session_id=str(session.id),\n        query=payload.transcript,\n        clinical_context_id=payload.clinical_context_id,\n    )\n\n    query_response = await voice_query_orchestrator.handle_query(query_request, trace_id=str(user_message.id))\n\n    # Persist assistant message\n    assistant_message = Message(\n        session_id=session.id,\n        role=\"assistant\",\n        content=query_response.answer,\n        tokens=query_response.tokens,\n        model=query_response.model,\n        message_metadata={\n            \"source\": \"voice_relay\",\n            \"citations\": [c.dict() for c in query_response.citations],\n            \"finish_reason\": query_response.finish_reason,\n            \"clinical_context_id\": payload.clinical_context_id,\n            \"reply_time_ms\": int((time.monotonic() - start_time) * 1000),\n        },\n    )\n    db.add(assistant_message)\n    session.message_count = (session.message_count or 0) + 1\n    db.commit()\n    db.refresh(assistant_message)\n\n    duration_ms = int((time.monotonic() - start_time) * 1000)\n    logger.info(\n        \"Voice relay completed\",\n        extra={\n            \"user_id\": current_user.id,\n            \"conversation_id\": payload.conversation_id,\n            \"user_message_id\": str(user_message.id),\n            \"assistant_message_id\": str(assistant_message.id),\n            \"duration_ms\": duration_ms,\n        },\n    )\n\n    return VoiceRelayResponse(\n        user_message_id=str(user_message.id),\n        assistant_message_id=str(assistant_message.id),\n        answer=query_response.answer,\n        citations=[c.dict() for c in query_response.citations],\n    )\n\n\n@router.websocket(\"/relay-ws\")\nasync def voice_relay_websocket(websocket: WebSocket, db: Session = Depends(get_db)):\n    \"\"\"\n    Lightweight voice relay WebSocket.\n\n    Protocol:\n    Client -> Server:\n    {\n        \"type\": \"transcript.final\",\n        \"conversation_id\": \"<uuid>\",\n        \"transcript\": \"<final user transcript>\",\n        \"clinical_context_id\": \"<optional>\"\n    }\n\n    Server -> Client:\n    - chunk events:\n      {\"type\": \"chunk\", \"content\": \"<partial answer>\"}\n    - final:\n      {\"type\": \"done\", \"answer\": \"<full answer>\", \"citations\": [...]}\n    - error:\n      {\"type\": \"error\", \"message\": \"<reason>\"}\n    \"\"\"\n    await websocket.accept()\n    connection_start = time.monotonic()\n    try:\n        # Basic token auth via query param `token`\n        token = websocket.query_params.get(\"token\")\n        if not token:\n            await websocket.send_json({\"type\": \"error\", \"message\": \"Unauthorized: missing token\"})\n            await websocket.close(code=1008)\n            return\n\n        payload = verify_token(token)\n        if not payload or payload.get(\"type\") != \"access\":\n            await websocket.send_json({\"type\": \"error\", \"message\": \"Unauthorized: invalid token\"})\n            await websocket.close(code=1008)\n            return\n\n        user_id = payload.get(\"sub\")\n\n        while True:\n            data = await websocket.receive_json()\n            msg_type = data.get(\"type\")\n            if msg_type != \"transcript.final\":\n                await websocket.send_json({\"type\": \"error\", \"message\": \"Unknown message type\"})\n                continue\n\n            transcript = data.get(\"transcript\") or \"\"\n            conversation_id = data.get(\"conversation_id\")\n            clinical_context_id = data.get(\"clinical_context_id\")\n\n            ttfb_start = time.monotonic()\n\n            try:\n                session_uuid = uuid.UUID(conversation_id)\n            except Exception:\n                await websocket.send_json({\"type\": \"error\", \"message\": \"Invalid conversation_id\"})\n                continue\n\n            session: ChatSession | None = db.query(ChatSession).filter(ChatSession.id == session_uuid).first()\n            if not session or str(session.user_id) != str(user_id):\n                await websocket.send_json({\"type\": \"error\", \"message\": \"Conversation not found\"})\n                continue\n\n            # Persist user message\n            user_message = Message(\n                session_id=session.id,\n                role=\"user\",\n                content=transcript,\n                message_metadata={\n                    \"source\": \"voice_ws_relay\",\n                    \"clinical_context_id\": clinical_context_id,\n                },\n            )\n            db.add(user_message)\n            session.message_count = (session.message_count or 0) + 1\n            db.commit()\n            db.refresh(user_message)\n\n            query_request = QueryRequest(\n                session_id=str(session.id),\n                query=transcript,\n                clinical_context_id=clinical_context_id,\n            )\n\n            full_answer_parts: list[str] = []\n\n            async def emit_chunk(chunk: str):\n                full_answer_parts.append(chunk)\n                await websocket.send_json({\"type\": \"chunk\", \"content\": chunk})\n\n            query_response = await voice_query_orchestrator.stream_query(\n                query_request, trace_id=str(user_message.id), on_chunk=emit_chunk\n            )\n            voice_first_audio_latency_seconds.observe(time.monotonic() - ttfb_start)\n\n            # Persist assistant message\n            assistant_message = Message(\n                session_id=session.id,\n                role=\"assistant\",\n                content=query_response.answer,\n                tokens=query_response.tokens,\n                model=query_response.model,\n                message_metadata={\n                    \"source\": \"voice_ws_relay\",\n                    \"citations\": [c.dict() for c in query_response.citations],\n                    \"finish_reason\": query_response.finish_reason,\n                    \"clinical_context_id\": clinical_context_id,\n                },\n            )\n            db.add(assistant_message)\n            session.message_count = (session.message_count or 0) + 1\n            db.commit()\n\n            await websocket.send_json(\n                {\n                    \"type\": \"done\",\n                    \"answer\": query_response.answer,\n                    \"citations\": [c.dict() for c in query_response.citations],\n                    \"user_message_id\": str(user_message.id),\n                    \"assistant_message_id\": str(assistant_message.id),\n                }\n            )\n\n            # Metrics\n            ttfb = time.monotonic() - ttfb_start\n            voice_proxy_ttfb_seconds.observe(ttfb)\n            relay_duration = time.monotonic() - connection_start\n            voice_relay_latency_seconds.labels(path=\"ws\").observe(relay_duration)\n\n    except WebSocketDisconnect:\n        logger.info(\"Voice relay websocket disconnected\")\n    except Exception as exc:  # noqa: BLE001\n        logger.error(\"Voice relay websocket error: %s\", exc, exc_info=True)\n        try:\n            await websocket.send_json({\"type\": \"error\", \"message\": str(exc)})\n        except Exception:\n            pass\n        await websocket.close(code=1011)\n\n\n# ==============================================================================\n# Voice Authentication Endpoints\n# ==============================================================================\n\n\n@router.post(\n    \"/auth/enroll/start\",\n    response_model=VoiceAuthStartResponse,\n    summary=\"Start voice enrollment\",\n    description=\"Begin voice biometric enrollment process\",\n)\nasync def start_voice_enrollment(\n    current_user: User = Depends(get_current_user),\n):\n    \"\"\"\n    Start voice enrollment for the authenticated user.\n\n    The enrollment process requires multiple voice samples (typically 3-10)\n    to create a reliable voice print for speaker verification.\n    \"\"\"\n    user_id = str(current_user.id)\n\n    # Check if already enrolled\n    if voice_auth_service.is_enrolled(user_id):\n        raise HTTPException(\n            status_code=status.HTTP_400_BAD_REQUEST,\n            detail=\"User already enrolled. Delete existing voice print first.\",\n        )\n\n    # Start enrollment session\n    voice_auth_service.start_enrollment(user_id)\n\n    logger.info(\n        f\"Started voice enrollment for user {user_id}\",\n        extra={\"user_id\": user_id},\n    )\n\n    return VoiceAuthStartResponse(\n        status=\"in_progress\",\n        message=\"Enrollment started. Please provide voice samples.\",\n        min_samples=voice_auth_service.config.min_enrollment_samples,\n        max_samples=voice_auth_service.config.max_enrollment_samples,\n    )\n\n\n@router.post(\n    \"/auth/enroll/sample\",\n    response_model=VoiceAuthSampleResponse,\n    summary=\"Add enrollment sample\",\n    description=\"Add a voice sample to the enrollment process\",\n)\nasync def add_enrollment_sample(\n    audio: UploadFile = File(..., description=\"Voice sample (WAV/PCM16)\"),\n    current_user: User = Depends(get_current_user),\n):\n    \"\"\"\n    Add a voice sample to the enrollment process.\n\n    The audio should be clear speech (2-10 seconds) with minimal background noise.\n    \"\"\"\n    user_id = str(current_user.id)\n\n    # Read audio data\n    audio_data = await audio.read()\n\n    # Validate size (max 5MB)\n    if len(audio_data) > 5 * 1024 * 1024:\n        raise HTTPException(\n            status_code=status.HTTP_400_BAD_REQUEST,\n            detail=\"Audio file too large (max 5MB)\",\n        )\n\n    # Add sample\n    success, message = voice_auth_service.add_enrollment_sample(user_id, audio_data)\n\n    if not success:\n        raise HTTPException(\n            status_code=status.HTTP_400_BAD_REQUEST,\n            detail=message,\n        )\n\n    # Get current status\n    status_info = voice_auth_service.get_enrollment_status(user_id)\n    samples_collected = status_info.get(\"sample_count\", 0)\n    samples_needed = max(0, voice_auth_service.config.min_enrollment_samples - samples_collected)\n\n    return VoiceAuthSampleResponse(\n        success=True,\n        message=message,\n        samples_collected=samples_collected,\n        samples_needed=samples_needed,\n    )\n\n\n@router.post(\n    \"/auth/enroll/complete\",\n    response_model=VoiceAuthCompleteResponse,\n    summary=\"Complete voice enrollment\",\n    description=\"Finalize voice enrollment and create voice print\",\n)\nasync def complete_voice_enrollment(\n    current_user: User = Depends(get_current_user),\n):\n    \"\"\"\n    Complete the voice enrollment process and create the voice print.\n\n    Must have collected minimum required samples before calling.\n    \"\"\"\n    user_id = str(current_user.id)\n\n    success, message = voice_auth_service.complete_enrollment(user_id)\n\n    if not success:\n        raise HTTPException(\n            status_code=status.HTTP_400_BAD_REQUEST,\n            detail=message,\n        )\n\n    logger.info(\n        f\"Completed voice enrollment for user {user_id}\",\n        extra={\"user_id\": user_id},\n    )\n\n    return VoiceAuthCompleteResponse(\n        success=True,\n        message=message,\n    )\n\n\n@router.post(\n    \"/auth/verify\",\n    response_model=VoiceAuthVerifyResponse,\n    summary=\"Verify voice\",\n    description=\"Verify speaker identity using voice biometrics\",\n)\nasync def verify_voice(\n    audio: UploadFile = File(..., description=\"Voice sample for verification\"),\n    current_user: User = Depends(get_current_user),\n):\n    \"\"\"\n    Verify the speaker's identity using their voice.\n\n    Compares the provided audio against the enrolled voice print.\n    \"\"\"\n    user_id = str(current_user.id)\n\n    # Check if enrolled\n    if not voice_auth_service.is_enrolled(user_id):\n        raise HTTPException(\n            status_code=status.HTTP_400_BAD_REQUEST,\n            detail=\"User not enrolled. Complete enrollment first.\",\n        )\n\n    # Read audio data\n    audio_data = await audio.read()\n\n    # Validate size\n    if len(audio_data) > 5 * 1024 * 1024:\n        raise HTTPException(\n            status_code=status.HTTP_400_BAD_REQUEST,\n            detail=\"Audio file too large (max 5MB)\",\n        )\n\n    # Verify\n    result = voice_auth_service.verify(user_id, audio_data)\n\n    logger.info(\n        f\"Voice verification for user {user_id}: {result.status.value}\",\n        extra={\n            \"user_id\": user_id,\n            \"verified\": result.verified,\n            \"confidence\": result.confidence,\n        },\n    )\n\n    return VoiceAuthVerifyResponse(\n        verified=result.verified,\n        confidence=result.confidence,\n        status=result.status.value,\n        details=result.details,\n    )\n\n\n@router.get(\n    \"/auth/status\",\n    response_model=VoiceAuthStatusResponse,\n    summary=\"Get enrollment status\",\n    description=\"Get voice biometric enrollment status for current user\",\n)\nasync def get_voice_auth_status(\n    current_user: User = Depends(get_current_user),\n):\n    \"\"\"\n    Get the voice biometric enrollment status for the authenticated user.\n    \"\"\"\n    user_id = str(current_user.id)\n    status_info = voice_auth_service.get_enrollment_status(user_id)\n\n    return VoiceAuthStatusResponse(\n        enrolled=status_info[\"status\"] == \"enrolled\",\n        status=status_info[\"status\"],\n        sample_count=status_info.get(\"sample_count\"),\n        created_at=status_info.get(\"created_at\"),\n    )\n\n\n@router.delete(\n    \"/auth/voiceprint\",\n    summary=\"Delete voice print\",\n    description=\"Delete enrolled voice print for current user\",\n)\nasync def delete_voice_print(\n    current_user: User = Depends(get_current_user),\n):\n    \"\"\"\n    Delete the enrolled voice print for the authenticated user.\n\n    This allows re-enrollment with fresh samples.\n    \"\"\"\n    user_id = str(current_user.id)\n\n    if not voice_auth_service.is_enrolled(user_id):\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND,\n            detail=\"No voice print found for user\",\n        )\n\n    voice_auth_service.delete_voice_print(user_id)\n\n    logger.info(\n        f\"Deleted voice print for user {user_id}\",\n        extra={\"user_id\": user_id},\n    )\n\n    return {\"status\": \"deleted\", \"message\": \"Voice print deleted successfully\"}\n\n\n# ==============================================================================\n# Voice Preferences Endpoints\n# ==============================================================================\n\n\n@router.get(\n    \"/preferences\",\n    response_model=VoicePreferencesResponse,\n    summary=\"Get user voice preferences\",\n    description=\"Get the authenticated user's voice/TTS preferences\",\n)\nasync def get_voice_preferences(\n    current_user: User = Depends(get_current_user),\n    db: Session = Depends(get_db),\n):\n    \"\"\"\n    Get the user's voice preferences.\n\n    Returns current TTS provider, voice selection, and audio parameters.\n    Creates default preferences if none exist.\n    \"\"\"\n    user_id = current_user.id\n\n    # Get or create preferences\n    prefs = db.query(UserVoicePreferences).filter(UserVoicePreferences.user_id == user_id).first()\n\n    if not prefs:\n        # Create default preferences\n        prefs = UserVoicePreferences.get_default_preferences(user_id)\n        db.add(prefs)\n        db.commit()\n        db.refresh(prefs)\n\n        logger.info(\n            f\"Created default voice preferences for user {user_id}\",\n            extra={\"user_id\": str(user_id)},\n        )\n\n    return VoicePreferencesResponse(\n        id=str(prefs.id),\n        user_id=str(prefs.user_id),\n        tts_provider=prefs.tts_provider,\n        openai_voice_id=prefs.openai_voice_id,\n        elevenlabs_voice_id=prefs.elevenlabs_voice_id,\n        speech_rate=prefs.speech_rate,\n        stability=prefs.stability,\n        similarity_boost=prefs.similarity_boost,\n        style=prefs.style,\n        speaker_boost=prefs.speaker_boost,\n        auto_play=prefs.auto_play,\n        context_aware_style=prefs.context_aware_style,\n        preferred_language=prefs.preferred_language,\n        created_at=prefs.created_at.isoformat() if prefs.created_at else None,\n        updated_at=prefs.updated_at.isoformat() if prefs.updated_at else None,\n    )\n\n\n@router.put(\n    \"/preferences\",\n    response_model=VoicePreferencesResponse,\n    summary=\"Update user voice preferences\",\n    description=\"Update the authenticated user's voice/TTS preferences\",\n)\nasync def update_voice_preferences(\n    request: VoicePreferencesRequest,\n    current_user: User = Depends(get_current_user),\n    db: Session = Depends(get_db),\n):\n    \"\"\"\n    Update the user's voice preferences.\n\n    Only provided fields are updated; others retain current values.\n    Creates preferences if none exist.\n    \"\"\"\n    user_id = current_user.id\n\n    # Get or create preferences\n    prefs = db.query(UserVoicePreferences).filter(UserVoicePreferences.user_id == user_id).first()\n\n    if not prefs:\n        prefs = UserVoicePreferences.get_default_preferences(user_id)\n        db.add(prefs)\n        db.flush()\n\n    # Update only provided fields\n    update_fields = request.model_dump(exclude_unset=True, exclude_none=True)\n\n    # Validate speech_rate range\n    if \"speech_rate\" in update_fields:\n        update_fields[\"speech_rate\"] = max(0.5, min(2.0, update_fields[\"speech_rate\"]))\n\n    # Validate 0-1 ranges\n    for field in [\"stability\", \"similarity_boost\", \"style\"]:\n        if field in update_fields:\n            update_fields[field] = max(0.0, min(1.0, update_fields[field]))\n\n    # Validate tts_provider\n    if \"tts_provider\" in update_fields:\n        if update_fields[\"tts_provider\"] not in [\"openai\", \"elevenlabs\"]:\n            raise HTTPException(\n                status_code=status.HTTP_400_BAD_REQUEST,\n                detail=\"tts_provider must be 'openai' or 'elevenlabs'\",\n            )\n\n    # Validate openai_voice_id\n    valid_openai_voices = {\"alloy\", \"echo\", \"fable\", \"onyx\", \"nova\", \"shimmer\"}\n    if \"openai_voice_id\" in update_fields:\n        if update_fields[\"openai_voice_id\"] not in valid_openai_voices:\n            raise HTTPException(\n                status_code=status.HTTP_400_BAD_REQUEST,\n                detail=f\"openai_voice_id must be one of: {', '.join(valid_openai_voices)}\",\n            )\n\n    # Apply updates\n    for field, value in update_fields.items():\n        setattr(prefs, field, value)\n\n    db.commit()\n    db.refresh(prefs)\n\n    logger.info(\n        f\"Updated voice preferences for user {user_id}\",\n        extra={\n            \"user_id\": str(user_id),\n            \"updated_fields\": list(update_fields.keys()),\n        },\n    )\n\n    return VoicePreferencesResponse(\n        id=str(prefs.id),\n        user_id=str(prefs.user_id),\n        tts_provider=prefs.tts_provider,\n        openai_voice_id=prefs.openai_voice_id,\n        elevenlabs_voice_id=prefs.elevenlabs_voice_id,\n        speech_rate=prefs.speech_rate,\n        stability=prefs.stability,\n        similarity_boost=prefs.similarity_boost,\n        style=prefs.style,\n        speaker_boost=prefs.speaker_boost,\n        auto_play=prefs.auto_play,\n        context_aware_style=prefs.context_aware_style,\n        preferred_language=prefs.preferred_language,\n        created_at=prefs.created_at.isoformat() if prefs.created_at else None,\n        updated_at=prefs.updated_at.isoformat() if prefs.updated_at else None,\n    )\n\n\n@router.post(\n    \"/preferences/reset\",\n    response_model=VoicePreferencesResponse,\n    summary=\"Reset voice preferences to defaults\",\n    description=\"Reset the authenticated user's voice preferences to default values\",\n)\nasync def reset_voice_preferences(\n    current_user: User = Depends(get_current_user),\n    db: Session = Depends(get_db),\n):\n    \"\"\"\n    Reset the user's voice preferences to default values.\n\n    This deletes existing preferences and creates new ones with defaults.\n    \"\"\"\n    user_id = current_user.id\n\n    # Delete existing preferences\n    db.query(UserVoicePreferences).filter(UserVoicePreferences.user_id == user_id).delete()\n\n    # Create new default preferences\n    prefs = UserVoicePreferences.get_default_preferences(user_id)\n    db.add(prefs)\n    db.commit()\n    db.refresh(prefs)\n\n    logger.info(\n        f\"Reset voice preferences for user {user_id}\",\n        extra={\"user_id\": str(user_id)},\n    )\n\n    return VoicePreferencesResponse(\n        id=str(prefs.id),\n        user_id=str(prefs.user_id),\n        tts_provider=prefs.tts_provider,\n        openai_voice_id=prefs.openai_voice_id,\n        elevenlabs_voice_id=prefs.elevenlabs_voice_id,\n        speech_rate=prefs.speech_rate,\n        stability=prefs.stability,\n        similarity_boost=prefs.similarity_boost,\n        style=prefs.style,\n        speaker_boost=prefs.speaker_boost,\n        auto_play=prefs.auto_play,\n        context_aware_style=prefs.context_aware_style,\n        preferred_language=prefs.preferred_language,\n        created_at=prefs.created_at.isoformat() if prefs.created_at else None,\n        updated_at=prefs.updated_at.isoformat() if prefs.updated_at else None,\n    )\n\n\n@router.get(\n    \"/style-presets\",\n    response_model=VoiceStylePresetsListResponse,\n    summary=\"Get voice style presets\",\n    description=\"Get all available voice style presets for context-aware TTS\",\n)\nasync def get_voice_style_presets(\n    current_user: User = Depends(get_current_user),\n):\n    \"\"\"\n    Get all available voice style presets.\n\n    These presets define TTS parameters for different content contexts:\n    - CALM: Default medical explanations\n    - URGENT: Medical warnings/emergencies\n    - EMPATHETIC: Sensitive health topics\n    - INSTRUCTIONAL: Step-by-step guidance\n    - CONVERSATIONAL: General chat\n    \"\"\"\n    presets = voice_style_detector.get_all_presets()\n\n    logger.debug(\n        f\"Retrieved voice style presets for user {current_user.id}\",\n        extra={\"user_id\": str(current_user.id), \"preset_count\": len(presets)},\n    )\n\n    return VoiceStylePresetsListResponse(presets=presets)\n\n\n# ==============================================================================\n# Thinker/Talker Voice Pipeline WebSocket\n# ==============================================================================\n\n\n@router.websocket(\"/pipeline-ws\")\nasync def voice_pipeline_websocket(\n    websocket: WebSocket,\n    db: Session = Depends(get_db),\n):\n    \"\"\"\n    Thinker/Talker voice pipeline WebSocket.\n\n    This endpoint provides a unified voice experience using:\n    - Deepgram for streaming STT\n    - GPT-4o for reasoning (with full tool/RAG support)\n    - ElevenLabs for streaming TTS\n\n    Benefits over /relay-ws:\n    - Unified conversation context with chat mode\n    - Full tool calling support in voice\n    - Better TTS quality with ElevenLabs\n    - Lower latency through streaming at all stages\n\n    Protocol (Client -> Server):\n    - audio.input: {\"type\": \"audio.input\", \"audio\": \"<base64 PCM16>\"}\n    - audio.input.complete: {\"type\": \"audio.input.complete\"}\n    - message: {\"type\": \"message\", \"content\": \"<text>\"}\n    - barge_in: {\"type\": \"barge_in\"}\n    - voice.mode: {\"type\": \"voice.mode\", \"mode\": \"activate|deactivate\"}\n\n    Protocol (Server -> Client):\n    - session.ready: Pipeline is ready\n    - transcript.delta: Partial/final transcript\n    - transcript.complete: Complete user transcript\n    - response.delta: LLM response token\n    - response.complete: Complete AI response\n    - audio.output: TTS audio chunk\n    - tool.call: Tool being called\n    - tool.result: Tool result\n    - voice.state: Pipeline state\n    - emotion.detected: User emotion detected (Hume AI integration)\n    - backchannel.trigger: Natural verbal cue audio (e.g., \"uh-huh\", \"I see\")\n    - error: Error message\n\n    Query Parameters:\n    - token: JWT auth token\n    - conversation_id: Optional conversation ID\n    - voice_id: Optional ElevenLabs voice ID\n    - language: Optional language code (default: en)\n    \"\"\"\n    # Get query parameters\n    token = websocket.query_params.get(\"token\")\n    conversation_id = websocket.query_params.get(\"conversation_id\")\n    voice_id = websocket.query_params.get(\"voice_id\", settings.ELEVENLABS_VOICE_ID)\n    language = websocket.query_params.get(\"language\", \"en\")\n\n    # Validate token\n    if not token:\n        await websocket.accept()\n        await websocket.send_json({\"type\": \"error\", \"code\": \"auth_required\", \"message\": \"Missing token\"})\n        await websocket.close(code=1008)\n        return\n\n    try:\n        payload = verify_token_func(token, token_type=\"access\")\n        if not payload:\n            raise ValueError(\"Invalid or expired token\")\n        user_id = payload.get(\"sub\")\n        if not user_id:\n            raise ValueError(\"Invalid token: missing user ID\")\n    except Exception as e:\n        await websocket.accept()\n        await websocket.send_json({\"type\": \"error\", \"code\": \"auth_failed\", \"message\": str(e)})\n        await websocket.close(code=1008)\n        return\n\n    # Check if pipeline is available\n    if not voice_pipeline_service.is_available():\n        await websocket.accept()\n        await websocket.send_json(\n            {\n                \"type\": \"error\",\n                \"code\": \"pipeline_unavailable\",\n                \"message\": \"Voice pipeline not available. Check DEEPGRAM_API_KEY and ELEVENLABS_API_KEY.\",\n            }\n        )\n        await websocket.close(code=1011)\n        return\n\n    # Create session config\n    session_id = str(uuid.uuid4())\n    config = TTSessionConfig(\n        user_id=user_id,\n        session_id=session_id,\n        conversation_id=conversation_id,\n        voice_id=voice_id,\n        language=language,\n        barge_in_enabled=settings.BARGE_IN_ENABLED,\n    )\n\n    # Create handler\n    try:\n        handler = await thinker_talker_session_manager.create_session(\n            websocket=websocket,\n            config=config,\n        )\n    except ValueError as e:\n        await websocket.accept()\n        await websocket.send_json({\"type\": \"error\", \"code\": \"session_limit\", \"message\": str(e)})\n        await websocket.close(code=1013)\n        return\n\n    # Track metrics\n    voice_sessions_total.labels(status=\"started\").inc()\n\n    connection_start = time.monotonic()\n\n    try:\n        # Start handler\n        if not await handler.start():\n            logger.error(f\"Failed to start T/T handler: {session_id}\")\n            return\n\n        # Wait for handler to complete (runs until disconnect or error)\n        while handler._running:\n            await asyncio.sleep(1)\n\n    except WebSocketDisconnect:\n        logger.info(f\"T/T WebSocket disconnected: {session_id}\")\n    except Exception as e:\n        logger.error(f\"T/T WebSocket error: {e}\")\n    finally:\n        # Cleanup\n        metrics = await handler.stop()\n        await thinker_talker_session_manager.remove_session(session_id)\n\n        # Record session duration\n        duration = time.monotonic() - connection_start\n        voice_session_duration_seconds.observe(duration)\n\n        logger.info(\n            f\"T/T session ended: {session_id}\",\n            extra={\n                \"duration_sec\": duration,\n                \"utterances\": metrics.user_utterance_count,\n                \"responses\": metrics.ai_response_count,\n            },\n        )\n\n\n@router.get(\n    \"/pipeline/status\",\n    summary=\"Get voice pipeline status\",\n    description=\"Check if the Thinker/Talker voice pipeline is available\",\n)\nasync def get_pipeline_status(\n    current_user: User = Depends(get_current_user),\n):\n    \"\"\"\n    Get the status of the Thinker/Talker voice pipeline.\n\n    Returns availability of each component:\n    - STT (Deepgram)\n    - LLM (OpenAI)\n    - TTS (ElevenLabs)\n    \"\"\"\n    from app.services.streaming_stt_service import streaming_stt_service\n    from app.services.talker_service import talker_service\n\n    return {\n        \"pipeline_available\": voice_pipeline_service.is_available(),\n        \"mode\": settings.VOICE_PIPELINE_MODE,\n        \"components\": {\n            \"stt\": {\n                \"streaming_available\": streaming_stt_service.is_streaming_available(),\n                \"fallback_available\": streaming_stt_service.is_fallback_available(),\n                \"primary_provider\": settings.VOICE_PIPELINE_STT_PRIMARY,\n                \"fallback_provider\": settings.VOICE_PIPELINE_STT_FALLBACK,\n            },\n            \"tts\": {\n                \"available\": talker_service.is_enabled(),\n                \"provider\": settings.VOICE_PIPELINE_TTS_PROVIDER,\n                \"default_voice\": settings.ELEVENLABS_VOICE_ID,\n            },\n            \"llm\": {\n                \"model\": settings.VOICE_PIPELINE_LLM_MODEL,\n            },\n        },\n        \"settings\": {\n            \"barge_in_enabled\": settings.BARGE_IN_ENABLED,\n            \"target_latency_ms\": settings.TARGET_TOTAL_LATENCY_MS,\n        },\n        \"active_sessions\": thinker_talker_session_manager.get_active_session_count(),\n    }\n"
}
