{
  "path": "services/api-gateway/app/api/medical_ai.py",
  "language": "python",
  "size": 18335,
  "last_modified": "2025-12-04T11:26:48.511Z",
  "lines": 630,
  "content": "\"\"\"\nMedical AI API Endpoints\n\nProvides endpoints for medical-specific AI capabilities:\n- Medical text embeddings (PubMedBERT, BioGPT, SciBERT)\n- Named Entity Recognition (NER) with UMLS linking\n- Multi-hop reasoning for complex medical queries\n\"\"\"\n\nfrom typing import Any, Dict, List, Optional\n\nfrom app.core.dependencies import get_current_user\nfrom app.core.logging import get_logger\nfrom app.models.user import User\nfrom fastapi import APIRouter, Depends, HTTPException\nfrom pydantic import BaseModel, Field\n\nlogger = get_logger(__name__)\n\nrouter = APIRouter(prefix=\"/api/medical-ai\", tags=[\"medical-ai\"])\n\n\n# Request/Response Models\n\n\nclass EmbeddingRequest(BaseModel):\n    \"\"\"Request for medical embedding generation\"\"\"\n\n    text: str = Field(..., min_length=1, max_length=10000)\n    model_type: str = Field(\n        default=\"pubmedbert\",\n        description=\"Model to use: pubmedbert, biogpt, scibert\",\n    )\n    pooling: str = Field(default=\"cls\", description=\"Pooling strategy: cls or mean\")\n\n\nclass EmbeddingResponse(BaseModel):\n    \"\"\"Response with embedding vector\"\"\"\n\n    embedding: List[float]\n    model: str\n    text_length: int\n    truncated: bool\n    embedding_dim: int\n    metadata: Dict[str, Any]\n\n\nclass BatchEmbeddingRequest(BaseModel):\n    \"\"\"Request for batch embedding generation\"\"\"\n\n    texts: List[str] = Field(..., min_items=1, max_items=100)\n    model_type: str = Field(default=\"pubmedbert\")\n    pooling: str = Field(default=\"cls\")\n\n\nclass TextGenerationRequest(BaseModel):\n    \"\"\"Request for medical text generation\"\"\"\n\n    prompt: str = Field(..., min_length=1, max_length=2000)\n    max_length: int = Field(default=200, ge=10, le=1000)\n    temperature: float = Field(default=0.7, ge=0.0, le=2.0)\n    top_p: float = Field(default=0.9, ge=0.0, le=1.0)\n\n\nclass NERRequest(BaseModel):\n    \"\"\"Request for medical NER\"\"\"\n\n    text: str = Field(..., min_length=1, max_length=50000)\n    detect_negation: bool = Field(default=True)\n    min_confidence: float = Field(default=0.7, ge=0.0, le=1.0)\n    include_ontology_mappings: bool = Field(default=True)\n\n\nclass NEREntity(BaseModel):\n    \"\"\"Extracted medical entity\"\"\"\n\n    text: str\n    type: str\n    start: int\n    end: int\n    negated: bool\n    uncertain: bool\n    umls_concepts: List[Dict[str, Any]]\n    ontology_mappings: List[Dict[str, Any]]\n\n\nclass NERResponse(BaseModel):\n    \"\"\"Response with extracted entities\"\"\"\n\n    entities: List[NEREntity]\n    text_length: int\n    processing_time_ms: float\n    model_used: str\n    abbreviations: Dict[str, str]\n\n\nclass ReasoningRequest(BaseModel):\n    \"\"\"Request for multi-hop reasoning\"\"\"\n\n    query: str = Field(..., min_length=5, max_length=2000)\n    max_hops: int = Field(default=3, ge=1, le=5)\n    context: Optional[str] = Field(default=None, max_length=5000)\n    strategy: Optional[str] = Field(\n        default=None,\n        description=\"Reasoning strategy: direct, multi_hop, comparative, causal, temporal\",\n    )\n\n\nclass ReasoningStep(BaseModel):\n    \"\"\"A step in the reasoning chain\"\"\"\n\n    step: int\n    question: str\n    answer: str\n    confidence: float\n    sources: List[str]\n\n\nclass ReasoningResponse(BaseModel):\n    \"\"\"Response with reasoning result\"\"\"\n\n    original_query: str\n    strategy: str\n    reasoning_chain: List[ReasoningStep]\n    final_answer: str\n    confidence: float\n    sources: List[str]\n    metadata: Dict[str, Any]\n\n\nclass SimilarityRequest(BaseModel):\n    \"\"\"Request for semantic similarity computation\"\"\"\n\n    text1: str = Field(..., min_length=1, max_length=5000)\n    text2: str = Field(..., min_length=1, max_length=5000)\n    model_type: str = Field(default=\"pubmedbert\")\n\n\nclass SimilarityResponse(BaseModel):\n    \"\"\"Response with similarity score\"\"\"\n\n    similarity: float\n    model: str\n\n\nclass SearchRequest(BaseModel):\n    \"\"\"Request for hybrid search\"\"\"\n\n    query: str = Field(..., min_length=1, max_length=1000)\n    top_k: int = Field(default=10, ge=1, le=100)\n    alpha: float = Field(\n        default=0.5,\n        ge=0.0,\n        le=1.0,\n        description=\"Balance: 1.0 = pure semantic, 0.0 = pure keyword\",\n    )\n    rerank: bool = Field(default=True)\n    expand_query: bool = Field(default=True)\n    filters: Optional[Dict[str, Any]] = None\n\n\nclass SearchResult(BaseModel):\n    \"\"\"Search result item\"\"\"\n\n    doc_id: str\n    content: str\n    score: float\n    source: str\n    metadata: Dict[str, Any]\n\n\nclass SearchResponse(BaseModel):\n    \"\"\"Response with search results\"\"\"\n\n    results: List[SearchResult]\n    query: str\n    expanded_query: Optional[str]\n    total: int\n\n\n# API Endpoints\n\n\n@router.post(\"/embed\", response_model=EmbeddingResponse)\nasync def generate_medical_embedding(\n    request: EmbeddingRequest,\n    current_user: User = Depends(get_current_user),\n):\n    \"\"\"\n    Generate medical-specific embeddings using specialized models.\n\n    Models available:\n    - pubmedbert: Best for biomedical literature\n    - biogpt: For medical text (will fallback to pubmedbert for embeddings)\n    - scibert: For general scientific text\n    \"\"\"\n    try:\n        from app.services.medical_embedding_service import MedicalEmbeddingService, MedicalModelType\n\n        service = MedicalEmbeddingService(lazy_load=True)\n\n        # Map string to enum\n        model_map = {\n            \"pubmedbert\": MedicalModelType.PUBMEDBERT,\n            \"biogpt\": MedicalModelType.BIOGPT,\n            \"scibert\": MedicalModelType.SCIBERT,\n        }\n\n        model_type = model_map.get(request.model_type.lower())\n        if not model_type:\n            raise HTTPException(\n                status_code=400,\n                detail=f\"Invalid model type: {request.model_type}\",\n            )\n\n        result = await service.generate_embedding(\n            text=request.text,\n            model_type=model_type,\n            pooling=request.pooling,\n        )\n\n        return EmbeddingResponse(\n            embedding=result.embedding,\n            model=result.model,\n            text_length=result.text_length,\n            truncated=result.truncated,\n            embedding_dim=len(result.embedding),\n            metadata=result.metadata,\n        )\n\n    except ValueError as e:\n        raise HTTPException(status_code=400, detail=str(e))\n    except RuntimeError as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n\n@router.post(\"/embed/batch\")\nasync def generate_batch_embeddings(\n    request: BatchEmbeddingRequest,\n    current_user: User = Depends(get_current_user),\n):\n    \"\"\"\n    Generate embeddings for multiple texts in batch.\n\n    More efficient than individual calls for large numbers of texts.\n    \"\"\"\n    try:\n        from app.services.medical_embedding_service import MedicalEmbeddingService, MedicalModelType\n\n        service = MedicalEmbeddingService(lazy_load=True)\n\n        model_map = {\n            \"pubmedbert\": MedicalModelType.PUBMEDBERT,\n            \"scibert\": MedicalModelType.SCIBERT,\n        }\n\n        model_type = model_map.get(request.model_type.lower(), MedicalModelType.PUBMEDBERT)\n\n        results = await service.generate_embeddings_batch(\n            texts=request.texts,\n            model_type=model_type,\n            pooling=request.pooling,\n        )\n\n        return {\n            \"embeddings\": [\n                {\n                    \"embedding\": r.embedding,\n                    \"model\": r.model,\n                    \"text_length\": r.text_length,\n                    \"truncated\": r.truncated,\n                }\n                for r in results\n            ],\n            \"total\": len(results),\n        }\n\n    except Exception as e:\n        logger.error(f\"Batch embedding failed: {e}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n\n@router.post(\"/generate\")\nasync def generate_medical_text(\n    request: TextGenerationRequest,\n    current_user: User = Depends(get_current_user),\n):\n    \"\"\"\n    Generate medical text using BioGPT.\n\n    Useful for:\n    - Completing medical sentences\n    - Generating medical descriptions\n    - Expanding medical concepts\n    \"\"\"\n    try:\n        from app.services.medical_embedding_service import MedicalEmbeddingService\n\n        service = MedicalEmbeddingService(lazy_load=True)\n\n        result = await service.generate_text(\n            prompt=request.prompt,\n            max_length=request.max_length,\n            temperature=request.temperature,\n            top_p=request.top_p,\n        )\n\n        return {\n            \"generated_text\": result.generated_text,\n            \"model\": result.model,\n            \"prompt_length\": result.prompt_length,\n            \"generation_length\": result.generation_length,\n            \"metadata\": result.metadata,\n        }\n\n    except RuntimeError as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n\n@router.post(\"/extract-entities\", response_model=NERResponse)\nasync def extract_medical_entities(\n    request: NERRequest,\n    current_user: User = Depends(get_current_user),\n):\n    \"\"\"\n    Extract medical entities from clinical text.\n\n    Entities include:\n    - Diseases and conditions\n    - Medications and drugs\n    - Procedures\n    - Anatomical structures\n    - Lab tests\n    - Genes and proteins\n\n    Returns UMLS concept links and optional ontology mappings.\n    \"\"\"\n    try:\n        from app.services.medical_ner_service import MedicalNERService, OntologyType\n\n        service = MedicalNERService(lazy_load=True)\n\n        # Extract entities\n        result = await service.extract_entities(\n            text=request.text,\n            detect_negation=request.detect_negation,\n            min_confidence=request.min_confidence,\n        )\n\n        # Optionally normalize to ontologies\n        if request.include_ontology_mappings and result.entities:\n            result.entities = await service.normalize_entities(\n                result.entities,\n                ontologies=[\n                    OntologyType.ICD10,\n                    OntologyType.RXNORM,\n                    OntologyType.SNOMED,\n                ],\n            )\n\n        # Convert to response format\n        entities = [\n            NEREntity(\n                text=e.text,\n                type=e.entity_type.value,\n                start=e.start_char,\n                end=e.end_char,\n                negated=e.negated,\n                uncertain=e.uncertain,\n                umls_concepts=[\n                    {\n                        \"cui\": c.cui,\n                        \"name\": c.name,\n                        \"semantic_types\": c.semantic_types,\n                        \"score\": c.score,\n                    }\n                    for c in e.umls_concepts\n                ],\n                ontology_mappings=[\n                    {\n                        \"ontology\": m.ontology.value,\n                        \"code\": m.code,\n                        \"display_name\": m.display_name,\n                    }\n                    for m in e.ontology_mappings\n                ],\n            )\n            for e in result.entities\n        ]\n\n        return NERResponse(\n            entities=entities,\n            text_length=result.text_length,\n            processing_time_ms=result.processing_time_ms,\n            model_used=result.model_used,\n            abbreviations=result.metadata.get(\"abbreviations\", {}),\n        )\n\n    except Exception as e:\n        logger.error(f\"NER extraction failed: {e}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n\n@router.post(\"/reason\", response_model=ReasoningResponse)\nasync def perform_multi_hop_reasoning(\n    request: ReasoningRequest,\n    current_user: User = Depends(get_current_user),\n):\n    \"\"\"\n    Perform multi-hop reasoning on complex medical queries.\n\n    This endpoint:\n    1. Decomposes complex questions into sub-questions\n    2. Answers each iteratively with retrieval\n    3. Synthesizes a comprehensive final answer\n\n    Strategies:\n    - direct: Single-step for simple queries\n    - multi_hop: Iterative for complex queries\n    - comparative: For comparing treatments/conditions\n    - causal: For cause-effect explanations\n    - temporal: For timeline-based questions\n    \"\"\"\n    try:\n        from app.services.multi_hop_reasoning_service import MultiHopReasoner, ReasoningStrategy\n\n        reasoner = MultiHopReasoner()\n\n        # Parse strategy if provided\n        strategy = None\n        if request.strategy:\n            try:\n                strategy = ReasoningStrategy(request.strategy)\n            except ValueError:\n                raise HTTPException(\n                    status_code=400,\n                    detail=f\"Invalid strategy: {request.strategy}\",\n                )\n\n        result = await reasoner.reason(\n            query=request.query,\n            max_hops=request.max_hops,\n            context=request.context,\n            strategy=strategy,\n        )\n\n        return ReasoningResponse(\n            original_query=result.original_query,\n            strategy=result.strategy.value,\n            reasoning_chain=[\n                ReasoningStep(\n                    step=s.step_number,\n                    question=s.question,\n                    answer=s.answer,\n                    confidence=s.confidence,\n                    sources=s.sources,\n                )\n                for s in result.reasoning_chain\n            ],\n            final_answer=result.final_answer,\n            confidence=result.confidence,\n            sources=result.sources,\n            metadata=result.metadata,\n        )\n\n    except Exception as e:\n        logger.error(f\"Multi-hop reasoning failed: {e}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n\n@router.post(\"/similarity\", response_model=SimilarityResponse)\nasync def compute_similarity(\n    request: SimilarityRequest,\n    current_user: User = Depends(get_current_user),\n):\n    \"\"\"\n    Compute semantic similarity between two medical texts.\n\n    Returns a score between 0 (dissimilar) and 1 (identical meaning).\n    \"\"\"\n    try:\n        from app.services.medical_embedding_service import MedicalEmbeddingService, MedicalModelType\n\n        service = MedicalEmbeddingService(lazy_load=True)\n\n        model_map = {\n            \"pubmedbert\": MedicalModelType.PUBMEDBERT,\n            \"scibert\": MedicalModelType.SCIBERT,\n        }\n\n        model_type = model_map.get(request.model_type.lower(), MedicalModelType.PUBMEDBERT)\n\n        similarity = await service.compute_similarity(request.text1, request.text2, model_type)\n\n        return SimilarityResponse(\n            similarity=similarity,\n            model=model_type.value,\n        )\n\n    except Exception as e:\n        logger.error(f\"Similarity computation failed: {e}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n\n@router.post(\"/search\", response_model=SearchResponse)\nasync def hybrid_search(\n    request: SearchRequest,\n    current_user: User = Depends(get_current_user),\n):\n    \"\"\"\n    Perform hybrid search combining semantic and keyword search.\n\n    Features:\n    - Query expansion with medical synonyms\n    - Reciprocal Rank Fusion of semantic + keyword results\n    - Cross-encoder re-ranking for improved relevance\n    \"\"\"\n    try:\n        from app.services.multi_hop_reasoning_service import HybridSearchEngine\n\n        search_engine = HybridSearchEngine(lazy_load=True)\n\n        results = await search_engine.search(\n            query=request.query,\n            top_k=request.top_k,\n            alpha=request.alpha,\n            filters=request.filters,\n            rerank=request.rerank,\n            expand_query=request.expand_query,\n        )\n\n        # Get expanded query if enabled\n        expanded = None\n        if request.expand_query:\n            expanded = await search_engine._expand_query(request.query)\n\n        return SearchResponse(\n            results=[\n                SearchResult(\n                    doc_id=r.doc_id,\n                    content=r.content,\n                    score=r.score,\n                    source=r.source,\n                    metadata=r.metadata,\n                )\n                for r in results\n            ],\n            query=request.query,\n            expanded_query=expanded if expanded != request.query else None,\n            total=len(results),\n        )\n\n    except Exception as e:\n        logger.error(f\"Hybrid search failed: {e}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n\n@router.get(\"/models\")\nasync def list_available_models(\n    current_user: User = Depends(get_current_user),\n):\n    \"\"\"\n    List available medical AI models and their capabilities.\n    \"\"\"\n    try:\n        from app.services.medical_embedding_service import MedicalEmbeddingService\n\n        service = MedicalEmbeddingService(lazy_load=True)\n        models = service.get_available_models()\n\n        return {\n            \"models\": models,\n            \"embedding_models\": [\"pubmedbert\", \"scibert\"],\n            \"generation_models\": [\"biogpt\"],\n            \"ner_models\": [\"en_core_sci_lg\"],\n        }\n\n    except Exception:\n        return {\n            \"models\": [],\n            \"embedding_models\": [\"pubmedbert\", \"scibert\"],\n            \"generation_models\": [\"biogpt\"],\n            \"ner_models\": [\"en_core_sci_lg\"],\n            \"note\": \"Models not loaded - install dependencies to enable\",\n        }\n\n\n@router.get(\"/health\")\nasync def check_health():\n    \"\"\"\n    Check health of medical AI services.\n    \"\"\"\n    health = {\n        \"embedding_service\": \"unknown\",\n        \"ner_service\": \"unknown\",\n        \"reasoning_service\": \"unknown\",\n    }\n\n    # Check embedding service\n    try:\n        from app.services.medical_embedding_service import MedicalEmbeddingService\n\n        MedicalEmbeddingService(lazy_load=True)\n        health[\"embedding_service\"] = \"available\"\n    except Exception:\n        health[\"embedding_service\"] = \"unavailable\"\n\n    # Check NER service\n    try:\n        from app.services.medical_ner_service import MedicalNERService\n\n        MedicalNERService(lazy_load=True)\n        health[\"ner_service\"] = \"available\"\n    except Exception:\n        health[\"ner_service\"] = \"unavailable\"\n\n    # Check reasoning service\n    try:\n        from app.services.multi_hop_reasoning_service import MultiHopReasoner\n\n        MultiHopReasoner()\n        health[\"reasoning_service\"] = \"available\"\n    except Exception:\n        health[\"reasoning_service\"] = \"unavailable\"\n\n    return {\n        \"status\": (\"healthy\" if all(v == \"available\" for v in health.values()) else \"degraded\"),\n        \"services\": health,\n    }\n"
}
