{
  "path": "services/api-gateway/app/services/variant_assignment.py",
  "language": "python",
  "size": 40940,
  "last_modified": "2025-12-04T12:32:40.272Z",
  "lines": 1144,
  "content": "\"\"\"Variant Assignment Service for Feature Flags.\n\nHandles consistent variant assignment for multivariate feature flags with:\n- Hash bucket caching (Redis and per-request) to avoid repeated SHA-256 computations\n- Scheduled variant percentage changes (gradual ramp-up)\n- Targeting rule evaluation\n- Consistent user-to-variant mapping\n\nUsage:\n    from app.services.variant_assignment import variant_assignment_service\n\n    # Get variant for user\n    variant = await variant_assignment_service.get_variant(\n        flag_name=\"onboarding_flow\",\n        user_id=\"user-123\",\n        context={\"country\": \"US\", \"tier\": \"premium\"}\n    )\n\"\"\"\n\nfrom __future__ import annotations\n\nimport hashlib\nimport json\nfrom datetime import datetime, timezone\nfrom typing import Any, Dict, List, Optional, Tuple\n\nfrom app.core.database import redis_client\nfrom app.core.logging import get_logger\n\nlogger = get_logger(__name__)\n\n# Redis key prefixes\nBUCKET_CACHE_PREFIX = \"flag_bucket:\"\nFLAG_VARIANTS_VERSION_PREFIX = \"flag_variants_version:\"  # Version tracking per flag\nSCHEDULED_CHANGES_PREFIX = \"flag_scheduled_changes:\"  # Scheduled changes per flag\n\n# Configurable TTLs (can be overridden via environment)\nBUCKET_CACHE_TTL = 3600  # 1 hour default (buckets are stable)\nBUCKET_CACHE_TTL_ON_CHANGE = 300  # 5 minutes after variant change\nSCHEDULED_CHANGES_TTL = 86400 * 7  # 7 days for scheduled changes\n\n# Number of buckets for consistent hashing (0-9999)\nBUCKET_COUNT = 10000\n\n\nclass FlagVariant:\n    \"\"\"Represents a variant in a multivariate feature flag.\"\"\"\n\n    def __init__(\n        self,\n        id: str,\n        name: str,\n        value: Any,\n        weight: int,\n        description: Optional[str] = None,\n    ):\n        self.id = id\n        self.name = name\n        self.value = value\n        self.weight = max(0, weight)  # Ensure non-negative\n        self.description = description\n\n    @classmethod\n    def from_dict(cls, data: Dict[str, Any]) -> \"FlagVariant\":\n        \"\"\"Create variant from dictionary.\"\"\"\n        return cls(\n            id=data.get(\"id\", \"\"),\n            name=data.get(\"name\", \"\"),\n            value=data.get(\"value\"),\n            weight=data.get(\"weight\", 0),\n            description=data.get(\"description\"),\n        )\n\n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Convert to dictionary.\"\"\"\n        return {\n            \"id\": self.id,\n            \"name\": self.name,\n            \"value\": self.value,\n            \"weight\": self.weight,\n            \"description\": self.description,\n        }\n\n\nclass RuleCondition:\n    \"\"\"Represents a condition in a targeting rule.\"\"\"\n\n    OPERATORS = {\n        \"equals\": lambda a, b: a == b,\n        \"not_equals\": lambda a, b: a != b,\n        \"contains\": lambda a, b: b in str(a) if a else False,\n        \"not_contains\": lambda a, b: b not in str(a) if a else True,\n        \"starts_with\": lambda a, b: str(a).startswith(str(b)) if a else False,\n        \"ends_with\": lambda a, b: str(a).endswith(str(b)) if a else False,\n        \"greater_than\": lambda a, b: float(a) > float(b) if a is not None else False,\n        \"less_than\": lambda a, b: float(a) < float(b) if a is not None else False,\n        \"greater_than_or_equal\": lambda a, b: (float(a) >= float(b) if a is not None else False),\n        \"less_than_or_equal\": lambda a, b: (float(a) <= float(b) if a is not None else False),\n        \"in_list\": lambda a, b: a in b if isinstance(b, (list, tuple, set)) else a == b,\n        \"not_in_list\": lambda a, b: (a not in b if isinstance(b, (list, tuple, set)) else a != b),\n        \"regex_match\": lambda a, b: _regex_match(a, b),\n        \"semver_gt\": lambda a, b: _semver_compare(a, b) > 0,\n        \"semver_lt\": lambda a, b: _semver_compare(a, b) < 0,\n        \"semver_gte\": lambda a, b: _semver_compare(a, b) >= 0,\n        \"semver_lte\": lambda a, b: _semver_compare(a, b) <= 0,\n        \"semver_eq\": lambda a, b: _semver_compare(a, b) == 0,\n    }\n\n    def __init__(self, attribute: str, operator: str, value: Any):\n        self.attribute = attribute\n        self.operator = operator\n        self.value = value\n\n    def evaluate(self, context: Dict[str, Any]) -> bool:\n        \"\"\"Evaluate the condition against the given context.\"\"\"\n        actual_value = context.get(self.attribute)\n        op_func = self.OPERATORS.get(self.operator)\n\n        if not op_func:\n            logger.warning(f\"Unknown operator: {self.operator}\")\n            return False\n\n        try:\n            return op_func(actual_value, self.value)\n        except Exception as e:\n            logger.warning(f\"Error evaluating condition {self.attribute} {self.operator}: {e}\")\n            return False\n\n    @classmethod\n    def from_dict(cls, data: Dict[str, Any]) -> \"RuleCondition\":\n        \"\"\"Create condition from dictionary.\"\"\"\n        return cls(\n            attribute=data.get(\"attribute\", \"\"),\n            operator=data.get(\"operator\", \"equals\"),\n            value=data.get(\"value\"),\n        )\n\n\nclass TargetingRule:\n    \"\"\"Represents a targeting rule for variant assignment.\"\"\"\n\n    def __init__(\n        self,\n        id: str,\n        name: str,\n        priority: int,\n        conditions: List[RuleCondition],\n        variant: Optional[str] = None,\n        enabled: Optional[bool] = None,\n        percentage: Optional[int] = None,\n    ):\n        self.id = id\n        self.name = name\n        self.priority = priority\n        self.conditions = conditions\n        self.variant = variant\n        self.enabled = enabled\n        self.percentage = percentage  # For partial rollout within rule\n\n    def evaluate(self, context: Dict[str, Any]) -> bool:\n        \"\"\"Evaluate if all conditions match the context.\"\"\"\n        return all(cond.evaluate(context) for cond in self.conditions)\n\n    @classmethod\n    def from_dict(cls, data: Dict[str, Any]) -> \"TargetingRule\":\n        \"\"\"Create rule from dictionary.\"\"\"\n        conditions = [RuleCondition.from_dict(c) for c in data.get(\"conditions\", [])]\n        return cls(\n            id=data.get(\"id\", \"\"),\n            name=data.get(\"name\", \"\"),\n            priority=data.get(\"priority\", 0),\n            conditions=conditions,\n            variant=data.get(\"variant\"),\n            enabled=data.get(\"enabled\"),\n            percentage=data.get(\"percentage\"),\n        )\n\n\nclass ScheduledChange:\n    \"\"\"Represents a scheduled change to variant weights.\n\n    Supports timezone-aware scheduling with modification tracking.\n    \"\"\"\n\n    def __init__(\n        self,\n        id: str,\n        scheduled_at: datetime,\n        changes: Dict[str, int],  # variant_id -> new_weight\n        applied: bool = False,\n        flag_name: Optional[str] = None,\n        description: Optional[str] = None,\n        created_at: Optional[datetime] = None,\n        created_by: Optional[str] = None,\n        modified_at: Optional[datetime] = None,\n        modified_by: Optional[str] = None,\n        timezone_id: str = \"UTC\",  # IANA timezone identifier\n        cancelled: bool = False,\n        cancelled_at: Optional[datetime] = None,\n        cancelled_by: Optional[str] = None,\n    ):\n        self.id = id\n        self.scheduled_at = scheduled_at\n        self.changes = changes\n        self.applied = applied\n        self.flag_name = flag_name\n        self.description = description\n        self.created_at = created_at or datetime.now(timezone.utc)\n        self.created_by = created_by\n        self.modified_at = modified_at\n        self.modified_by = modified_by\n        self.timezone_id = timezone_id\n        self.cancelled = cancelled\n        self.cancelled_at = cancelled_at\n        self.cancelled_by = cancelled_by\n\n    @classmethod\n    def from_dict(cls, data: Dict[str, Any]) -> \"ScheduledChange\":\n        \"\"\"Create from dictionary.\"\"\"\n        scheduled_at = data.get(\"scheduled_at\")\n        if isinstance(scheduled_at, str):\n            scheduled_at = datetime.fromisoformat(scheduled_at.replace(\"Z\", \"+00:00\"))\n\n        created_at = data.get(\"created_at\")\n        if isinstance(created_at, str):\n            created_at = datetime.fromisoformat(created_at.replace(\"Z\", \"+00:00\"))\n\n        modified_at = data.get(\"modified_at\")\n        if isinstance(modified_at, str):\n            modified_at = datetime.fromisoformat(modified_at.replace(\"Z\", \"+00:00\"))\n\n        cancelled_at = data.get(\"cancelled_at\")\n        if isinstance(cancelled_at, str):\n            cancelled_at = datetime.fromisoformat(cancelled_at.replace(\"Z\", \"+00:00\"))\n\n        return cls(\n            id=data.get(\"id\", \"\"),\n            scheduled_at=scheduled_at,\n            changes=data.get(\"changes\", {}),\n            applied=data.get(\"applied\", False),\n            flag_name=data.get(\"flag_name\"),\n            description=data.get(\"description\"),\n            created_at=created_at,\n            created_by=data.get(\"created_by\"),\n            modified_at=modified_at,\n            modified_by=data.get(\"modified_by\"),\n            timezone_id=data.get(\"timezone_id\", \"UTC\"),\n            cancelled=data.get(\"cancelled\", False),\n            cancelled_at=cancelled_at,\n            cancelled_by=data.get(\"cancelled_by\"),\n        )\n\n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Convert to dictionary.\"\"\"\n        return {\n            \"id\": self.id,\n            \"scheduled_at\": (self.scheduled_at.isoformat() if self.scheduled_at else None),\n            \"changes\": self.changes,\n            \"applied\": self.applied,\n            \"flag_name\": self.flag_name,\n            \"description\": self.description,\n            \"created_at\": self.created_at.isoformat() if self.created_at else None,\n            \"created_by\": self.created_by,\n            \"modified_at\": self.modified_at.isoformat() if self.modified_at else None,\n            \"modified_by\": self.modified_by,\n            \"timezone_id\": self.timezone_id,\n            \"cancelled\": self.cancelled,\n            \"cancelled_at\": (self.cancelled_at.isoformat() if self.cancelled_at else None),\n            \"cancelled_by\": self.cancelled_by,\n        }\n\n    def is_due(self, current_time: Optional[datetime] = None) -> bool:\n        \"\"\"Check if this change is due for application.\n\n        Args:\n            current_time: Current time (defaults to now in UTC)\n\n        Returns:\n            True if the change should be applied now\n        \"\"\"\n        if self.applied or self.cancelled:\n            return False\n\n        if current_time is None:\n            current_time = datetime.now(timezone.utc)\n\n        return self.scheduled_at <= current_time\n\n    def preview(self, variants: List[\"FlagVariant\"]) -> Dict[str, Any]:\n        \"\"\"Preview what this change would do to variants.\n\n        Args:\n            variants: Current variant list\n\n        Returns:\n            Dictionary showing before/after state\n        \"\"\"\n        preview_data = {\n            \"scheduled_change_id\": self.id,\n            \"scheduled_at\": (self.scheduled_at.isoformat() if self.scheduled_at else None),\n            \"changes\": [],\n        }\n\n        variant_map = {v.id: v for v in variants}\n        for variant_id, new_weight in self.changes.items():\n            current_weight = variant_map.get(variant_id, FlagVariant(\"\", \"\", None, 0)).weight\n            preview_data[\"changes\"].append(\n                {\n                    \"variant_id\": variant_id,\n                    \"current_weight\": current_weight,\n                    \"new_weight\": new_weight,\n                    \"difference\": new_weight - current_weight,\n                }\n            )\n\n        return preview_data\n\n\nclass VariantAssignmentService:\n    \"\"\"Service for consistent variant assignment with caching and scheduling.\n\n    Features:\n    - Consistent hash-based bucket assignment (deterministic per user/flag)\n    - Redis cache for hash buckets (avoid repeated SHA-256)\n    - Per-request context cache for multi-flag evaluation\n    - Targeting rule evaluation with priority ordering\n    - Scheduled weight changes for gradual rollout\n    - Weight normalization for variants\n    \"\"\"\n\n    def __init__(self):\n        self.logger = get_logger(__name__)\n        # Per-request cache for hash buckets (cleared between requests)\n        self._request_bucket_cache: Dict[str, int] = {}\n\n    def clear_request_cache(self) -> None:\n        \"\"\"Clear per-request bucket cache. Call at start of each request.\"\"\"\n        self._request_bucket_cache.clear()\n\n    def _compute_hash_bucket(self, user_id: str, flag_name: str, salt: Optional[str] = None) -> int:\n        \"\"\"Compute consistent hash bucket for user/flag combination.\n\n        Uses SHA-256 for uniform distribution across 10000 buckets.\n\n        Args:\n            user_id: Unique user identifier\n            flag_name: Feature flag name\n            salt: Optional salt for additional randomization\n\n        Returns:\n            Bucket number (0-9999)\n        \"\"\"\n        # Construct hash input\n        hash_input = f\"{user_id}:{flag_name}\"\n        if salt:\n            hash_input = f\"{hash_input}:{salt}\"\n\n        # Compute SHA-256 hash\n        hash_bytes = hashlib.sha256(hash_input.encode()).digest()\n\n        # Use first 4 bytes to get a number, then mod by bucket count\n        hash_int = int.from_bytes(hash_bytes[:4], byteorder=\"big\")\n        return hash_int % BUCKET_COUNT\n\n    async def get_bucket(\n        self,\n        user_id: str,\n        flag_name: str,\n        salt: Optional[str] = None,\n        use_cache: bool = True,\n    ) -> int:\n        \"\"\"Get hash bucket for user/flag with caching.\n\n        Checks caches in order:\n        1. Per-request cache (fastest, process-local)\n        2. Redis cache (shared across instances)\n        3. Compute and cache\n\n        Args:\n            user_id: Unique user identifier\n            flag_name: Feature flag name\n            salt: Optional salt\n            use_cache: Whether to use caching\n\n        Returns:\n            Bucket number (0-9999)\n        \"\"\"\n        cache_key = f\"{user_id}:{flag_name}:{salt or ''}\"\n\n        # Check per-request cache first\n        if use_cache and cache_key in self._request_bucket_cache:\n            self.logger.debug(f\"Request cache hit for bucket: {cache_key}\")\n            return self._request_bucket_cache[cache_key]\n\n        # Check Redis cache\n        if use_cache:\n            try:\n                redis_key = f\"{BUCKET_CACHE_PREFIX}{cache_key}\"\n                cached = redis_client.get(redis_key)\n                if cached is not None:\n                    bucket = int(cached)\n                    self._request_bucket_cache[cache_key] = bucket\n                    self.logger.debug(f\"Redis cache hit for bucket: {cache_key}\")\n                    return bucket\n            except Exception as e:\n                self.logger.warning(f\"Redis bucket cache error: {e}\")\n\n        # Compute bucket\n        bucket = self._compute_hash_bucket(user_id, flag_name, salt)\n\n        # Cache in both levels\n        if use_cache:\n            self._request_bucket_cache[cache_key] = bucket\n            try:\n                redis_key = f\"{BUCKET_CACHE_PREFIX}{cache_key}\"\n                redis_client.setex(redis_key, BUCKET_CACHE_TTL, str(bucket))\n            except Exception as e:\n                self.logger.warning(f\"Failed to cache bucket in Redis: {e}\")\n\n        return bucket\n\n    def normalize_weights(self, variants: List[FlagVariant]) -> List[Tuple[FlagVariant, int, int]]:\n        \"\"\"Normalize variant weights to bucket ranges.\n\n        Converts weights to bucket ranges (0-9999). For example:\n        - Variant A (weight 50) -> buckets 0-4999\n        - Variant B (weight 30) -> buckets 5000-7999\n        - Variant C (weight 20) -> buckets 8000-9999\n\n        Args:\n            variants: List of variants with weights\n\n        Returns:\n            List of (variant, start_bucket, end_bucket) tuples\n        \"\"\"\n        total_weight = sum(v.weight for v in variants)\n        if total_weight == 0:\n            self.logger.warning(\"All variant weights are zero\")\n            return []\n\n        ranges = []\n        current_bucket = 0\n\n        for variant in variants:\n            if variant.weight <= 0:\n                continue\n\n            # Calculate bucket range for this variant\n            bucket_share = int((variant.weight / total_weight) * BUCKET_COUNT)\n\n            # Ensure at least 1 bucket if weight > 0\n            bucket_share = max(1, bucket_share)\n\n            start = current_bucket\n            end = min(start + bucket_share - 1, BUCKET_COUNT - 1)\n\n            ranges.append((variant, start, end))\n            current_bucket = end + 1\n\n            if current_bucket >= BUCKET_COUNT:\n                break\n\n        # Adjust last variant to cover remaining buckets (handles rounding)\n        if ranges and ranges[-1][2] < BUCKET_COUNT - 1:\n            last_variant, start, _ = ranges[-1]\n            ranges[-1] = (last_variant, start, BUCKET_COUNT - 1)\n\n        return ranges\n\n    async def apply_scheduled_changes(\n        self,\n        variants: List[FlagVariant],\n        scheduled_changes: List[ScheduledChange],\n        current_time: Optional[datetime] = None,\n        flag_name: Optional[str] = None,\n    ) -> Tuple[List[FlagVariant], List[ScheduledChange], Dict[str, Any]]:\n        \"\"\"Apply scheduled weight changes that are due.\n\n        Enhanced with timezone handling, duplicate detection, and detailed logging.\n\n        Args:\n            variants: Current variant list\n            scheduled_changes: List of scheduled changes\n            current_time: Current time (defaults to now in UTC)\n            flag_name: Optional flag name for logging\n\n        Returns:\n            Tuple of (updated_variants, updated_scheduled_changes, apply_log)\n            apply_log contains details about what was applied/skipped\n        \"\"\"\n        if current_time is None:\n            current_time = datetime.now(timezone.utc)\n\n        # Ensure current_time is timezone-aware\n        if current_time.tzinfo is None:\n            current_time = current_time.replace(tzinfo=timezone.utc)\n\n        # Create variant lookup\n        variant_map = {v.id: v for v in variants}\n\n        # Track what we did\n        apply_log = {\n            \"applied\": [],\n            \"skipped_already_applied\": [],\n            \"skipped_cancelled\": [],\n            \"skipped_future\": [],\n            \"skipped_timezone_issue\": [],\n            \"skipped_duplicate\": [],\n            \"errors\": [],\n        }\n\n        # Track applied change IDs to detect duplicates\n        applied_change_ids = set()\n\n        # Sort by scheduled_at to ensure consistent ordering\n        sorted_changes = sorted(\n            scheduled_changes,\n            key=lambda s: s.scheduled_at.timestamp() if s.scheduled_at else 0,\n        )\n\n        # Apply due changes\n        updated_schedules = []\n        for schedule in sorted_changes:\n            # Skip already applied\n            if schedule.applied:\n                apply_log[\"skipped_already_applied\"].append(\n                    {\n                        \"id\": schedule.id,\n                        \"scheduled_at\": (schedule.scheduled_at.isoformat() if schedule.scheduled_at else None),\n                    }\n                )\n                updated_schedules.append(schedule)\n                continue\n\n            # Skip cancelled\n            if schedule.cancelled:\n                apply_log[\"skipped_cancelled\"].append(\n                    {\n                        \"id\": schedule.id,\n                        \"cancelled_at\": (schedule.cancelled_at.isoformat() if schedule.cancelled_at else None),\n                        \"cancelled_by\": schedule.cancelled_by,\n                    }\n                )\n                updated_schedules.append(schedule)\n                continue\n\n            # Detect duplicates (same ID applied in this batch)\n            if schedule.id in applied_change_ids:\n                self.logger.warning(\n                    f\"Skipping duplicate scheduled change: {schedule.id}\",\n                    extra={\"flag_name\": flag_name},\n                )\n                apply_log[\"skipped_duplicate\"].append({\"id\": schedule.id})\n                updated_schedules.append(schedule)\n                continue\n\n            # Handle timezone comparison\n            try:\n                scheduled_at = schedule.scheduled_at\n                if scheduled_at.tzinfo is None:\n                    # Assume UTC if no timezone\n                    scheduled_at = scheduled_at.replace(tzinfo=timezone.utc)\n                    self.logger.warning(\n                        f\"Scheduled change {schedule.id} has no timezone, assuming UTC\",\n                        extra={\"flag_name\": flag_name},\n                    )\n            except Exception as e:\n                self.logger.error(\n                    f\"Timezone error for scheduled change {schedule.id}: {e}\",\n                    extra={\"flag_name\": flag_name},\n                )\n                apply_log[\"skipped_timezone_issue\"].append(\n                    {\n                        \"id\": schedule.id,\n                        \"error\": str(e),\n                    }\n                )\n                updated_schedules.append(schedule)\n                continue\n\n            # Check if due\n            if scheduled_at <= current_time:\n                # Apply changes\n                changes_applied = []\n                for variant_id, new_weight in schedule.changes.items():\n                    if variant_id in variant_map:\n                        old_weight = variant_map[variant_id].weight\n                        variant_map[variant_id].weight = max(0, new_weight)\n                        changes_applied.append(\n                            {\n                                \"variant_id\": variant_id,\n                                \"old_weight\": old_weight,\n                                \"new_weight\": new_weight,\n                            }\n                        )\n                        self.logger.info(\n                            f\"Applied scheduled weight change: {variant_id} {old_weight} -> {new_weight}\",\n                            extra={\"flag_name\": flag_name, \"change_id\": schedule.id},\n                        )\n                    else:\n                        self.logger.warning(\n                            f\"Variant {variant_id} not found for scheduled change {schedule.id}\",\n                            extra={\"flag_name\": flag_name},\n                        )\n                        apply_log[\"errors\"].append(\n                            {\n                                \"id\": schedule.id,\n                                \"error\": f\"Variant {variant_id} not found\",\n                            }\n                        )\n\n                # Mark as applied\n                schedule.applied = True\n                applied_change_ids.add(schedule.id)\n\n                apply_log[\"applied\"].append(\n                    {\n                        \"id\": schedule.id,\n                        \"scheduled_at\": (schedule.scheduled_at.isoformat() if schedule.scheduled_at else None),\n                        \"changes\": changes_applied,\n                    }\n                )\n            else:\n                # Not yet due\n                apply_log[\"skipped_future\"].append(\n                    {\n                        \"id\": schedule.id,\n                        \"scheduled_at\": (schedule.scheduled_at.isoformat() if schedule.scheduled_at else None),\n                        \"seconds_until_due\": (scheduled_at - current_time).total_seconds(),\n                    }\n                )\n\n            updated_schedules.append(schedule)\n\n        # Log summary\n        if apply_log[\"applied\"]:\n            self.logger.info(\n                f\"Applied {len(apply_log['applied'])} scheduled changes for flag {flag_name}\",\n                extra={\"applied_ids\": [c[\"id\"] for c in apply_log[\"applied\"]]},\n            )\n\n        return list(variant_map.values()), updated_schedules, apply_log\n\n    async def evaluate_targeting_rules(\n        self,\n        rules: List[TargetingRule],\n        context: Dict[str, Any],\n        user_id: str,\n        flag_name: str,\n        salt: Optional[str] = None,\n    ) -> Optional[Tuple[Optional[str], Optional[bool]]]:\n        \"\"\"Evaluate targeting rules against context.\n\n        Rules are evaluated in priority order (lower number = higher priority).\n        First matching rule wins.\n\n        Args:\n            rules: List of targeting rules\n            context: User context for evaluation\n            user_id: User ID for percentage-based rules\n            flag_name: Flag name for bucket calculation\n            salt: Optional salt for bucket calculation\n\n        Returns:\n            Tuple of (variant_id, enabled) if rule matches, None if no match\n        \"\"\"\n        # Sort by priority (ascending)\n        sorted_rules = sorted(rules, key=lambda r: r.priority)\n\n        for rule in sorted_rules:\n            if rule.evaluate(context):\n                # Check percentage if specified\n                if rule.percentage is not None and rule.percentage < 100:\n                    bucket = await self.get_bucket(user_id, f\"{flag_name}:rule:{rule.id}\", salt)\n                    if bucket >= (rule.percentage * 100):  # Convert percentage to bucket threshold\n                        continue  # User not in percentage\n\n                self.logger.debug(f\"Targeting rule matched: {rule.name}\")\n                return (rule.variant, rule.enabled)\n\n        return None\n\n    async def get_variant(\n        self,\n        flag_name: str,\n        user_id: str,\n        variants: List[FlagVariant],\n        targeting_rules: Optional[List[TargetingRule]] = None,\n        scheduled_changes: Optional[List[ScheduledChange]] = None,\n        context: Optional[Dict[str, Any]] = None,\n        salt: Optional[str] = None,\n        default_variant: Optional[str] = None,\n    ) -> Tuple[Optional[FlagVariant], Dict[str, Any]]:\n        \"\"\"Get variant assignment for a user.\n\n        Assignment logic:\n        1. Apply any due scheduled weight changes\n        2. Evaluate targeting rules (if any match, use that variant)\n        3. Fall back to weight-based bucket assignment\n\n        Args:\n            flag_name: Feature flag name\n            user_id: Unique user identifier\n            variants: List of available variants\n            targeting_rules: Optional targeting rules\n            scheduled_changes: Optional scheduled changes\n            context: User context for targeting rules\n            salt: Optional salt for bucket calculation\n            default_variant: Default variant ID if assignment fails\n\n        Returns:\n            Tuple of (assigned_variant, metadata)\n        \"\"\"\n        metadata = {\n            \"flag_name\": flag_name,\n            \"user_id\": user_id,\n            \"assignment_method\": None,\n            \"bucket\": None,\n            \"rule_matched\": None,\n        }\n\n        if not variants:\n            self.logger.warning(f\"No variants for flag: {flag_name}\")\n            return None, metadata\n\n        # Apply scheduled changes\n        if scheduled_changes:\n            variants, _, apply_log = await self.apply_scheduled_changes(\n                variants, scheduled_changes, flag_name=flag_name\n            )\n            if apply_log.get(\"applied\"):\n                # Invalidate cache since variants changed\n                await self.invalidate_bucket_cache_for_flag(flag_name)\n\n        # Evaluate targeting rules first\n        if targeting_rules and context:\n            rule_result = await self.evaluate_targeting_rules(targeting_rules, context, user_id, flag_name, salt)\n            if rule_result:\n                variant_id, _ = rule_result\n                if variant_id:\n                    for variant in variants:\n                        if variant.id == variant_id:\n                            metadata[\"assignment_method\"] = \"targeting_rule\"\n                            return variant, metadata\n\n        # Fall back to weight-based assignment\n        bucket = await self.get_bucket(user_id, flag_name, salt)\n        metadata[\"bucket\"] = bucket\n        metadata[\"assignment_method\"] = \"bucket\"\n\n        # Normalize weights and find matching variant\n        ranges = self.normalize_weights(variants)\n        for variant, start, end in ranges:\n            if start <= bucket <= end:\n                return variant, metadata\n\n        # Fallback to default variant if specified\n        if default_variant:\n            for variant in variants:\n                if variant.id == default_variant:\n                    metadata[\"assignment_method\"] = \"default\"\n                    return variant, metadata\n\n        # Last resort: return first variant\n        if variants:\n            metadata[\"assignment_method\"] = \"fallback\"\n            return variants[0], metadata\n\n        return None, metadata\n\n    async def is_user_in_rollout(\n        self,\n        flag_name: str,\n        user_id: str,\n        rollout_percentage: int,\n        salt: Optional[str] = None,\n    ) -> bool:\n        \"\"\"Check if user is included in a percentage rollout.\n\n        Used for simple boolean flags with rollout_percentage.\n\n        Args:\n            flag_name: Feature flag name\n            user_id: User identifier\n            rollout_percentage: Percentage (0-100) of users to include\n            salt: Optional salt for bucket calculation\n\n        Returns:\n            True if user is in rollout percentage\n        \"\"\"\n        if rollout_percentage >= 100:\n            return True\n        if rollout_percentage <= 0:\n            return False\n\n        bucket = await self.get_bucket(user_id, flag_name, salt)\n        threshold = rollout_percentage * 100  # Convert to bucket range (0-10000)\n        return bucket < threshold\n\n    # ========================================================================\n    # Cache Invalidation Methods\n    # ========================================================================\n\n    async def invalidate_bucket_cache_for_flag(self, flag_name: str) -> int:\n        \"\"\"Invalidate all cached buckets for a specific flag.\n\n        Called when variant definitions or weights change to ensure\n        users get re-evaluated with new variant configuration.\n\n        Args:\n            flag_name: The flag whose buckets should be invalidated\n\n        Returns:\n            Number of cache entries invalidated\n        \"\"\"\n        pattern = f\"{BUCKET_CACHE_PREFIX}*:{flag_name}:*\"\n        invalidated = 0\n\n        try:\n            # Use SCAN to find all matching keys\n            cursor = 0\n            keys_to_delete = []\n\n            while True:\n                cursor, keys = redis_client.scan(cursor, match=pattern, count=100)\n                keys_to_delete.extend(keys)\n                if cursor == 0:\n                    break\n\n            # Delete all matching keys\n            if keys_to_delete:\n                invalidated = redis_client.delete(*keys_to_delete)\n                self.logger.info(f\"Invalidated {invalidated} bucket cache entries for flag: {flag_name}\")\n\n            # Also clear per-request cache for this flag\n            keys_to_remove = [k for k in self._request_bucket_cache if f\":{flag_name}:\" in k]\n            for k in keys_to_remove:\n                del self._request_bucket_cache[k]\n\n            # Increment flag variants version to signal change\n            version_key = f\"{FLAG_VARIANTS_VERSION_PREFIX}{flag_name}\"\n            redis_client.incr(version_key)\n\n        except Exception as e:\n            self.logger.warning(f\"Failed to invalidate bucket cache for {flag_name}: {e}\")\n\n        return invalidated\n\n    async def invalidate_bucket_cache_for_user(\n        self,\n        user_id: str,\n        flag_name: Optional[str] = None,\n    ) -> int:\n        \"\"\"Invalidate cached buckets for a specific user.\n\n        Useful when a user's targeting context changes significantly\n        or when testing flag behavior for a specific user.\n\n        Args:\n            user_id: The user whose buckets should be invalidated\n            flag_name: Optional flag to limit invalidation to\n\n        Returns:\n            Number of cache entries invalidated\n        \"\"\"\n        if flag_name:\n            pattern = f\"{BUCKET_CACHE_PREFIX}{user_id}:{flag_name}:*\"\n        else:\n            pattern = f\"{BUCKET_CACHE_PREFIX}{user_id}:*\"\n\n        invalidated = 0\n\n        try:\n            cursor = 0\n            keys_to_delete = []\n\n            while True:\n                cursor, keys = redis_client.scan(cursor, match=pattern, count=100)\n                keys_to_delete.extend(keys)\n                if cursor == 0:\n                    break\n\n            if keys_to_delete:\n                invalidated = redis_client.delete(*keys_to_delete)\n                self.logger.info(f\"Invalidated {invalidated} bucket cache entries for user: {user_id}\")\n\n            # Clear per-request cache\n            if flag_name:\n                keys_to_remove = [k for k in self._request_bucket_cache if k.startswith(f\"{user_id}:{flag_name}:\")]\n            else:\n                keys_to_remove = [k for k in self._request_bucket_cache if k.startswith(f\"{user_id}:\")]\n            for k in keys_to_remove:\n                del self._request_bucket_cache[k]\n\n        except Exception as e:\n            self.logger.warning(f\"Failed to invalidate bucket cache for user {user_id}: {e}\")\n\n        return invalidated\n\n    def get_flag_variants_version(self, flag_name: str) -> int:\n        \"\"\"Get the current variants version for a flag.\n\n        Used to detect if cached bucket assignments are stale.\n\n        Args:\n            flag_name: The flag name\n\n        Returns:\n            Current version number (0 if not set)\n        \"\"\"\n        try:\n            version_key = f\"{FLAG_VARIANTS_VERSION_PREFIX}{flag_name}\"\n            version = redis_client.get(version_key)\n            return int(version) if version else 0\n        except Exception:\n            return 0\n\n    # ========================================================================\n    # Scheduled Changes Persistence\n    # ========================================================================\n\n    async def save_scheduled_change(\n        self,\n        flag_name: str,\n        change: ScheduledChange,\n    ) -> bool:\n        \"\"\"Persist a scheduled change to Redis.\n\n        Args:\n            flag_name: Flag name for this change\n            change: The scheduled change to save\n\n        Returns:\n            True if saved successfully\n        \"\"\"\n        try:\n            key = f\"{SCHEDULED_CHANGES_PREFIX}{flag_name}\"\n            change_data = change.to_dict()\n            change_data[\"flag_name\"] = flag_name\n\n            # Use sorted set with scheduled_at as score for ordering\n            score = change.scheduled_at.timestamp() if change.scheduled_at else 0\n            redis_client.zadd(key, {json.dumps(change_data): score})\n            redis_client.expire(key, SCHEDULED_CHANGES_TTL)\n\n            self.logger.info(\n                f\"Saved scheduled change {change.id} for flag {flag_name}\",\n                extra={\"scheduled_at\": change.scheduled_at.isoformat()},\n            )\n            return True\n        except Exception as e:\n            self.logger.error(f\"Failed to save scheduled change: {e}\")\n            return False\n\n    async def get_scheduled_changes(\n        self,\n        flag_name: str,\n        include_applied: bool = False,\n        include_cancelled: bool = False,\n    ) -> List[ScheduledChange]:\n        \"\"\"Get all scheduled changes for a flag.\n\n        Args:\n            flag_name: Flag name to get changes for\n            include_applied: Include already-applied changes\n            include_cancelled: Include cancelled changes\n\n        Returns:\n            List of scheduled changes sorted by scheduled_at\n        \"\"\"\n        try:\n            key = f\"{SCHEDULED_CHANGES_PREFIX}{flag_name}\"\n            # Get all entries ordered by score (scheduled_at)\n            entries = redis_client.zrange(key, 0, -1, withscores=True)\n\n            changes = []\n            for entry_json, _ in entries:\n                try:\n                    data = json.loads(entry_json)\n                    change = ScheduledChange.from_dict(data)\n\n                    # Filter based on flags\n                    if not include_applied and change.applied:\n                        continue\n                    if not include_cancelled and change.cancelled:\n                        continue\n\n                    changes.append(change)\n                except json.JSONDecodeError:\n                    continue\n\n            return changes\n        except Exception as e:\n            self.logger.warning(f\"Failed to get scheduled changes for {flag_name}: {e}\")\n            return []\n\n    async def cancel_scheduled_change(\n        self,\n        flag_name: str,\n        change_id: str,\n        cancelled_by: Optional[str] = None,\n    ) -> bool:\n        \"\"\"Cancel a scheduled change.\n\n        Args:\n            flag_name: Flag name\n            change_id: ID of the change to cancel\n            cancelled_by: User who cancelled (for audit)\n\n        Returns:\n            True if cancelled successfully\n        \"\"\"\n        try:\n            key = f\"{SCHEDULED_CHANGES_PREFIX}{flag_name}\"\n            entries = redis_client.zrange(key, 0, -1, withscores=True)\n\n            for entry_json, score in entries:\n                data = json.loads(entry_json)\n                if data.get(\"id\") == change_id:\n                    # Remove old entry\n                    redis_client.zrem(key, entry_json)\n\n                    # Update and re-add with cancelled flag\n                    data[\"cancelled\"] = True\n                    data[\"cancelled_at\"] = datetime.now(timezone.utc).isoformat()\n                    data[\"cancelled_by\"] = cancelled_by\n\n                    redis_client.zadd(key, {json.dumps(data): score})\n\n                    self.logger.info(\n                        f\"Cancelled scheduled change {change_id} for flag {flag_name}\",\n                        extra={\"cancelled_by\": cancelled_by},\n                    )\n                    return True\n\n            self.logger.warning(f\"Scheduled change {change_id} not found for flag {flag_name}\")\n            return False\n        except Exception as e:\n            self.logger.error(f\"Failed to cancel scheduled change: {e}\")\n            return False\n\n    async def delete_scheduled_change(\n        self,\n        flag_name: str,\n        change_id: str,\n    ) -> bool:\n        \"\"\"Permanently delete a scheduled change.\n\n        Args:\n            flag_name: Flag name\n            change_id: ID of the change to delete\n\n        Returns:\n            True if deleted successfully\n        \"\"\"\n        try:\n            key = f\"{SCHEDULED_CHANGES_PREFIX}{flag_name}\"\n            entries = redis_client.zrange(key, 0, -1)\n\n            for entry_json in entries:\n                data = json.loads(entry_json)\n                if data.get(\"id\") == change_id:\n                    redis_client.zrem(key, entry_json)\n                    self.logger.info(f\"Deleted scheduled change {change_id}\")\n                    return True\n\n            return False\n        except Exception as e:\n            self.logger.error(f\"Failed to delete scheduled change: {e}\")\n            return False\n\n    async def get_all_pending_scheduled_changes(\n        self,\n    ) -> Dict[str, List[ScheduledChange]]:\n        \"\"\"Get all pending scheduled changes across all flags.\n\n        Returns:\n            Dictionary of flag_name -> list of pending changes\n        \"\"\"\n        try:\n            pattern = f\"{SCHEDULED_CHANGES_PREFIX}*\"\n            all_changes: Dict[str, List[ScheduledChange]] = {}\n\n            cursor = 0\n            while True:\n                cursor, keys = redis_client.scan(cursor, match=pattern, count=100)\n                for key in keys:\n                    flag_name = key.replace(SCHEDULED_CHANGES_PREFIX, \"\")\n                    changes = await self.get_scheduled_changes(flag_name)\n                    if changes:\n                        all_changes[flag_name] = changes\n                if cursor == 0:\n                    break\n\n            return all_changes\n        except Exception as e:\n            self.logger.warning(f\"Failed to get all pending scheduled changes: {e}\")\n            return {}\n\n\n# Helper functions for condition operators\n\n\ndef _regex_match(value: Any, pattern: str) -> bool:\n    \"\"\"Match value against regex pattern.\"\"\"\n    import re\n\n    if value is None:\n        return False\n    try:\n        return bool(re.search(pattern, str(value)))\n    except re.error:\n        return False\n\n\ndef _semver_compare(a: str, b: str) -> int:\n    \"\"\"Compare two semantic versions.\n\n    Returns:\n        -1 if a < b, 0 if a == b, 1 if a > b\n    \"\"\"\n    try:\n        from packaging.version import Version\n\n        va = Version(str(a) if a else \"0.0.0\")\n        vb = Version(str(b) if b else \"0.0.0\")\n\n        if va < vb:\n            return -1\n        elif va > vb:\n            return 1\n        return 0\n    except Exception:\n        # Fallback to string comparison\n        if str(a) < str(b):\n            return -1\n        elif str(a) > str(b):\n            return 1\n        return 0\n\n\n# Global singleton instance\nvariant_assignment_service = VariantAssignmentService()\n"
}
