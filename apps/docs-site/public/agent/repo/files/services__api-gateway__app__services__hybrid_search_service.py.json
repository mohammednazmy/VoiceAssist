{
  "path": "services/api-gateway/app/services/hybrid_search_service.py",
  "language": "python",
  "size": 21963,
  "last_modified": "2025-12-04T11:26:57.636Z",
  "lines": 656,
  "content": "\"\"\"\nHybrid Search Service (Phase 5 - Advanced RAG)\n\nCombines multiple search strategies for improved retrieval:\n1. Dense Vector Search (Qdrant) - Semantic similarity\n2. Sparse Vector Search (BM25) - Keyword matching\n3. Hybrid Fusion - Reciprocal Rank Fusion (RRF)\n\nFeatures:\n- Configurable fusion weights\n- Score normalization\n- Query-dependent strategy selection\n- Async execution with parallel search\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport logging\nimport math\nimport re\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nfrom typing import Any, Dict, List, Optional, Tuple\n\nimport openai\nfrom app.core.config import settings\nfrom app.services.cache_service import cache_service, generate_cache_key\nfrom qdrant_client.models import FieldCondition, Filter, MatchAny, MatchValue\n\nlogger = logging.getLogger(__name__)\n\n\nclass SearchStrategy(str, Enum):\n    \"\"\"Search strategy options.\"\"\"\n\n    VECTOR_ONLY = \"vector_only\"\n    BM25_ONLY = \"bm25_only\"\n    HYBRID = \"hybrid\"\n    AUTO = \"auto\"  # Automatically select based on query\n\n\n@dataclass\nclass SearchResult:\n    \"\"\"Unified search result from any search method.\"\"\"\n\n    chunk_id: str\n    document_id: str\n    content: str\n    score: float\n    metadata: Dict[str, Any] = field(default_factory=dict)\n    source: str = \"unknown\"  # vector, bm25, hybrid\n\n\n@dataclass\nclass HybridSearchConfig:\n    \"\"\"Configuration for hybrid search.\"\"\"\n\n    vector_weight: float = 0.6  # Weight for vector search\n    bm25_weight: float = 0.4  # Weight for BM25 search\n    rrf_k: int = 60  # RRF constant (higher = more weight to lower ranks)\n    normalize_scores: bool = True\n    min_score_threshold: float = 0.1\n    max_results: int = 20\n\n\nclass BM25Index:\n    \"\"\"\n    Simple BM25 index for keyword search.\n\n    In production, this would typically be backed by Elasticsearch\n    or Meilisearch. This implementation uses an in-memory index for MVP.\n    \"\"\"\n\n    def __init__(\n        self,\n        k1: float = 1.5,  # Term frequency saturation\n        b: float = 0.75,  # Length normalization\n    ):\n        self.k1 = k1\n        self.b = b\n        self.documents: Dict[str, Dict[str, Any]] = {}\n        self.doc_lengths: Dict[str, int] = {}\n        self.avg_doc_length: float = 0.0\n        self.term_doc_freqs: Dict[str, int] = {}\n        self.inverted_index: Dict[str, Dict[str, float]] = {}\n        self._initialized = False\n\n    def _tokenize(self, text: str) -> List[str]:\n        \"\"\"Simple tokenization - lowercase and split on non-alphanumeric.\"\"\"\n        text = text.lower()\n        tokens = re.findall(r\"\\b[a-z0-9]+\\b\", text)\n        return tokens\n\n    def _compute_tf(self, term: str, doc_tokens: List[str]) -> float:\n        \"\"\"Compute term frequency.\"\"\"\n        return doc_tokens.count(term)\n\n    def add_document(\n        self,\n        doc_id: str,\n        content: str,\n        metadata: Optional[Dict[str, Any]] = None,\n    ) -> None:\n        \"\"\"Add document to the index.\"\"\"\n        tokens = self._tokenize(content)\n        self.documents[doc_id] = {\n            \"content\": content,\n            \"tokens\": tokens,\n            \"metadata\": metadata or {},\n        }\n        self.doc_lengths[doc_id] = len(tokens)\n\n        # Update inverted index\n        unique_terms = set(tokens)\n        for term in unique_terms:\n            if term not in self.inverted_index:\n                self.inverted_index[term] = {}\n                self.term_doc_freqs[term] = 0\n            self.term_doc_freqs[term] += 1\n            self.inverted_index[term][doc_id] = self._compute_tf(term, tokens)\n\n        # Update average document length\n        total_length = sum(self.doc_lengths.values())\n        self.avg_doc_length = total_length / len(self.doc_lengths) if self.doc_lengths else 0\n        self._initialized = True\n\n    def search(\n        self,\n        query: str,\n        top_k: int = 10,\n    ) -> List[Tuple[str, float]]:\n        \"\"\"\n        Search the index using BM25.\n\n        Returns list of (doc_id, score) tuples.\n        \"\"\"\n        if not self._initialized:\n            return []\n\n        query_tokens = self._tokenize(query)\n        scores: Dict[str, float] = {}\n        n_docs = len(self.documents)\n\n        for term in query_tokens:\n            if term not in self.inverted_index:\n                continue\n\n            # IDF component\n            df = self.term_doc_freqs[term]\n            idf = math.log((n_docs - df + 0.5) / (df + 0.5) + 1)\n\n            # Score each document containing this term\n            for doc_id, tf in self.inverted_index[term].items():\n                doc_len = self.doc_lengths[doc_id]\n\n                # BM25 formula\n                numerator = tf * (self.k1 + 1)\n                denominator = tf + self.k1 * (1 - self.b + self.b * (doc_len / self.avg_doc_length))\n                term_score = idf * (numerator / denominator)\n\n                scores[doc_id] = scores.get(doc_id, 0.0) + term_score\n\n        # Sort by score and return top_k\n        sorted_results = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n        return sorted_results[:top_k]\n\n    def get_document(self, doc_id: str) -> Optional[Dict[str, Any]]:\n        \"\"\"Get document by ID.\"\"\"\n        return self.documents.get(doc_id)\n\n\nclass HybridSearchService:\n    \"\"\"\n    Hybrid search combining vector and BM25 search.\n\n    Uses Reciprocal Rank Fusion (RRF) to combine results from\n    multiple search strategies.\n    \"\"\"\n\n    def __init__(\n        self,\n        qdrant_url: str = \"http://qdrant:6333\",\n        collection_name: str = \"medical_kb\",\n        embedding_model: str = \"text-embedding-3-small\",\n        config: Optional[HybridSearchConfig] = None,\n    ):\n        self.qdrant_url = qdrant_url\n        self.collection_name = collection_name\n        self.embedding_model = embedding_model\n        self.config = config or HybridSearchConfig()\n\n        # Initialize BM25 index (in-memory for MVP)\n        self.bm25_index = BM25Index()\n\n        # Check if Qdrant is enabled\n        self.qdrant_enabled = getattr(settings, \"QDRANT_ENABLED\", True)\n        self.qdrant_client = None\n\n        if self.qdrant_enabled:\n            try:\n                from qdrant_client import QdrantClient\n\n                self.qdrant_client = QdrantClient(url=qdrant_url, timeout=5.0)\n            except Exception as e:\n                logger.warning(f\"Failed to initialize Qdrant client: {e}\")\n                self.qdrant_enabled = False\n\n    async def initialize_bm25_index(self, documents: List[Dict[str, Any]]) -> int:\n        \"\"\"\n        Initialize BM25 index with documents.\n\n        Args:\n            documents: List of dicts with 'id', 'content', 'metadata' keys\n\n        Returns:\n            Number of documents indexed\n        \"\"\"\n        count = 0\n        for doc in documents:\n            self.bm25_index.add_document(\n                doc_id=doc[\"id\"],\n                content=doc[\"content\"],\n                metadata=doc.get(\"metadata\", {}),\n            )\n            count += 1\n\n        logger.info(f\"Initialized BM25 index with {count} documents\")\n        return count\n\n    async def _generate_embedding(self, text: str) -> List[float]:\n        \"\"\"Generate embedding using OpenAI.\"\"\"\n        cache_key = generate_cache_key(\"hybrid_embedding\", text, model=self.embedding_model)\n        cached = await cache_service.get(cache_key)\n        if cached is not None:\n            return cached\n\n        try:\n            response = await asyncio.wait_for(\n                openai.embeddings.create(model=self.embedding_model, input=text),\n                timeout=15,\n            )\n            embedding = response.data[0].embedding\n            await cache_service.set(cache_key, embedding, ttl=86400)\n            return embedding\n        except Exception as e:\n            logger.error(f\"Error generating embedding: {e}\")\n            raise\n\n    def _build_filter(self, filters: Optional[Dict[str, Any]]) -> Optional[Filter]:\n        \"\"\"Convert simple dict filters into a Qdrant filter.\"\"\"\n\n        if not filters:\n            return None\n\n        must: List[FieldCondition] = []\n        for key, value in filters.items():\n            if isinstance(value, (list, tuple, set)):\n                must.append(FieldCondition(key=key, match=MatchAny(any=list(value))))\n            else:\n                must.append(FieldCondition(key=key, match=MatchValue(value=value)))\n\n        return Filter(must=must)\n\n    def _metadata_matches_filters(self, metadata: Dict[str, Any], filters: Optional[Dict[str, Any]]) -> bool:\n        \"\"\"Check if metadata satisfies filter conditions.\"\"\"\n\n        if not filters:\n            return True\n\n        for key, expected in filters.items():\n            if isinstance(expected, (list, tuple, set)):\n                if metadata.get(key) not in expected:\n                    return False\n            else:\n                if metadata.get(key) != expected:\n                    return False\n        return True\n\n    async def _vector_search(\n        self,\n        query: str,\n        top_k: int,\n        score_threshold: float = 0.0,\n        filters: Optional[Dict[str, Any]] = None,\n    ) -> List[SearchResult]:\n        \"\"\"Perform vector search using Qdrant.\"\"\"\n        if not self.qdrant_enabled or not self.qdrant_client:\n            return []\n\n        try:\n            query_embedding = await self._generate_embedding(query)\n\n            # Execute Qdrant search in thread pool\n            search_filter = self._build_filter(filters)\n            results = await asyncio.wait_for(\n                asyncio.to_thread(\n                    self.qdrant_client.search,\n                    collection_name=self.collection_name,\n                    query_vector=query_embedding,\n                    limit=top_k,\n                    score_threshold=score_threshold,\n                    query_filter=search_filter,\n                ),\n                timeout=5,\n            )\n\n            search_results = []\n            for result in results:\n                search_results.append(\n                    SearchResult(\n                        chunk_id=str(result.id),\n                        document_id=result.payload.get(\"document_id\", \"unknown\"),\n                        content=result.payload.get(\"content\", \"\"),\n                        score=result.score,\n                        metadata={\n                            \"title\": result.payload.get(\"title\", \"\"),\n                            \"source_type\": result.payload.get(\"source_type\", \"\"),\n                            **{\n                                k: v\n                                for k, v in result.payload.items()\n                                if k\n                                not in [\n                                    \"content\",\n                                    \"document_id\",\n                                    \"title\",\n                                    \"source_type\",\n                                ]\n                            },\n                        },\n                        source=\"vector\",\n                    )\n                )\n\n            return search_results\n\n        except Exception as e:\n            logger.error(f\"Vector search error: {e}\")\n            return []\n\n    async def _bm25_search(\n        self,\n        query: str,\n        top_k: int,\n        filters: Optional[Dict[str, Any]] = None,\n    ) -> List[SearchResult]:\n        \"\"\"Perform BM25 keyword search.\"\"\"\n        try:\n            results = self.bm25_index.search(query, top_k=top_k)\n\n            search_results = []\n            for doc_id, score in results:\n                doc = self.bm25_index.get_document(doc_id)\n                if doc and self._metadata_matches_filters(doc.get(\"metadata\", {}), filters):\n                    search_results.append(\n                        SearchResult(\n                            chunk_id=doc_id,\n                            document_id=doc[\"metadata\"].get(\"document_id\", doc_id),\n                            content=doc[\"content\"],\n                            score=score,\n                            metadata=doc[\"metadata\"],\n                            source=\"bm25\",\n                        )\n                    )\n\n            return search_results\n\n        except Exception as e:\n            logger.error(f\"BM25 search error: {e}\")\n            return []\n\n    def _normalize_scores(\n        self,\n        results: List[SearchResult],\n    ) -> List[SearchResult]:\n        \"\"\"Normalize scores to 0-1 range.\"\"\"\n        if not results:\n            return results\n\n        scores = [r.score for r in results]\n        min_score = min(scores)\n        max_score = max(scores)\n\n        if max_score == min_score:\n            for r in results:\n                r.score = 1.0\n        else:\n            for r in results:\n                r.score = (r.score - min_score) / (max_score - min_score)\n\n        return results\n\n    def _reciprocal_rank_fusion(\n        self,\n        result_lists: List[List[SearchResult]],\n        k: int = 60,\n    ) -> List[SearchResult]:\n        \"\"\"\n        Combine multiple result lists using Reciprocal Rank Fusion.\n\n        RRF score = sum(1 / (k + rank_i)) for each list\n\n        Args:\n            result_lists: List of result lists from different search methods\n            k: RRF constant (default 60)\n\n        Returns:\n            Combined and re-ranked results\n        \"\"\"\n        # Calculate RRF scores\n        rrf_scores: Dict[str, float] = {}\n        result_lookup: Dict[str, SearchResult] = {}\n\n        for results in result_lists:\n            for rank, result in enumerate(results, start=1):\n                rrf_score = 1.0 / (k + rank)\n                rrf_scores[result.chunk_id] = rrf_scores.get(result.chunk_id, 0.0) + rrf_score\n                # Keep the result with highest original score\n                if result.chunk_id not in result_lookup or result.score > result_lookup[result.chunk_id].score:\n                    result_lookup[result.chunk_id] = result\n\n        # Create final results with RRF scores\n        final_results = []\n        for chunk_id, rrf_score in sorted(rrf_scores.items(), key=lambda x: x[1], reverse=True):\n            result = result_lookup[chunk_id]\n            # Update score to RRF score\n            result.score = rrf_score\n            result.source = \"hybrid\"\n            final_results.append(result)\n\n        return final_results\n\n    def _weighted_fusion(\n        self,\n        vector_results: List[SearchResult],\n        bm25_results: List[SearchResult],\n    ) -> List[SearchResult]:\n        \"\"\"\n        Combine results using weighted score fusion.\n\n        Args:\n            vector_results: Results from vector search\n            bm25_results: Results from BM25 search\n\n        Returns:\n            Combined results with weighted scores\n        \"\"\"\n        # Normalize scores if configured\n        if self.config.normalize_scores:\n            vector_results = self._normalize_scores(vector_results)\n            bm25_results = self._normalize_scores(bm25_results)\n\n        # Combine scores\n        combined_scores: Dict[str, float] = {}\n        result_lookup: Dict[str, SearchResult] = {}\n\n        for result in vector_results:\n            combined_scores[result.chunk_id] = result.score * self.config.vector_weight\n            result_lookup[result.chunk_id] = result\n\n        for result in bm25_results:\n            if result.chunk_id in combined_scores:\n                combined_scores[result.chunk_id] += result.score * self.config.bm25_weight\n            else:\n                combined_scores[result.chunk_id] = result.score * self.config.bm25_weight\n                result_lookup[result.chunk_id] = result\n\n        # Create final results\n        final_results = []\n        for chunk_id, score in sorted(combined_scores.items(), key=lambda x: x[1], reverse=True):\n            result = result_lookup[chunk_id]\n            result.score = score\n            result.source = \"hybrid\"\n            final_results.append(result)\n\n        return final_results\n\n    def _select_strategy(self, query: str) -> SearchStrategy:\n        \"\"\"\n        Automatically select search strategy based on query characteristics.\n\n        - Short queries with specific terms -> BM25 might be better\n        - Long, semantic queries -> Vector search might be better\n        - Mixed queries -> Hybrid\n        \"\"\"\n        tokens = query.lower().split()\n\n        # Very short queries (1-2 words) - use hybrid with BM25 emphasis\n        if len(tokens) <= 2:\n            return SearchStrategy.HYBRID\n\n        # Queries with medical terms (acronyms, specific terms)\n        medical_indicators = [\n            \"mg\",\n            \"ml\",\n            \"mmhg\",\n            \"bpm\",\n            \"gfr\",\n            \"hba1c\",\n            \"ldl\",\n            \"hdl\",\n            \"icd\",\n            \"cpt\",\n            \"icd-10\",\n            \"snomed\",\n            \"rxnorm\",\n        ]\n        has_medical_terms = any(indicator in query.lower() for indicator in medical_indicators)\n\n        if has_medical_terms:\n            return SearchStrategy.HYBRID\n\n        # Question queries - vector search is usually better\n        question_words = [\"what\", \"how\", \"why\", \"when\", \"where\", \"which\", \"explain\"]\n        is_question = any(query.lower().startswith(q) for q in question_words)\n\n        if is_question and len(tokens) > 5:\n            return SearchStrategy.VECTOR_ONLY\n\n        return SearchStrategy.HYBRID\n\n    async def search(\n        self,\n        query: str,\n        top_k: int = 10,\n        strategy: SearchStrategy = SearchStrategy.AUTO,\n        score_threshold: Optional[float] = None,\n        filters: Optional[Dict[str, Any]] = None,\n    ) -> List[SearchResult]:\n        \"\"\"\n        Perform hybrid search.\n\n        Args:\n            query: Search query\n            top_k: Number of results to return\n            strategy: Search strategy to use\n            score_threshold: Minimum score threshold\n\n        Returns:\n            List of search results\n        \"\"\"\n        threshold = score_threshold or self.config.min_score_threshold\n\n        # Auto-select strategy if requested\n        if strategy == SearchStrategy.AUTO:\n            strategy = self._select_strategy(query)\n            logger.debug(f\"Auto-selected search strategy: {strategy}\")\n\n        # Execute search based on strategy\n        if strategy == SearchStrategy.VECTOR_ONLY:\n            results = await self._vector_search(query, top_k, threshold, filters)\n\n        elif strategy == SearchStrategy.BM25_ONLY:\n            results = await self._bm25_search(query, top_k, filters)\n            results = [r for r in results if r.score >= threshold]\n\n        elif strategy == SearchStrategy.HYBRID:\n            # Execute both searches in parallel\n            vector_task = self._vector_search(query, top_k * 2, 0.0, filters)\n            bm25_task = self._bm25_search(query, top_k * 2, filters)\n\n            vector_results, bm25_results = await asyncio.gather(vector_task, bm25_task)\n\n            # Use RRF for fusion\n            results = self._reciprocal_rank_fusion(\n                [vector_results, bm25_results],\n                k=self.config.rrf_k,\n            )\n\n            # Apply threshold filter\n            results = [r for r in results if r.score >= threshold]\n\n        else:\n            results = []\n\n        # Limit to top_k\n        return results[:top_k]\n\n    async def search_with_expansion(\n        self,\n        query: str,\n        top_k: int = 10,\n        expand_query: bool = True,\n        filters: Optional[Dict[str, Any]] = None,\n    ) -> List[SearchResult]:\n        \"\"\"\n        Search with optional query expansion.\n\n        Query expansion adds related terms to improve recall.\n        \"\"\"\n        queries = [query]\n\n        if expand_query:\n            # Simple query expansion using synonyms/related terms\n            # In production, this would use a medical thesaurus or LLM\n            expanded = await self._expand_query(query)\n            if expanded:\n                queries.append(expanded)\n\n        # Search with all queries and merge results\n        all_results = []\n        for q in queries:\n            results = await self.search(q, top_k=top_k, filters=filters)\n            all_results.append(results)\n\n        if len(all_results) == 1:\n            return all_results[0]\n\n        # Merge using RRF\n        return self._reciprocal_rank_fusion(all_results)[:top_k]\n\n    async def _expand_query(self, query: str) -> Optional[str]:\n        \"\"\"\n        Expand query with related terms.\n\n        Uses simple heuristics for MVP. Could be enhanced with:\n        - Medical thesaurus (UMLS)\n        - LLM-based expansion\n        - Query reformulation\n        \"\"\"\n        # Medical abbreviation expansions\n        expansions = {\n            \"mi\": \"myocardial infarction heart attack\",\n            \"cad\": \"coronary artery disease\",\n            \"chf\": \"congestive heart failure\",\n            \"copd\": \"chronic obstructive pulmonary disease\",\n            \"dm\": \"diabetes mellitus\",\n            \"htn\": \"hypertension high blood pressure\",\n            \"ckd\": \"chronic kidney disease\",\n            \"afib\": \"atrial fibrillation\",\n            \"pe\": \"pulmonary embolism\",\n            \"dvt\": \"deep vein thrombosis\",\n            \"uti\": \"urinary tract infection\",\n            \"cva\": \"cerebrovascular accident stroke\",\n            \"tia\": \"transient ischemic attack\",\n            \"gerd\": \"gastroesophageal reflux disease\",\n            \"ra\": \"rheumatoid arthritis\",\n            \"oa\": \"osteoarthritis\",\n            \"nsaid\": \"non-steroidal anti-inflammatory drug\",\n            \"ace\": \"angiotensin converting enzyme inhibitor\",\n            \"arb\": \"angiotensin receptor blocker\",\n            \"ssri\": \"selective serotonin reuptake inhibitor\",\n        }\n\n        query_lower = query.lower()\n        expanded_terms = []\n\n        for abbrev, expansion in expansions.items():\n            if abbrev in query_lower.split():\n                expanded_terms.append(expansion)\n\n        if expanded_terms:\n            return query + \" \" + \" \".join(expanded_terms)\n\n        return None\n"
}
