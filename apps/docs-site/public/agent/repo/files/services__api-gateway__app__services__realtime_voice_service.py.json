{
  "path": "services/api-gateway/app/services/realtime_voice_service.py",
  "language": "python",
  "size": 34954,
  "last_modified": "2025-12-04T11:27:00.448Z",
  "lines": 941,
  "content": "\"\"\"\nOpenAI Realtime API Integration Service\n\nDEPRECATED: This module is LEGACY and maintained for backwards compatibility only.\nThe Thinker/Talker pipeline (Deepgram STT + GPT-4o + ElevenLabs TTS) is now the\nprimary voice implementation. See voice_pipeline_service.py for the current approach.\n\nThis service was used to generate ephemeral tokens and session configuration for\nOpenAI's Realtime API voice mode.\n\nHistorical functionality:\n1. OpenAI Realtime API session configuration\n2. Provider abstraction for future STT/TTS integrations\n3. Safe provider config (never exposes raw API keys)\n\nNote: The AdaptiveVADManager in this file may still be used for VAD tuning\nacross both implementations.\n\"\"\"\n\nimport asyncio\nimport base64\nimport hashlib\nimport hmac\nimport json\nimport secrets\nimport time\nfrom dataclasses import dataclass, field\nfrom typing import Any, Dict, Optional\n\nimport httpx\nfrom app.core.config import settings\nfrom app.core.logging import get_logger\n\nlogger = get_logger(__name__)\n\n\n# ==============================================================================\n# Adaptive VAD System\n# ==============================================================================\n\n\n@dataclass\nclass UserVADProfile:\n    \"\"\"Tracks user's speech patterns for adaptive VAD tuning.\"\"\"\n\n    user_id: str\n    # Speech pattern metrics (rolling averages)\n    avg_pause_duration_ms: float = 500.0  # Average pause between utterances\n    avg_utterance_duration_ms: float = 3000.0  # Average utterance length\n    speech_rate_wpm: float = 150.0  # Words per minute estimate\n    # Adaptive parameters\n    optimal_silence_duration_ms: int = 500  # Calculated optimal silence threshold\n    # Statistics\n    total_sessions: int = 0\n    total_utterances: int = 0\n    last_updated: float = field(default_factory=time.time)\n\n    def update_from_session(\n        self,\n        pause_durations: list[float],\n        utterance_durations: list[float],\n    ) -> None:\n        \"\"\"Update profile from session metrics using exponential moving average.\"\"\"\n        if pause_durations:\n            # Use EMA with alpha=0.5 for faster adaptation (aggressive latency optimization)\n            alpha = 0.5\n            avg_pause = sum(pause_durations) / len(pause_durations)\n            self.avg_pause_duration_ms = alpha * avg_pause + (1 - alpha) * self.avg_pause_duration_ms\n\n        if utterance_durations:\n            alpha = 0.5\n            avg_utt = sum(utterance_durations) / len(utterance_durations)\n            self.avg_utterance_duration_ms = alpha * avg_utt + (1 - alpha) * self.avg_utterance_duration_ms\n\n        # Calculate optimal silence duration based on user patterns\n        # Users with shorter pauses get more aggressive detection\n        # Clamp between 200ms (very fast speakers) and 800ms (thoughtful speakers)\n        self.optimal_silence_duration_ms = int(max(200, min(800, self.avg_pause_duration_ms * 0.6)))\n\n        self.total_sessions += 1\n        self.total_utterances += len(utterance_durations)\n        self.last_updated = time.time()\n\n\nclass AdaptiveVADManager:\n    \"\"\"\n    Manages user VAD profiles for adaptive turn detection.\n\n    Starts with aggressive settings (200ms silence) for low latency and learns user\n    patterns over time. Uses EMA with alpha=0.5 for faster adaptation.\n    \"\"\"\n\n    def __init__(self):\n        self._profiles: Dict[str, UserVADProfile] = {}\n        self._lock = asyncio.Lock()\n        self._default_silence_ms = 200  # Aggressive default for low latency\n        self._min_silence_ms = 150  # Minimum for fast speakers\n        self._max_silence_ms = 800  # Maximum for thoughtful speakers\n\n    async def get_optimal_silence_duration(\n        self,\n        user_id: str,\n        requested_silence_ms: Optional[int] = None,\n        adaptive_enabled: bool = True,\n    ) -> int:\n        \"\"\"\n        Get optimal silence duration for a user.\n\n        Args:\n            user_id: User identifier\n            requested_silence_ms: Explicit user preference (overrides adaptive)\n            adaptive_enabled: Whether to use adaptive learning\n\n        Returns:\n            Optimal silence duration in milliseconds\n        \"\"\"\n        # If user explicitly requested a value, respect it\n        if requested_silence_ms is not None:\n            return max(self._min_silence_ms, min(self._max_silence_ms, requested_silence_ms))\n\n        # If adaptive is disabled, use default\n        if not adaptive_enabled:\n            return self._default_silence_ms\n\n        # Get or create user profile\n        async with self._lock:\n            profile = self._profiles.get(user_id)\n\n            if profile is None:\n                # New user - start conservative\n                profile = UserVADProfile(user_id=user_id)\n                self._profiles[user_id] = profile\n                logger.debug(f\"Created new VAD profile for user {user_id}\")\n                return self._default_silence_ms\n\n            # Return learned optimal value\n            return profile.optimal_silence_duration_ms\n\n    async def update_user_profile(\n        self,\n        user_id: str,\n        pause_durations_ms: list[float],\n        utterance_durations_ms: list[float],\n    ) -> None:\n        \"\"\"\n        Update user's VAD profile from session metrics.\n\n        Called at end of voice session with collected timing data.\n        \"\"\"\n        async with self._lock:\n            if user_id not in self._profiles:\n                self._profiles[user_id] = UserVADProfile(user_id=user_id)\n\n            profile = self._profiles[user_id]\n            profile.update_from_session(pause_durations_ms, utterance_durations_ms)\n\n            logger.info(\n                f\"Updated VAD profile for user {user_id}\",\n                extra={\n                    \"user_id\": user_id,\n                    \"optimal_silence_ms\": profile.optimal_silence_duration_ms,\n                    \"avg_pause_ms\": profile.avg_pause_duration_ms,\n                    \"total_sessions\": profile.total_sessions,\n                },\n            )\n\n    async def get_user_profile(self, user_id: str) -> Optional[UserVADProfile]:\n        \"\"\"Get user's VAD profile if it exists.\"\"\"\n        async with self._lock:\n            return self._profiles.get(user_id)\n\n    def get_stats(self) -> Dict[str, Any]:\n        \"\"\"Get manager statistics.\"\"\"\n        return {\n            \"total_profiles\": len(self._profiles),\n            \"profiles\": {\n                uid: {\n                    \"optimal_silence_ms\": p.optimal_silence_duration_ms,\n                    \"total_sessions\": p.total_sessions,\n                    \"avg_pause_ms\": p.avg_pause_duration_ms,\n                }\n                for uid, p in self._profiles.items()\n            },\n        }\n\n\n# Global adaptive VAD manager\nadaptive_vad_manager = AdaptiveVADManager()\n\n\n@dataclass\nclass TTSProviderConfig:\n    \"\"\"Configuration for Text-to-Speech providers.\n\n    This data class provides metadata about TTS providers without exposing\n    sensitive API keys. Used for frontend feature detection and backend routing.\n    \"\"\"\n\n    provider: str  # e.g., \"openai\", \"elevenlabs\", \"azure\", \"gcp\"\n    enabled: bool  # Whether this provider is configured and available\n    api_key_present: bool  # Whether API key is configured (but not the key itself)\n    default_voice: Optional[str] = None  # Default voice ID for this provider\n    available_voices: Optional[list[str]] = None  # List of available voice IDs\n    supports_streaming: bool = True  # Whether provider supports streaming audio\n    max_text_length: Optional[int] = None  # Max characters per request\n\n\n@dataclass\nclass STTProviderConfig:\n    \"\"\"Configuration for Speech-to-Text providers.\n\n    This data class provides metadata about STT providers without exposing\n    sensitive API keys. Used for frontend feature detection and backend routing.\n    \"\"\"\n\n    provider: str  # e.g., \"openai\", \"deepgram\", \"azure\", \"gcp\"\n    enabled: bool  # Whether this provider is configured and available\n    api_key_present: bool  # Whether API key is configured (but not the key itself)\n    supports_streaming: bool = True  # Whether provider supports streaming audio\n    supports_interim_results: bool = True  # Whether provider supports partial transcripts\n    supported_languages: Optional[list[str]] = None  # Supported language codes\n    max_audio_duration_sec: Optional[int] = None  # Max audio duration in seconds\n\n\n@dataclass\nclass CachedSession:\n    \"\"\"Cached OpenAI ephemeral session for reuse.\"\"\"\n\n    client_secret: str\n    expires_at: int\n    voice: str\n    created_at: float = field(default_factory=time.time)\n\n\nclass RealtimeVoiceService:\n    \"\"\"\n    Service for managing OpenAI Realtime API voice sessions.\n\n    The Realtime API uses WebSocket connections for bidirectional streaming\n    of audio and text. This service generates ephemeral session configuration\n    that the frontend uses to establish WebSocket connections.\n\n    Phase 11 Optimizations:\n    - Session caching: Reuse ephemeral tokens if not expiring within 60s\n    - Connection pooling: Persistent HTTP client with keep-alive\n    - HTTP/2: Multiplexed connections for faster token generation\n    \"\"\"\n\n    def __init__(self):\n        self.enabled = settings.REALTIME_ENABLED\n        self.model = settings.REALTIME_MODEL\n        self.base_url = settings.REALTIME_BASE_URL\n        self.api_key = settings.OPENAI_API_KEY\n        self.token_expiry = settings.REALTIME_TOKEN_EXPIRY_SEC\n\n        # Phase 11: Session caching for latency reduction\n        self._session_cache: Dict[str, CachedSession] = {}\n        self._cache_lock = asyncio.Lock()\n        self._cache_expiry_buffer_sec = 60  # Don't reuse if expiring within 60s\n\n        # Phase 11: Persistent HTTP client with connection pooling\n        # HTTP/2 enabled for multiplexed connections to OpenAI\n        self._http_client: Optional[httpx.AsyncClient] = None\n\n    def is_enabled(self) -> bool:\n        \"\"\"Check if Realtime API is enabled and configured\"\"\"\n        return self.enabled and bool(self.api_key)\n\n    async def _get_http_client(self) -> httpx.AsyncClient:\n        \"\"\"\n        Get or create persistent HTTP client with connection pooling.\n\n        Phase 11: Uses HTTP/2 for multiplexed connections, reducing\n        connection establishment overhead for repeated requests.\n\n        Returns:\n            Persistent AsyncClient instance\n        \"\"\"\n        if self._http_client is None or self._http_client.is_closed:\n            # Create client with HTTP/2 support and connection pooling\n            self._http_client = httpx.AsyncClient(\n                timeout=30.0,\n                http2=True,  # Enable HTTP/2 for multiplexing\n                limits=httpx.Limits(\n                    max_keepalive_connections=10,\n                    max_connections=20,\n                    keepalive_expiry=30.0,  # Keep connections alive for 30s\n                ),\n            )\n            logger.debug(\"Created persistent HTTP client with HTTP/2 and connection pooling\")\n        return self._http_client\n\n    def _get_cache_key(self, voice: str) -> str:\n        \"\"\"Generate cache key for session lookup.\"\"\"\n        return f\"{self.model}:{voice}\"\n\n    async def _get_cached_session(self, voice: str) -> Optional[CachedSession]:\n        \"\"\"\n        Get a cached session if available and not expiring soon.\n\n        Phase 11: Reuses ephemeral tokens if they have more than 60s remaining.\n        This avoids redundant OpenAI API calls for rapid pre-warm scenarios.\n\n        Returns:\n            Cached session if valid, None otherwise\n        \"\"\"\n        cache_key = self._get_cache_key(voice)\n        current_time = int(time.time())\n\n        async with self._cache_lock:\n            cached = self._session_cache.get(cache_key)\n            if cached is None:\n                return None\n\n            # Check if session is expiring within buffer period\n            time_remaining = cached.expires_at - current_time\n            if time_remaining < self._cache_expiry_buffer_sec:\n                # Session expiring soon, remove from cache\n                del self._session_cache[cache_key]\n                logger.debug(\n                    f\"Cache miss: session expiring in {time_remaining}s\",\n                    extra={\"voice\": voice, \"time_remaining\": time_remaining},\n                )\n                return None\n\n            logger.info(\n                f\"Cache hit: reusing session (expires in {time_remaining}s)\",\n                extra={\"voice\": voice, \"time_remaining\": time_remaining},\n            )\n            return cached\n\n    async def _cache_session(self, voice: str, session: CachedSession) -> None:\n        \"\"\"Store a session in the cache.\"\"\"\n        cache_key = self._get_cache_key(voice)\n        async with self._cache_lock:\n            self._session_cache[cache_key] = session\n            logger.debug(\n                f\"Cached session for voice {voice}\",\n                extra={\"voice\": voice, \"expires_at\": session.expires_at},\n            )\n\n    async def create_openai_ephemeral_session(self, model: str, voice: str = \"alloy\") -> Dict[str, Any]:\n        \"\"\"\n        Create an ephemeral session with OpenAI's Realtime API.\n\n        This calls OpenAI's session creation endpoint to get a real ephemeral\n        client secret that can be used to connect to the Realtime WebSocket.\n\n        Phase 11 Optimizations:\n        - Checks cache for valid session before making API call\n        - Uses persistent HTTP client with connection pooling\n        - HTTP/2 for multiplexed connections\n\n        Args:\n            model: The Realtime model to use (e.g., \"gpt-4o-realtime-preview-2024-12-17\")\n            voice: The voice to use (default: \"alloy\")\n\n        Returns:\n            Dict containing:\n            - client_secret: Ephemeral token for client use\n            - expires_at: Unix timestamp when token expires\n            - cached: Whether this was served from cache (Phase 11)\n\n        Raises:\n            ValueError: If session creation fails\n        \"\"\"\n        # Phase 11: Check cache first\n        cached_session = await self._get_cached_session(voice)\n        if cached_session:\n            return {\n                \"client_secret\": cached_session.client_secret,\n                \"expires_at\": cached_session.expires_at,\n                \"cached\": True,\n            }\n\n        try:\n            # Phase 11: Use persistent HTTP client instead of creating new one\n            client = await self._get_http_client()\n            response = await client.post(\n                \"https://api.openai.com/v1/realtime/sessions\",\n                headers={\n                    \"Authorization\": f\"Bearer {self.api_key}\",\n                    \"Content-Type\": \"application/json\",\n                },\n                json={\n                    \"model\": model,\n                    \"voice\": voice,\n                },\n            )\n\n            if response.status_code != 200:\n                error_detail = response.text\n                logger.error(\n                    f\"OpenAI session creation failed: {error_detail}\",\n                    extra={\n                        \"status_code\": response.status_code,\n                        \"response\": error_detail,\n                    },\n                )\n                raise ValueError(f\"Failed to create OpenAI session: {response.status_code}\")\n\n            data = response.json()\n\n            # OpenAI returns: { \"client_secret\": { \"value\": \"ek_...\", \"expires_at\": timestamp } }\n            client_secret_data = data.get(\"client_secret\", {})\n            client_secret = client_secret_data.get(\"value\")\n            expires_at = client_secret_data.get(\"expires_at\")\n\n            if not client_secret:\n                raise ValueError(\"OpenAI did not return a client_secret\")\n\n            # Phase 11: Cache the session for future requests\n            await self._cache_session(\n                voice,\n                CachedSession(\n                    client_secret=client_secret,\n                    expires_at=expires_at,\n                    voice=voice,\n                ),\n            )\n\n            logger.info(\n                \"Created OpenAI ephemeral session\",\n                extra={\n                    \"model\": model,\n                    \"expires_at\": expires_at,\n                    \"cached\": False,\n                },\n            )\n\n            return {\n                \"client_secret\": client_secret,\n                \"expires_at\": expires_at,\n                \"cached\": False,\n            }\n\n        except httpx.TimeoutException as e:\n            logger.error(f\"OpenAI session creation timeout: {str(e)}\")\n            raise ValueError(\"OpenAI session creation timed out\")\n        except httpx.HTTPError as e:\n            logger.error(f\"OpenAI session creation HTTP error: {str(e)}\")\n            raise ValueError(f\"OpenAI session creation failed: {str(e)}\")\n        except Exception as e:\n            logger.error(f\"OpenAI session creation error: {str(e)}\")\n            raise ValueError(f\"Failed to create OpenAI session: {str(e)}\")\n\n    def generate_ephemeral_token(self, user_id: str, session_id: str, expires_at: int) -> str:\n        \"\"\"\n        Generate an HMAC-signed ephemeral token for voice session.\n\n        This token is sent to the client instead of the raw OpenAI API key.\n        It encodes the user_id, session_id, model, and expiry time, and is\n        signed with the JWT_SECRET to prevent tampering.\n\n        Args:\n            user_id: User ID for this session\n            session_id: Unique session identifier\n            expires_at: Unix timestamp when token expires\n\n        Returns:\n            Base64-encoded token string containing payload + signature\n        \"\"\"\n        # Build token payload\n        payload = {\n            \"user_id\": user_id,\n            \"session_id\": session_id,\n            \"model\": self.model,\n            \"expires_at\": expires_at,\n            \"issued_at\": int(time.time()),\n        }\n\n        # Serialize payload to JSON\n        payload_json = json.dumps(payload, separators=(\",\", \":\"), sort_keys=True)\n        payload_b64 = base64.urlsafe_b64encode(payload_json.encode()).decode()\n\n        # Generate HMAC signature using JWT_SECRET\n        signature = hmac.new(\n            settings.JWT_SECRET.encode(),\n            payload_b64.encode(),\n            hashlib.sha256,\n        ).digest()\n        signature_b64 = base64.urlsafe_b64encode(signature).decode()\n\n        # Combine payload and signature\n        token = f\"{payload_b64}.{signature_b64}\"\n\n        logger.debug(\n            f\"Generated ephemeral token for user {user_id}\",\n            extra={\n                \"user_id\": user_id,\n                \"session_id\": session_id,\n                \"expires_at\": expires_at,\n            },\n        )\n\n        return token\n\n    def validate_ephemeral_token(self, token: str) -> Dict[str, Any]:\n        \"\"\"\n        Validate and decode an ephemeral token.\n\n        Args:\n            token: Token string to validate\n\n        Returns:\n            Decoded payload dict\n\n        Raises:\n            ValueError: If token is invalid, expired, or tampered with\n        \"\"\"\n        try:\n            # Split token into payload and signature\n            parts = token.split(\".\")\n            if len(parts) != 2:\n                raise ValueError(\"Invalid token format\")\n\n            payload_b64, signature_b64 = parts\n\n            # Verify signature\n            expected_signature = hmac.new(\n                settings.JWT_SECRET.encode(),\n                payload_b64.encode(),\n                hashlib.sha256,\n            ).digest()\n            expected_signature_b64 = base64.urlsafe_b64encode(expected_signature).decode()\n\n            if not hmac.compare_digest(signature_b64, expected_signature_b64):\n                raise ValueError(\"Invalid token signature\")\n\n            # Decode payload\n            payload_json = base64.urlsafe_b64decode(payload_b64).decode()\n            payload = json.loads(payload_json)\n\n            # Check expiry\n            if payload[\"expires_at\"] < int(time.time()):\n                raise ValueError(\"Token expired\")\n\n            return payload\n\n        except Exception as e:\n            logger.warning(\n                f\"Token validation failed: {str(e)}\",\n                extra={\"error\": str(e)},\n            )\n            raise ValueError(f\"Invalid token: {str(e)}\")\n\n    async def generate_session_config(\n        self,\n        user_id: str,\n        conversation_id: str | None = None,\n        voice: str | None = None,\n        language: str | None = None,\n        vad_sensitivity: int | None = None,\n        enable_noise_suppression: bool | None = None,\n        silence_duration_ms: int | None = None,\n        adaptive_vad: bool = True,\n    ) -> Dict[str, Any]:\n        \"\"\"\n        Generate session configuration for OpenAI Realtime API.\n\n        This method creates a real ephemeral session with OpenAI and returns\n        the configuration needed for the client to connect via WebSocket.\n\n        Args:\n            user_id: User ID for the session\n            conversation_id: Optional conversation ID to resume\n            voice: Optional voice selection (alloy, echo, fable, onyx, nova, shimmer)\n            language: Optional language code (en, es, fr, de, it, pt)\n            vad_sensitivity: Optional VAD sensitivity 0-100 (maps to threshold)\n            silence_duration_ms: Optional explicit silence duration (200-800ms)\n            adaptive_vad: Whether to use adaptive VAD based on user patterns\n\n        Returns:\n            Dictionary with session configuration including:\n            - url: WebSocket URL for Realtime API\n            - model: Realtime model to use\n            - auth: Authentication structure with OpenAI ephemeral token\n            - session_id: Unique session identifier\n            - expires_at: Unix timestamp when session expires\n            - voice_config: Voice configuration with user preferences\n\n        Raises:\n            ValueError: If Realtime API is not enabled or configured\n        \"\"\"\n        if not self.is_enabled():\n            raise ValueError(\"Realtime API is not enabled or OpenAI API key not configured\")\n\n        # Validate and select voice\n        valid_voices = [\"alloy\", \"echo\", \"fable\", \"onyx\", \"nova\", \"shimmer\"]\n        selected_voice = voice if voice in valid_voices else \"alloy\"\n\n        # Map VAD sensitivity (0-100) to threshold (0.1-0.9)\n        # High sensitivity (100) => low threshold (0.1) - picks up more\n        # Low sensitivity (0) => high threshold (0.9) - needs louder sound\n        if vad_sensitivity is not None:\n            vad_sensitivity = max(0, min(100, vad_sensitivity))\n            vad_threshold = 0.9 - (vad_sensitivity / 100.0 * 0.8)\n        else:\n            vad_threshold = 0.5  # Default threshold\n\n        # Get optimal silence duration from adaptive VAD manager\n        optimal_silence_ms = await adaptive_vad_manager.get_optimal_silence_duration(\n            user_id=user_id,\n            requested_silence_ms=silence_duration_ms,\n            adaptive_enabled=adaptive_vad,\n        )\n\n        # Aggressive prefix padding for low latency\n        # Fixed at 150ms for all users (was 200-300ms adaptive)\n        prefix_padding_ms = 150\n\n        # Determine noise suppression preference (defaults to settings or True)\n        if enable_noise_suppression is None:\n            enable_noise_suppression = True\n\n        # Generate unique session ID for tracking\n        session_id = f\"rtc_{user_id}_{secrets.token_urlsafe(16)}\"\n\n        # Create ephemeral session with OpenAI\n        # This gets us a real client_secret that works with the Realtime API\n        openai_session = await self.create_openai_ephemeral_session(\n            model=self.model,\n            voice=selected_voice,\n        )\n\n        client_secret = openai_session[\"client_secret\"]\n        expires_at = openai_session[\"expires_at\"]\n\n        # Build session configuration\n        # NOTE: We use the real OpenAI ephemeral token, NOT the raw API key\n        config = {\n            \"url\": self.base_url,\n            \"model\": self.model,\n            \"session_id\": session_id,\n            \"expires_at\": expires_at,\n            \"conversation_id\": conversation_id,\n            \"auth\": {\n                \"type\": \"ephemeral_token\",\n                \"token\": client_secret,  # Real OpenAI ephemeral client secret\n                \"expires_at\": expires_at,\n            },\n            \"voice_config\": {\n                \"voice\": selected_voice,\n                \"language\": language,  # Kept as metadata for now\n                \"modalities\": [\"text\", \"audio\"],  # Both text and audio\n                \"input_audio_format\": \"pcm16\",  # 16-bit PCM\n                \"output_audio_format\": \"pcm16\",\n                \"input_audio_transcription\": {\"model\": \"whisper-1\"},\n                \"turn_detection\": {\n                    \"type\": \"server_vad\",  # Server-side VAD\n                    \"threshold\": round(vad_threshold, 2),\n                    \"prefix_padding_ms\": prefix_padding_ms,\n                    \"silence_duration_ms\": optimal_silence_ms,\n                },\n            },\n            \"audio_enhancements\": {\n                \"noise_suppression\": enable_noise_suppression,\n                \"vad_threshold\": round(vad_threshold, 2),\n            },\n            \"adaptive_vad\": {\n                \"enabled\": adaptive_vad,\n                \"silence_duration_ms\": optimal_silence_ms,\n                \"prefix_padding_ms\": prefix_padding_ms,\n                \"is_learned\": silence_duration_ms is None and adaptive_vad,\n            },\n        }\n\n        logger.info(\n            \"Generated Realtime session config\",\n            extra={\n                \"user_id\": user_id,\n                \"session_id\": session_id,\n                \"conversation_id\": conversation_id,\n                \"expires_at\": expires_at,\n                \"voice\": selected_voice,\n                \"language\": language,\n                \"vad_threshold\": round(vad_threshold, 2),\n                \"silence_duration_ms\": optimal_silence_ms,\n                \"adaptive_vad\": adaptive_vad,\n            },\n        )\n\n        return config\n\n    def get_api_key_for_token(self, token: str) -> str:\n        \"\"\"\n        Validate an ephemeral token and return the real OpenAI API key.\n\n        This method is used server-side (e.g., in a WebSocket proxy) to validate\n        the client's token and retrieve the actual API key for making OpenAI calls.\n\n        Args:\n            token: Ephemeral token to validate\n\n        Returns:\n            The real OpenAI API key\n\n        Raises:\n            ValueError: If token is invalid or expired\n        \"\"\"\n        # Validate token (will raise ValueError if invalid/expired)\n        _payload = self.validate_ephemeral_token(token)  # noqa: F841\n\n        # Token is valid, return the real API key\n        # In a future proxy implementation, this would be used to authenticate\n        # backend-to-OpenAI requests\n        return self.api_key\n\n    def validate_session(self, session_id: str) -> bool:\n        \"\"\"\n        Validate a session ID (basic check).\n\n        Args:\n            session_id: Session ID to validate\n\n        Returns:\n            True if session format is valid, False otherwise\n        \"\"\"\n        if not session_id or not isinstance(session_id, str):\n            return False\n\n        # Basic format check: should start with \"rtc_\"\n        if not session_id.startswith(\"rtc_\"):\n            return False\n\n        # Check length (should be: rtc_ + user_id + _ + token)\n        parts = session_id.split(\"_\")\n        if len(parts) < 3:\n            return False\n\n        return True\n\n    # Default voice instructions for fallback\n    _DEFAULT_VOICE_INSTRUCTIONS = \"\"\"You are a helpful medical AI assistant in voice mode.\n\nGuidelines:\n- Keep responses concise and conversational\n- Use natural spoken language, not written text\n- Ask clarifying questions when needed\n- Be empathetic and professional\n- Cite sources when providing medical information\n- Maintain HIPAA compliance at all times\n\nWhen speaking:\n- Use short sentences\n- Avoid complex medical jargon unless requested\n- Confirm understanding before proceeding\n- Offer to provide more details if needed\n\"\"\"\n\n    async def get_session_instructions_async(\n        self, conversation_id: str | None = None, persona: str | None = None\n    ) -> str:\n        \"\"\"\n        Get system instructions for the Realtime session with dynamic lookup.\n\n        Uses the PromptService for dynamic prompt management with fallback\n        to default instructions if the dynamic lookup fails.\n\n        Args:\n            conversation_id: Optional conversation ID for context\n            persona: Optional persona name to use\n\n        Returns:\n            System instructions string\n        \"\"\"\n        try:\n            # Import here to avoid circular imports\n            from app.services.prompt_service import prompt_service\n\n            # Try dynamic prompt lookup\n            instructions = await prompt_service.get_voice_instructions(persona=persona, conversation_id=conversation_id)\n            if instructions:\n                return instructions\n        except Exception as e:\n            logger.warning(f\"Failed to get dynamic voice instructions: {e}\")\n\n        # Fallback to default\n        instructions = self._DEFAULT_VOICE_INSTRUCTIONS\n        if conversation_id:\n            instructions += f\"\\nResuming conversation: {conversation_id}\"\n        return instructions\n\n    def get_session_instructions(self, conversation_id: str | None = None) -> str:\n        \"\"\"\n        Get system instructions for the Realtime session (synchronous fallback).\n\n        Note: Prefer using get_session_instructions_async() for async contexts.\n        This method exists for backward compatibility.\n\n        Args:\n            conversation_id: Optional conversation ID for context\n\n        Returns:\n            System instructions string\n        \"\"\"\n        instructions = self._DEFAULT_VOICE_INSTRUCTIONS\n\n        if conversation_id:\n            instructions += f\"\\nResuming conversation: {conversation_id}\"\n\n        return instructions\n\n    def get_tts_config(self) -> TTSProviderConfig:\n        \"\"\"\n        Get TTS provider configuration.\n\n        Returns metadata about the configured TTS provider without exposing\n        sensitive API keys. Used for feature detection and routing.\n\n        Returns:\n            TTSProviderConfig with provider metadata\n        \"\"\"\n        provider = settings.TTS_PROVIDER or \"openai\"  # Default to OpenAI\n\n        if provider == \"openai\":\n            # OpenAI TTS (using main OpenAI key)\n            return TTSProviderConfig(\n                provider=\"openai\",\n                enabled=bool(settings.OPENAI_API_KEY),\n                api_key_present=bool(settings.OPENAI_API_KEY),\n                default_voice=settings.TTS_VOICE or \"alloy\",\n                available_voices=[\"alloy\", \"echo\", \"fable\", \"onyx\", \"nova\", \"shimmer\"],\n                supports_streaming=True,\n                max_text_length=4096,\n            )\n\n        elif provider == \"elevenlabs\":\n            # ElevenLabs TTS\n            return TTSProviderConfig(\n                provider=\"elevenlabs\",\n                enabled=bool(settings.ELEVENLABS_API_KEY),\n                api_key_present=bool(settings.ELEVENLABS_API_KEY),\n                default_voice=settings.TTS_VOICE,\n                available_voices=None,  # Would query API for available voices\n                supports_streaming=True,\n                max_text_length=5000,\n            )\n\n        else:\n            # Unknown/unsupported provider\n            logger.warning(f\"Unsupported TTS provider: {provider}\")\n            return TTSProviderConfig(\n                provider=provider,\n                enabled=False,\n                api_key_present=False,\n                supports_streaming=False,\n            )\n\n    def get_stt_config(self) -> STTProviderConfig:\n        \"\"\"\n        Get STT provider configuration.\n\n        Returns metadata about the configured STT provider without exposing\n        sensitive API keys. Used for feature detection and routing.\n\n        Returns:\n            STTProviderConfig with provider metadata\n        \"\"\"\n        provider = settings.STT_PROVIDER or \"openai\"  # Default to OpenAI\n\n        if provider == \"openai\":\n            # OpenAI Whisper (using main OpenAI key)\n            return STTProviderConfig(\n                provider=\"openai\",\n                enabled=bool(settings.OPENAI_API_KEY),\n                api_key_present=bool(settings.OPENAI_API_KEY),\n                supports_streaming=False,  # Whisper API is batch-only\n                supports_interim_results=False,\n                supported_languages=[\n                    \"en\",\n                    \"es\",\n                    \"fr\",\n                    \"de\",\n                    \"it\",\n                    \"pt\",\n                    \"nl\",\n                    \"pl\",\n                    \"ru\",\n                    \"ja\",\n                    \"ko\",\n                    \"zh\",\n                ],  # Whisper supports 99+ languages\n                max_audio_duration_sec=None,  # No hard limit\n            )\n\n        elif provider == \"deepgram\":\n            # Deepgram STT\n            return STTProviderConfig(\n                provider=\"deepgram\",\n                enabled=bool(settings.DEEPGRAM_API_KEY),\n                api_key_present=bool(settings.DEEPGRAM_API_KEY),\n                supports_streaming=True,  # Deepgram supports streaming\n                supports_interim_results=True,\n                supported_languages=[\"en\", \"es\", \"fr\", \"de\", \"it\", \"pt\", \"nl\", \"ja\"],\n                max_audio_duration_sec=None,  # No hard limit\n            )\n\n        else:\n            # Unknown/unsupported provider\n            logger.warning(f\"Unsupported STT provider: {provider}\")\n            return STTProviderConfig(\n                provider=provider,\n                enabled=False,\n                api_key_present=False,\n                supports_streaming=False,\n                supports_interim_results=False,\n            )\n\n    def get_available_providers(self) -> Dict[str, Any]:\n        \"\"\"\n        Get summary of all available voice providers.\n\n        Returns:\n            Dictionary with provider availability status\n        \"\"\"\n        return {\n            \"tts\": {\n                \"current\": settings.TTS_PROVIDER or \"openai\",\n                \"config\": {\n                    \"provider\": self.get_tts_config().provider,\n                    \"enabled\": self.get_tts_config().enabled,\n                },\n            },\n            \"stt\": {\n                \"current\": settings.STT_PROVIDER or \"openai\",\n                \"config\": {\n                    \"provider\": self.get_stt_config().provider,\n                    \"enabled\": self.get_stt_config().enabled,\n                },\n            },\n            \"realtime\": {\n                \"enabled\": self.is_enabled(),\n                \"model\": self.model if self.is_enabled() else None,\n            },\n        }\n\n\n# Global service instance\nrealtime_voice_service = RealtimeVoiceService()\n"
}
