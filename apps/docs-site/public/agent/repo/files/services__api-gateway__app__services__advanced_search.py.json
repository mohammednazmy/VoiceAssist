{
  "path": "services/api-gateway/app/services/advanced_search.py",
  "language": "python",
  "size": 14790,
  "last_modified": "2025-12-04T11:26:55.408Z",
  "lines": 456,
  "content": "\"\"\"\nAdvanced Search Aggregator (Phase 5 - Advanced RAG)\n\nUnified search service that orchestrates all advanced RAG components:\n- Hybrid search (BM25 + Vector)\n- Medical embeddings (PubMedBERT)\n- Re-ranking (Cross-encoder/Cohere)\n- Query expansion\n\nThis is the main entry point for advanced search operations.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport logging\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nfrom typing import Any, Dict, List, Optional\n\nfrom app.services.hybrid_search_service import HybridSearchConfig, HybridSearchService, SearchStrategy\nfrom app.services.medical_embeddings import EmbeddingConfig, MedicalEmbeddingService, MedicalModelType\nfrom app.services.query_expansion import QueryExpansionConfig, QueryExpansionService\nfrom app.services.reranking_service import RerankerConfig, RerankerType, RerankingService\n\nlogger = logging.getLogger(__name__)\n\n\nclass SearchMode(str, Enum):\n    \"\"\"Search modes with different precision/recall tradeoffs.\"\"\"\n\n    FAST = \"fast\"  # Quick results, lower precision\n    BALANCED = \"balanced\"  # Default balanced mode\n    PRECISE = \"precise\"  # High precision with re-ranking\n    COMPREHENSIVE = \"comprehensive\"  # Maximum recall with expansion\n\n\n@dataclass\nclass AdvancedSearchConfig:\n    \"\"\"Configuration for advanced search.\"\"\"\n\n    # Search mode\n    mode: SearchMode = SearchMode.BALANCED\n\n    # Hybrid search settings\n    enable_hybrid: bool = True\n    vector_weight: float = 0.6\n    bm25_weight: float = 0.4\n\n    # Re-ranking settings\n    enable_reranking: bool = True\n    reranker_type: RerankerType = RerankerType.COHERE\n\n    # Query expansion settings\n    enable_expansion: bool = True\n    expand_abbreviations: bool = True\n    expand_synonyms: bool = True\n    enable_llm_expansion: bool = False\n\n    # Medical embeddings\n    use_medical_embeddings: bool = False  # Requires GPU\n    medical_model: MedicalModelType = MedicalModelType.PUBMEDBERT\n\n    # Results\n    top_k: int = 10\n    min_score: float = 0.3\n    diversity_threshold: float = 0.8\n\n    # Caching\n    cache_ttl: int = 1800  # 30 minutes\n\n\n@dataclass\nclass AdvancedSearchResult:\n    \"\"\"Result from advanced search.\"\"\"\n\n    chunk_id: str\n    document_id: str\n    content: str\n    score: float\n    metadata: Dict[str, Any] = field(default_factory=dict)\n\n    # Search details\n    original_score: Optional[float] = None\n    rerank_score: Optional[float] = None\n    search_method: str = \"hybrid\"\n\n    # Citation info\n    title: Optional[str] = None\n    source_type: Optional[str] = None\n    url: Optional[str] = None\n\n\n@dataclass\nclass SearchMetrics:\n    \"\"\"Metrics for search operation.\"\"\"\n\n    total_time_ms: float = 0.0\n    embedding_time_ms: float = 0.0\n    search_time_ms: float = 0.0\n    rerank_time_ms: float = 0.0\n    expansion_time_ms: float = 0.0\n\n    results_found: int = 0\n    results_after_rerank: int = 0\n    query_expanded: bool = False\n    reranking_applied: bool = False\n\n\nclass AdvancedSearchAggregator:\n    \"\"\"\n    Advanced search aggregator with full RAG pipeline support.\n\n    Combines all Phase 5 components into a unified search interface.\n    \"\"\"\n\n    def __init__(\n        self,\n        config: Optional[AdvancedSearchConfig] = None,\n    ):\n        self.config = config or AdvancedSearchConfig()\n\n        # Initialize services based on config\n        self.hybrid_search = HybridSearchService(\n            config=HybridSearchConfig(\n                vector_weight=self.config.vector_weight,\n                bm25_weight=self.config.bm25_weight,\n            )\n        )\n\n        self.reranker = (\n            RerankingService(\n                config=RerankerConfig(\n                    reranker_type=self.config.reranker_type,\n                    top_n=self.config.top_k * 2,  # Re-rank more than we need\n                )\n            )\n            if self.config.enable_reranking\n            else None\n        )\n\n        self.query_expander = (\n            QueryExpansionService(\n                config=QueryExpansionConfig(\n                    enable_abbreviation=self.config.expand_abbreviations,\n                    enable_synonym=self.config.expand_synonyms,\n                    enable_llm=self.config.enable_llm_expansion,\n                )\n            )\n            if self.config.enable_expansion\n            else None\n        )\n\n        self.medical_embeddings = (\n            MedicalEmbeddingService(\n                config=EmbeddingConfig(\n                    model_type=self.config.medical_model,\n                )\n            )\n            if self.config.use_medical_embeddings\n            else None\n        )\n\n    async def search(\n        self,\n        query: str,\n        top_k: Optional[int] = None,\n        mode: Optional[SearchMode] = None,\n        filters: Optional[Dict[str, Any]] = None,\n    ) -> tuple[List[AdvancedSearchResult], SearchMetrics]:\n        \"\"\"\n        Perform advanced search with full RAG pipeline.\n\n        Args:\n            query: Search query\n            top_k: Number of results (default from config)\n            mode: Search mode (default from config)\n            filters: Optional metadata filters\n\n        Returns:\n            Tuple of (results, metrics)\n        \"\"\"\n        import time\n\n        start_time = time.time()\n        metrics = SearchMetrics()\n\n        top_k = top_k or self.config.top_k\n        mode = mode or self.config.mode\n\n        # Apply mode-specific settings\n        settings_for_mode = self._get_mode_settings(mode)\n\n        # Step 1: Query Expansion\n        expanded_query = query\n        if self.query_expander and settings_for_mode[\"expand\"]:\n            expansion_start = time.time()\n            expansion_result = await self.query_expander.expand(query)\n            expanded_query = expansion_result.expanded_query\n            metrics.expansion_time_ms = (time.time() - expansion_start) * 1000\n            metrics.query_expanded = expanded_query != query\n            logger.debug(f\"Expanded query: {expanded_query}\")\n\n        # Step 2: Search\n        search_start = time.time()\n\n        if settings_for_mode[\"hybrid\"]:\n            # Use hybrid search\n            strategy = SearchStrategy.HYBRID if settings_for_mode[\"use_bm25\"] else SearchStrategy.VECTOR_ONLY\n            search_results = await self.hybrid_search.search(\n                query=expanded_query,\n                top_k=top_k * 3,  # Get more for re-ranking\n                strategy=strategy,\n                filters=filters,\n            )\n        else:\n            # Fall back to vector-only search\n            search_results = await self.hybrid_search.search(\n                query=expanded_query,\n                top_k=top_k * 3,\n                strategy=SearchStrategy.VECTOR_ONLY,\n                filters=filters,\n            )\n\n        metrics.search_time_ms = (time.time() - search_start) * 1000\n        metrics.results_found = len(search_results)\n\n        # Step 3: Re-ranking\n        final_results = []\n        if self.reranker and settings_for_mode[\"rerank\"] and search_results:\n            rerank_start = time.time()\n\n            # Convert to dict format for reranker\n            results_dicts = [\n                {\n                    \"chunk_id\": r.chunk_id,\n                    \"document_id\": r.document_id,\n                    \"content\": r.content,\n                    \"score\": r.score,\n                    \"metadata\": r.metadata,\n                }\n                for r in search_results\n            ]\n\n            if settings_for_mode[\"diverse\"]:\n                reranked = await self.reranker.rerank_with_diversity(\n                    query=query,  # Use original query for reranking\n                    results=results_dicts,\n                    diversity_threshold=self.config.diversity_threshold,\n                )\n            else:\n                reranked = await self.reranker.rerank(\n                    query=query,\n                    results=results_dicts,\n                )\n\n            metrics.rerank_time_ms = (time.time() - rerank_start) * 1000\n            metrics.reranking_applied = True\n            metrics.results_after_rerank = len(reranked)\n\n            # Convert to AdvancedSearchResult\n            for r in reranked[:top_k]:\n                final_results.append(\n                    AdvancedSearchResult(\n                        chunk_id=r.chunk_id,\n                        document_id=r.document_id,\n                        content=r.content,\n                        score=r.final_score,\n                        metadata=r.metadata,\n                        original_score=r.original_score,\n                        rerank_score=r.rerank_score,\n                        search_method=\"hybrid_reranked\",\n                        title=r.metadata.get(\"title\"),\n                        source_type=r.metadata.get(\"source_type\"),\n                        url=r.metadata.get(\"url\"),\n                    )\n                )\n        else:\n            # No re-ranking - use search results directly\n            for r in search_results[:top_k]:\n                final_results.append(\n                    AdvancedSearchResult(\n                        chunk_id=r.chunk_id,\n                        document_id=r.document_id,\n                        content=r.content,\n                        score=r.score,\n                        metadata=r.metadata,\n                        search_method=r.source,\n                        title=r.metadata.get(\"title\"),\n                        source_type=r.metadata.get(\"source_type\"),\n                        url=r.metadata.get(\"url\"),\n                    )\n                )\n\n        # Filter by minimum score\n        final_results = [r for r in final_results if r.score >= self.config.min_score]\n\n        metrics.total_time_ms = (time.time() - start_time) * 1000\n\n        logger.info(\n            f\"Advanced search completed: \"\n            f\"query='{query[:50]}...', \"\n            f\"mode={mode.value}, \"\n            f\"results={len(final_results)}, \"\n            f\"time={metrics.total_time_ms:.0f}ms\"\n        )\n\n        return final_results, metrics\n\n    def _get_mode_settings(self, mode: SearchMode) -> Dict[str, bool]:\n        \"\"\"Get settings for search mode.\"\"\"\n        settings = {\n            SearchMode.FAST: {\n                \"expand\": False,\n                \"hybrid\": False,\n                \"use_bm25\": False,\n                \"rerank\": False,\n                \"diverse\": False,\n            },\n            SearchMode.BALANCED: {\n                \"expand\": True,\n                \"hybrid\": True,\n                \"use_bm25\": True,\n                \"rerank\": False,\n                \"diverse\": False,\n            },\n            SearchMode.PRECISE: {\n                \"expand\": True,\n                \"hybrid\": True,\n                \"use_bm25\": True,\n                \"rerank\": True,\n                \"diverse\": False,\n            },\n            SearchMode.COMPREHENSIVE: {\n                \"expand\": True,\n                \"hybrid\": True,\n                \"use_bm25\": True,\n                \"rerank\": True,\n                \"diverse\": True,\n            },\n        }\n        return settings.get(mode, settings[SearchMode.BALANCED])\n\n    async def search_with_context(\n        self,\n        query: str,\n        context: Optional[str] = None,\n        top_k: int = 5,\n    ) -> tuple[List[AdvancedSearchResult], str]:\n        \"\"\"\n        Search and format results for RAG context.\n\n        Args:\n            query: Search query\n            context: Additional context to include\n            top_k: Number of results\n\n        Returns:\n            Tuple of (results, formatted_context)\n        \"\"\"\n        results, metrics = await self.search(query, top_k=top_k)\n\n        # Format context for LLM\n        context_parts = []\n        if context:\n            context_parts.append(f\"Additional context:\\n{context}\\n\")\n\n        context_parts.append(\"Relevant information from knowledge base:\\n\")\n\n        for i, result in enumerate(results, 1):\n            source = result.title or result.source_type or \"Unknown source\"\n            source_tag = (result.source_type or result.metadata.get(\"source_type\") or \"source\").upper()\n            context_parts.append(f\"\\n[{i} | {source_tag}] {source}:\\n{result.content}\\n\")\n\n        formatted_context = \"\\n\".join(context_parts)\n        return results, formatted_context\n\n    async def multi_query_search(\n        self,\n        queries: List[str],\n        top_k_per_query: int = 3,\n        deduplicate: bool = True,\n    ) -> List[AdvancedSearchResult]:\n        \"\"\"\n        Search with multiple queries and merge results.\n\n        Useful for decomposed queries or multi-aspect search.\n        \"\"\"\n        # Search all queries in parallel\n        tasks = [self.search(q, top_k=top_k_per_query) for q in queries]\n        results_list = await asyncio.gather(*tasks)\n\n        # Merge results\n        all_results = []\n        seen_chunks: set = set()\n\n        for results, _ in results_list:\n            for result in results:\n                if deduplicate and result.chunk_id in seen_chunks:\n                    continue\n                seen_chunks.add(result.chunk_id)\n                all_results.append(result)\n\n        # Sort by score\n        all_results.sort(key=lambda r: r.score, reverse=True)\n\n        return all_results\n\n    def format_context_for_rag(\n        self,\n        results: List[AdvancedSearchResult],\n        max_tokens: int = 4000,\n    ) -> str:\n        \"\"\"\n        Format search results as context for RAG.\n\n        Includes source citations and relevance scores.\n        \"\"\"\n        if not results:\n            return \"No relevant information found in the knowledge base.\"\n\n        context_parts = []\n        estimated_tokens = 0\n        tokens_per_char = 0.25  # Rough estimate\n\n        for i, result in enumerate(results, 1):\n            source = result.title or result.source_type or \"Source\"\n            source_tag = (result.source_type or result.metadata.get(\"source_type\") or \"source\").upper()\n            score_str = f\"(relevance: {result.score:.2f})\"\n\n            entry = f\"\\n[{i} | {source_tag}] {source} {score_str}:\\n{result.content}\\n\"\n            entry_tokens = len(entry) * tokens_per_char\n\n            if estimated_tokens + entry_tokens > max_tokens:\n                break\n\n            context_parts.append(entry)\n            estimated_tokens += entry_tokens\n\n        if not context_parts:\n            return \"No relevant information found in the knowledge base.\"\n\n        return \"\".join(context_parts)\n\n\n# Singleton instance for convenience\n_advanced_search: Optional[AdvancedSearchAggregator] = None\n\n\ndef get_advanced_search() -> AdvancedSearchAggregator:\n    \"\"\"Get or create the advanced search aggregator instance.\"\"\"\n    global _advanced_search\n    if _advanced_search is None:\n        _advanced_search = AdvancedSearchAggregator()\n    return _advanced_search\n"
}
