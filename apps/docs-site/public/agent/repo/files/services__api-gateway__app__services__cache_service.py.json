{
  "path": "services/api-gateway/app/services/cache_service.py",
  "language": "python",
  "size": 12720,
  "last_modified": "2025-12-04T11:26:55.701Z",
  "lines": 379,
  "content": "\"\"\"Multi-level caching service (Phase 7 Integration Improvements - P2.1).\n\nImplements a two-tier caching strategy:\n- L1: In-memory LRU cache (cachetools) - for hot-path, low-latency access\n- L2: Redis cache - for distributed caching across instances\n\nArchitecture:\n- Cache keys use prefixes for namespacing (e.g., \"rag_query:\", \"user:\", \"doc_meta:\")\n- L1 cache checked first, then L2 on miss\n- Writes go to both L1 and L2\n- TTLs configured per data type based on volatility\n- Prometheus metrics track hit/miss rates, latency, size\n\nUsage:\n    cache = CacheService()\n\n    # Get with automatic L1 -> L2 fallback\n    value = await cache.get(\"rag_query:what_is_diabetes\")\n\n    # Set in both L1 and L2\n    await cache.set(\"rag_query:what_is_diabetes\", result, ttl=300)\n\n    # Invalidate across all layers\n    await cache.delete(\"user:123\")\n\"\"\"\n\nimport hashlib\nimport pickle\nimport time\nfrom typing import Any, Dict, Optional\n\nimport redis.asyncio as redis\nfrom app.core.config import settings\nfrom app.core.logging import get_logger\nfrom app.core.metrics import (\n    cache_entries_total,\n    cache_evictions_total,\n    cache_hits_total,\n    cache_latency_seconds,\n    cache_misses_total,\n)\nfrom cachetools import LRUCache\n\nlogger = get_logger(__name__)\n\n\nclass CacheConfig:\n    \"\"\"Cache configuration for different data types.\"\"\"\n\n    # L1 (in-memory) configuration\n    L1_MAX_SIZE = 1000  # Max entries in L1 cache\n\n    # TTL configurations (in seconds)\n    TTL_CONFIG = {\n        \"rag_query\": 3600,  # 1 hour - RAG queries are relatively stable\n        \"rag_embedding\": 86400,  # 24 hours - Embeddings rarely change\n        \"user\": 900,  # 15 minutes - User data changes moderately\n        \"doc_meta\": 7200,  # 2 hours - Document metadata is stable\n        \"search_results\": 1800,  # 30 minutes - Search results moderately stable\n        \"session\": 900,  # 15 minutes - Session data changes frequently\n        \"default\": 600,  # 10 minutes - Default fallback\n    }\n\n    @classmethod\n    def get_ttl(cls, key_prefix: str) -> int:\n        \"\"\"Get TTL for a given key prefix.\"\"\"\n        return cls.TTL_CONFIG.get(key_prefix, cls.TTL_CONFIG[\"default\"])\n\n\nclass CacheService:\n    \"\"\"Multi-level cache service with L1 (in-memory) and L2 (Redis) tiers.\"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize cache service with L1 (LRU) and L2 (Redis) backends.\"\"\"\n        # L1: In-memory LRU cache\n        self.l1_cache: LRUCache = LRUCache(maxsize=CacheConfig.L1_MAX_SIZE)\n\n        # L2: Redis connection pool (will be initialized lazily)\n        self._redis_client: Optional[redis.Redis] = None\n\n        logger.info(\n            \"cache_service_initialized\",\n            extra={\n                \"l1_max_size\": CacheConfig.L1_MAX_SIZE,\n                \"redis_host\": settings.REDIS_HOST,\n            },\n        )\n\n    async def get_redis_client(self) -> redis.Redis:\n        \"\"\"Get or create Redis client connection.\"\"\"\n        if self._redis_client is None:\n            self._redis_client = redis.Redis(\n                host=settings.REDIS_HOST,\n                port=settings.REDIS_PORT,\n                password=settings.REDIS_PASSWORD,\n                db=2,  # Use database 2 for caching (0=general, 1=ARQ)\n                decode_responses=False,  # Handle bytes ourselves\n                socket_connect_timeout=5,\n                socket_timeout=5,\n            )\n        return self._redis_client\n\n    def _extract_prefix(self, key: str) -> str:\n        \"\"\"Extract prefix from cache key (e.g., 'rag_query:foo' -> 'rag_query').\"\"\"\n        return key.split(\":\", 1)[0] if \":\" in key else \"default\"\n\n    def _serialize(self, value: Any) -> bytes:\n        \"\"\"Serialize value for caching using pickle.\"\"\"\n        try:\n            return pickle.dumps(value)\n        except Exception as e:\n            logger.error(f\"cache_serialization_error: {e}\", exc_info=True)\n            raise\n\n    def _deserialize(self, data: bytes) -> Any:\n        \"\"\"Deserialize cached value from bytes.\"\"\"\n        try:\n            return pickle.loads(data)  # nosec B301 - internal cache only, not untrusted data\n        except Exception as e:\n            logger.error(f\"cache_deserialization_error: {e}\", exc_info=True)\n            raise\n\n    async def get(self, key: str) -> Optional[Any]:\n        \"\"\"\n        Get value from cache, checking L1 first, then L2.\n\n        Args:\n            key: Cache key with prefix (e.g., \"rag_query:what_is_diabetes\")\n\n        Returns:\n            Cached value if found, None otherwise\n        \"\"\"\n        key_prefix = self._extract_prefix(key)\n        start_time = time.time()\n\n        # Try L1 (in-memory) first\n        try:\n            if key in self.l1_cache:\n                value = self.l1_cache[key]\n\n                # Metrics\n                cache_hits_total.labels(cache_layer=\"l1\", cache_key_prefix=key_prefix).inc()\n                cache_latency_seconds.labels(cache_layer=\"l1\", operation=\"get\").observe(time.time() - start_time)\n\n                logger.debug(f\"cache_hit_l1: key={key}\")\n                return value\n\n            # L1 miss\n            cache_misses_total.labels(cache_layer=\"l1\", cache_key_prefix=key_prefix).inc()\n\n        except Exception as e:\n            logger.warning(f\"l1_cache_error: {e}\")\n\n        # Try L2 (Redis)\n        try:\n            redis_client = await self.get_redis_client()\n            l2_start = time.time()\n\n            data = await redis_client.get(key)\n\n            if data:\n                value = self._deserialize(data)\n\n                # Promote to L1 for future quick access\n                try:\n                    self.l1_cache[key] = value\n                except Exception as e:\n                    logger.warning(f\"l1_promotion_error: {e}\")\n\n                # Metrics\n                cache_hits_total.labels(cache_layer=\"l2\", cache_key_prefix=key_prefix).inc()\n                cache_latency_seconds.labels(cache_layer=\"l2\", operation=\"get\").observe(time.time() - l2_start)\n\n                logger.debug(f\"cache_hit_l2: key={key}\")\n                return value\n\n            # L2 miss\n            cache_misses_total.labels(cache_layer=\"l2\", cache_key_prefix=key_prefix).inc()\n            cache_latency_seconds.labels(cache_layer=\"l2\", operation=\"get\").observe(time.time() - l2_start)\n\n        except Exception as e:\n            logger.error(f\"l2_cache_error: {e}\", exc_info=True)\n\n        logger.debug(f\"cache_miss: key={key}\")\n        return None\n\n    async def set(self, key: str, value: Any, ttl: Optional[int] = None) -> bool:\n        \"\"\"\n        Set value in both L1 and L2 caches.\n\n        Args:\n            key: Cache key with prefix\n            value: Value to cache\n            ttl: Time-to-live in seconds (auto-determined from prefix if not provided)\n\n        Returns:\n            True if successfully cached, False otherwise\n        \"\"\"\n        key_prefix = self._extract_prefix(key)\n\n        # Determine TTL\n        if ttl is None:\n            ttl = CacheConfig.get_ttl(key_prefix)\n\n        success = True\n\n        # Set in L1 (in-memory)\n        try:\n            start_time = time.time()\n            self.l1_cache[key] = value\n\n            cache_latency_seconds.labels(cache_layer=\"l1\", operation=\"set\").observe(time.time() - start_time)\n            cache_entries_total.labels(cache_layer=\"l1\").set(len(self.l1_cache))\n\n        except Exception as e:\n            logger.warning(f\"l1_cache_set_error: {e}\")\n            success = False\n\n        # Set in L2 (Redis)\n        try:\n            redis_client = await self.get_redis_client()\n            start_time = time.time()\n\n            serialized = self._serialize(value)\n            await redis_client.setex(key, ttl, serialized)\n\n            cache_latency_seconds.labels(cache_layer=\"l2\", operation=\"set\").observe(time.time() - start_time)\n\n        except Exception as e:\n            logger.error(f\"l2_cache_set_error: {e}\", exc_info=True)\n            success = False\n\n        if success:\n            logger.debug(f\"cache_set: key={key}, ttl={ttl}\")\n\n        return success\n\n    async def delete(self, key: str) -> bool:\n        \"\"\"\n        Delete key from both L1 and L2 caches.\n\n        Args:\n            key: Cache key to delete\n\n        Returns:\n            True if successfully deleted, False otherwise\n        \"\"\"\n        success = True\n\n        # Delete from L1\n        try:\n            if key in self.l1_cache:\n                del self.l1_cache[key]\n                cache_evictions_total.labels(cache_layer=\"l1\", reason=\"manual\").inc()\n                cache_entries_total.labels(cache_layer=\"l1\").set(len(self.l1_cache))\n        except Exception as e:\n            logger.warning(f\"l1_cache_delete_error: {e}\")\n            success = False\n\n        # Delete from L2\n        try:\n            redis_client = await self.get_redis_client()\n            await redis_client.delete(key)\n            cache_evictions_total.labels(cache_layer=\"l2\", reason=\"manual\").inc()\n        except Exception as e:\n            logger.error(f\"l2_cache_delete_error: {e}\", exc_info=True)\n            success = False\n\n        logger.debug(f\"cache_delete: key={key}\")\n        return success\n\n    async def delete_pattern(self, pattern: str) -> int:\n        \"\"\"\n        Delete all keys matching a pattern (L2 only - L1 cleared entirely).\n\n        Args:\n            pattern: Redis key pattern (e.g., \"rag_query:*\")\n\n        Returns:\n            Number of keys deleted\n        \"\"\"\n        try:\n            redis_client = await self.get_redis_client()\n\n            # Find matching keys\n            keys = []\n            async for key in redis_client.scan_iter(match=pattern):\n                keys.append(key)\n\n            # Delete in batch\n            if keys:\n                deleted = await redis_client.delete(*keys)\n                cache_evictions_total.labels(cache_layer=\"l2\", reason=\"pattern\").inc(deleted)\n            else:\n                deleted = 0\n\n            # Clear L1 cache entirely (can't do pattern matching on LRU)\n            self.l1_cache.clear()\n            cache_evictions_total.labels(cache_layer=\"l1\", reason=\"pattern\").inc()\n            cache_entries_total.labels(cache_layer=\"l1\").set(0)\n\n            logger.info(f\"cache_delete_pattern: pattern={pattern}, deleted={deleted}\")\n            return deleted\n\n        except Exception as e:\n            logger.error(f\"cache_delete_pattern_error: {e}\", exc_info=True)\n            return 0\n\n    async def clear(self) -> bool:\n        \"\"\"Clear all caches (L1 and L2).\"\"\"\n        try:\n            # Clear L1\n            self.l1_cache.clear()\n            cache_entries_total.labels(cache_layer=\"l1\").set(0)\n\n            # Clear L2 (only our database)\n            redis_client = await self.get_redis_client()\n            await redis_client.flushdb()\n\n            logger.info(\"cache_cleared: all layers\")\n            return True\n\n        except Exception as e:\n            logger.error(f\"cache_clear_error: {e}\", exc_info=True)\n            return False\n\n    async def get_stats(self) -> Dict[str, Any]:\n        \"\"\"Get cache statistics for monitoring.\"\"\"\n        try:\n            redis_client = await self.get_redis_client()\n            redis_info = await redis_client.info(\"memory\")\n\n            return {\n                \"l1\": {\n                    \"size\": len(self.l1_cache),\n                    \"max_size\": self.l1_cache.maxsize,\n                    \"utilization\": len(self.l1_cache) / self.l1_cache.maxsize,\n                },\n                \"l2\": {\n                    \"used_memory\": redis_info.get(\"used_memory\", 0),\n                    \"used_memory_human\": redis_info.get(\"used_memory_human\", \"0B\"),\n                    \"connected_clients\": redis_info.get(\"connected_clients\", 0),\n                },\n            }\n        except Exception as e:\n            logger.error(f\"cache_stats_error: {e}\", exc_info=True)\n            return {}\n\n\n# Global cache instance\ncache_service = CacheService()\n\n\ndef generate_cache_key(prefix: str, *args, **kwargs) -> str:\n    \"\"\"\n    Generate a stable cache key from prefix and arguments.\n\n    Args:\n        prefix: Cache key prefix (e.g., \"rag_query\", \"user\")\n        *args: Positional arguments to include in key\n        **kwargs: Keyword arguments to include in key\n\n    Returns:\n        Cache key string\n\n    Example:\n        generate_cache_key(\"rag_query\", \"what is diabetes\", top_k=5)\n        # Returns: \"rag_query:hash_abc123\"\n    \"\"\"\n    # Create deterministic hash from args and kwargs\n    key_parts = [str(arg) for arg in args]\n    key_parts.extend(f\"{k}={v}\" for k, v in sorted(kwargs.items()))\n    key_string = \"|\".join(key_parts)\n\n    # MD5 used for cache key generation, not security purposes\n    key_hash = hashlib.md5(key_string.encode(), usedforsecurity=False).hexdigest()[:12]\n\n    return f\"{prefix}:{key_hash}\"\n"
}
