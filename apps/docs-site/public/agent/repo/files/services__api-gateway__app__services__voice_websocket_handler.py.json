{
  "path": "services/api-gateway/app/services/voice_websocket_handler.py",
  "language": "python",
  "size": 32999,
  "last_modified": "2025-12-04T11:27:02.994Z",
  "lines": 908,
  "content": "\"\"\"\nEnhanced Voice WebSocket Handler\n\nProvides bidirectional voice streaming with:\n- Barge-in support (interrupt AI speech when user speaks)\n- Echo cancellation integration\n- Noise suppression integration\n- OpenAI Realtime API proxy\n- Connection state management\n- Metrics and observability\n\nThis handler acts as a proxy between the client and OpenAI's Realtime API,\nadding audio processing and conversation control features.\n\"\"\"\n\nimport asyncio\nimport base64\nimport json\nimport time\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nfrom typing import Any, Callable, Dict, List, Optional\n\nimport websockets\nfrom app.core.config import settings\nfrom app.core.logging import get_logger\nfrom app.services.audio_processor import StreamingAudioProcessor\nfrom app.services.voice_activity_detector import StreamingVAD, VADConfig\nfrom websockets.asyncio.client import ClientConnection\n\nlogger = get_logger(__name__)\n\n# Import tool service for function calling support\ntry:\n    from app.services.tools import tool_service\n    from app.services.tools.tool_service import ToolExecutionContext\n\n    TOOLS_AVAILABLE = True\nexcept ImportError:\n    TOOLS_AVAILABLE = False\n    logger.warning(\"Tool service not available - function calling disabled\")\n\n\nclass ConnectionState(Enum):\n    \"\"\"Voice connection states\"\"\"\n\n    DISCONNECTED = \"disconnected\"\n    CONNECTING = \"connecting\"\n    CONNECTED = \"connected\"\n    AUTHENTICATED = \"authenticated\"\n    READY = \"ready\"\n    ERROR = \"error\"\n\n\nclass ConversationState(Enum):\n    \"\"\"Conversation turn states\"\"\"\n\n    IDLE = \"idle\"\n    USER_SPEAKING = \"user_speaking\"\n    AI_THINKING = \"ai_thinking\"\n    AI_SPEAKING = \"ai_speaking\"\n    BARGE_IN = \"barge_in\"\n\n\n@dataclass\nclass VoiceSessionConfig:\n    \"\"\"Configuration for a voice session\"\"\"\n\n    # User and session identifiers\n    user_id: str\n    session_id: str\n    conversation_id: Optional[str] = None\n\n    # OpenAI configuration\n    model: str = \"gpt-4o-realtime-preview-2024-12-17\"\n    voice: str = \"alloy\"\n\n    # Audio processing settings\n    echo_cancellation: bool = True\n    noise_suppression: bool = True\n\n    # VAD settings (aggressive defaults for low latency)\n    vad_threshold: float = 0.5\n    vad_prefix_padding_ms: int = 150  # Reduced from 300ms\n    vad_silence_duration_ms: int = 200  # Reduced from 500ms\n\n    # Barge-in settings\n    barge_in_enabled: bool = True\n    barge_in_threshold_ms: int = 200  # Min speech duration to trigger barge-in\n    barge_in_debounce_ms: int = 100  # Debounce to prevent false positives\n\n    # Timeouts\n    connection_timeout_sec: float = 10.0\n    response_timeout_sec: float = 30.0\n    idle_timeout_sec: float = 300.0  # 5 minutes\n\n    # Tool/Function calling settings\n    tools_enabled: bool = True  # Enable function calling\n    tool_categories: Optional[List[str]] = None  # Filter tool categories (None = all)\n\n\n@dataclass\nclass SessionMetrics:\n    \"\"\"Metrics collected during a voice session\"\"\"\n\n    connection_start_time: float = 0.0\n    connection_time_ms: float = 0.0\n    first_audio_latency_ms: float = 0.0\n    total_user_speech_ms: float = 0.0\n    total_ai_speech_ms: float = 0.0\n    user_utterance_count: int = 0\n    ai_response_count: int = 0\n    barge_in_count: int = 0\n    reconnect_count: int = 0\n    error_count: int = 0\n    messages_sent: int = 0\n    messages_received: int = 0\n\n\n@dataclass\nclass VoiceSessionState:\n    \"\"\"State for a voice session\"\"\"\n\n    connection_state: ConnectionState = ConnectionState.DISCONNECTED\n    conversation_state: ConversationState = ConversationState.IDLE\n    current_response_id: Optional[str] = None\n    current_item_id: Optional[str] = None\n    is_playing_audio: bool = False\n    user_speech_start_time: Optional[float] = None\n    ai_speech_start_time: Optional[float] = None\n    barge_in_pending: bool = False\n    metrics: SessionMetrics = field(default_factory=SessionMetrics)\n\n\nclass VoiceWebSocketHandler:\n    \"\"\"\n    Enhanced voice WebSocket handler with barge-in and audio processing.\n\n    This handler manages the bidirectional communication between the client\n    and OpenAI's Realtime API, adding audio processing features like echo\n    cancellation, noise suppression, and barge-in support.\n    \"\"\"\n\n    def __init__(\n        self,\n        config: VoiceSessionConfig,\n        client_secret: str,\n        on_state_change: Optional[Callable[[ConnectionState], None]] = None,\n        on_transcript: Optional[Callable[[str, str], None]] = None,  # (role, text)\n        on_audio: Optional[Callable[[bytes], None]] = None,\n        on_error: Optional[Callable[[str], None]] = None,\n    ):\n        self.config = config\n        self.client_secret = client_secret\n        self.state = VoiceSessionState()\n\n        # Callbacks\n        self._on_state_change = on_state_change\n        self._on_transcript = on_transcript\n        self._on_audio = on_audio\n        self._on_error = on_error\n\n        # Audio processing\n        # Phase 11: Enable audio processor for both echo cancellation and noise suppression\n        if config.echo_cancellation or config.noise_suppression:\n            from app.services.audio_processor import AudioProcessorConfig\n\n            processor_config = AudioProcessorConfig(\n                sample_rate=24000,  # Match OpenAI Realtime API sample rate\n                echo_enabled=config.echo_cancellation,\n                noise_enabled=config.noise_suppression,\n                agc_enabled=True,  # Always enable AGC for consistent levels\n                highpass_enabled=True,  # Remove DC offset\n            )\n            self._audio_processor = StreamingAudioProcessor(processor_config)\n            logger.debug(\n                \"Audio processor initialized\",\n                extra={\n                    \"echo_cancellation\": config.echo_cancellation,\n                    \"noise_suppression\": config.noise_suppression,\n                },\n            )\n        else:\n            self._audio_processor = None\n\n        self._vad = StreamingVAD(\n            VADConfig(\n                threshold=config.vad_threshold,\n                prefix_padding_ms=config.vad_prefix_padding_ms,\n                silence_duration_ms=config.vad_silence_duration_ms,\n            )\n        )\n\n        # WebSocket connections\n        self._openai_ws: Optional[ClientConnection] = None\n        self._tasks: List[asyncio.Task] = []\n\n        # Buffers\n        self._outgoing_audio_buffer: asyncio.Queue[bytes] = asyncio.Queue()\n        self._incoming_audio_buffer: asyncio.Queue[bytes] = asyncio.Queue()\n        self._speaker_audio_buffer: bytes = b\"\"  # For echo cancellation\n\n        # Control\n        self._running = False\n        self._shutdown_event = asyncio.Event()\n\n    async def start(self) -> bool:\n        \"\"\"\n        Start the voice session.\n\n        Connects to OpenAI Realtime API and begins processing.\n\n        Returns:\n            True if connection successful, False otherwise\n        \"\"\"\n        if self._running:\n            logger.warning(\"Session already running\")\n            return True\n\n        self._running = True\n        self.state.metrics.connection_start_time = time.monotonic()\n        self._update_connection_state(ConnectionState.CONNECTING)\n\n        try:\n            # Connect to OpenAI Realtime API\n            connected = await self._connect_to_openai()\n            if not connected:\n                self._update_connection_state(ConnectionState.ERROR)\n                return False\n\n            # Configure session\n            await self._configure_session()\n\n            self._update_connection_state(ConnectionState.READY)\n\n            # Start processing tasks\n            self._tasks = [\n                asyncio.create_task(self._openai_receiver()),\n                asyncio.create_task(self._audio_sender()),\n                asyncio.create_task(self._idle_monitor()),\n            ]\n\n            return True\n\n        except Exception as e:\n            logger.error(f\"Failed to start voice session: {e}\")\n            self._update_connection_state(ConnectionState.ERROR)\n            self._emit_error(str(e))\n            return False\n\n    async def stop(self) -> None:\n        \"\"\"Stop the voice session and cleanup resources.\"\"\"\n        if not self._running:\n            return\n\n        self._running = False\n        self._shutdown_event.set()\n\n        # Cancel all tasks\n        for task in self._tasks:\n            if not task.done():\n                task.cancel()\n\n        # Wait for tasks to finish\n        if self._tasks:\n            await asyncio.gather(*self._tasks, return_exceptions=True)\n\n        # Close WebSocket connection\n        if self._openai_ws:\n            try:\n                await self._openai_ws.close()\n            except Exception as e:\n                logger.debug(f\"Error closing OpenAI WebSocket: {e}\")\n\n        self._update_connection_state(ConnectionState.DISCONNECTED)\n        logger.info(\n            \"Voice session stopped\",\n            extra={\n                \"session_id\": self.config.session_id,\n                \"metrics\": self._get_metrics_dict(),\n            },\n        )\n\n    async def send_audio(self, audio_data: bytes) -> None:\n        \"\"\"\n        Send audio data from the client.\n\n        The audio is processed (VAD, noise suppression) before being\n        sent to OpenAI.\n\n        Args:\n            audio_data: PCM16 audio data from the client\n        \"\"\"\n        if not self._running:\n            return\n\n        # Apply noise suppression if enabled\n        if self._audio_processor:\n            audio_data = await self._audio_processor.process_chunk(audio_data, self._speaker_audio_buffer)\n\n        # Process through VAD\n        vad_state = await self._vad.process_chunk(audio_data)\n\n        # Handle barge-in\n        if self.config.barge_in_enabled:\n            await self._handle_barge_in(vad_state)\n\n        # Queue audio for sending to OpenAI\n        await self._outgoing_audio_buffer.put(audio_data)\n\n    async def send_text(self, text: str) -> None:\n        \"\"\"\n        Send a text message to the conversation.\n\n        Args:\n            text: Text message from the user\n        \"\"\"\n        if not self._openai_ws or not self._running:\n            return\n\n        # Create conversation item\n        event = {\n            \"type\": \"conversation.item.create\",\n            \"item\": {\n                \"type\": \"message\",\n                \"role\": \"user\",\n                \"content\": [{\"type\": \"input_text\", \"text\": text}],\n            },\n        }\n        await self._send_to_openai(event)\n\n        # Trigger response generation\n        await self._send_to_openai({\"type\": \"response.create\"})\n\n    async def cancel_response(self) -> None:\n        \"\"\"Cancel the current AI response (barge-in).\"\"\"\n        if not self._openai_ws or not self._running:\n            return\n\n        if self.state.current_response_id:\n            logger.info(\n                f\"Cancelling response: {self.state.current_response_id}\",\n                extra={\"session_id\": self.config.session_id},\n            )\n\n            await self._send_to_openai({\"type\": \"response.cancel\"})\n            self.state.barge_in_pending = False\n            self.state.metrics.barge_in_count += 1\n\n    async def _connect_to_openai(self) -> bool:\n        \"\"\"Connect to OpenAI Realtime API.\"\"\"\n        try:\n            url = f\"{settings.REALTIME_BASE_URL}?model={self.config.model}\"\n\n            self._openai_ws = await asyncio.wait_for(\n                websockets.connect(\n                    url,\n                    additional_headers={\n                        \"Authorization\": f\"Bearer {self.client_secret}\",\n                        \"OpenAI-Beta\": \"realtime=v1\",\n                    },\n                ),\n                timeout=self.config.connection_timeout_sec,\n            )\n\n            self.state.metrics.connection_time_ms = (time.monotonic() - self.state.metrics.connection_start_time) * 1000\n\n            self._update_connection_state(ConnectionState.CONNECTED)\n            logger.info(\n                \"Connected to OpenAI Realtime API\",\n                extra={\n                    \"session_id\": self.config.session_id,\n                    \"connection_time_ms\": self.state.metrics.connection_time_ms,\n                },\n            )\n            return True\n\n        except asyncio.TimeoutError:\n            logger.error(\"OpenAI connection timeout\")\n            self._emit_error(\"Connection timeout\")\n            return False\n        except Exception as e:\n            logger.error(f\"OpenAI connection error: {e}\")\n            self._emit_error(str(e))\n            return False\n\n    async def _configure_session(self) -> None:\n        \"\"\"Configure the OpenAI session with desired settings.\"\"\"\n        session_config = {\n            \"type\": \"session.update\",\n            \"session\": {\n                \"modalities\": [\"text\", \"audio\"],\n                \"instructions\": self._get_system_instructions(),\n                \"voice\": self.config.voice,\n                \"input_audio_format\": \"pcm16\",\n                \"output_audio_format\": \"pcm16\",\n                \"input_audio_transcription\": {\"model\": \"whisper-1\"},\n                \"turn_detection\": {\n                    \"type\": \"server_vad\",\n                    \"threshold\": self.config.vad_threshold,\n                    \"prefix_padding_ms\": self.config.vad_prefix_padding_ms,\n                    \"silence_duration_ms\": self.config.vad_silence_duration_ms,\n                },\n            },\n        }\n\n        # Add tools if enabled and available\n        if self.config.tools_enabled and TOOLS_AVAILABLE:\n            try:\n                tools = tool_service.get_openai_tools_for_realtime()\n                if tools:\n                    session_config[\"session\"][\"tools\"] = tools\n                    session_config[\"session\"][\"tool_choice\"] = \"auto\"\n                    logger.info(\n                        f\"Configured {len(tools)} tools for voice session\",\n                        extra={\"session_id\": self.config.session_id},\n                    )\n            except Exception as e:\n                logger.warning(f\"Failed to configure tools: {e}\")\n\n        await self._send_to_openai(session_config)\n        self._update_connection_state(ConnectionState.AUTHENTICATED)\n\n    def _get_system_instructions(self) -> str:\n        \"\"\"Get system instructions for the AI.\"\"\"\n        return \"\"\"You are a helpful medical AI assistant in voice mode.\n\nGuidelines:\n- Keep responses concise and conversational\n- Use natural spoken language, not written text\n- Ask clarifying questions when needed\n- Be empathetic and professional\n- Cite sources when providing medical information\n- Maintain HIPAA compliance at all times\n\nWhen speaking:\n- Use short sentences\n- Avoid complex medical jargon unless requested\n- Confirm understanding before proceeding\n- Offer to provide more details if needed\"\"\"\n\n    async def _openai_receiver(self) -> None:\n        \"\"\"Receive and process messages from OpenAI.\"\"\"\n        try:\n            async for message in self._openai_ws:\n                if not self._running:\n                    break\n\n                try:\n                    event = json.loads(message)\n                    await self._handle_openai_event(event)\n                except json.JSONDecodeError:\n                    logger.warning(\"Invalid JSON from OpenAI\")\n                except Exception as e:\n                    logger.error(f\"Error handling OpenAI event: {e}\")\n                    self.state.metrics.error_count += 1\n\n        except websockets.ConnectionClosed as e:\n            logger.info(f\"OpenAI connection closed: {e}\")\n            if self._running:\n                self._update_connection_state(ConnectionState.DISCONNECTED)\n        except Exception as e:\n            logger.error(f\"OpenAI receiver error: {e}\")\n            self._emit_error(str(e))\n\n    async def _handle_openai_event(self, event: Dict[str, Any]) -> None:\n        \"\"\"Handle an event from OpenAI.\"\"\"\n        event_type = event.get(\"type\", \"\")\n        self.state.metrics.messages_received += 1\n\n        if event_type == \"session.created\":\n            logger.debug(\"Session created successfully\")\n\n        elif event_type == \"session.updated\":\n            logger.debug(\"Session updated\")\n\n        elif event_type == \"response.created\":\n            self.state.current_response_id = event.get(\"response\", {}).get(\"id\")\n            self._update_conversation_state(ConversationState.AI_THINKING)\n\n        elif event_type == \"response.output_item.added\":\n            item = event.get(\"item\", {})\n            self.state.current_item_id = item.get(\"id\")\n\n        elif event_type == \"response.audio.delta\":\n            # AI is sending audio\n            if self.state.conversation_state != ConversationState.AI_SPEAKING:\n                self._update_conversation_state(ConversationState.AI_SPEAKING)\n                self.state.ai_speech_start_time = time.monotonic()\n\n                # Record first audio latency\n                if self.state.metrics.first_audio_latency_ms == 0:\n                    self.state.metrics.first_audio_latency_ms = (\n                        time.monotonic() - self.state.metrics.connection_start_time\n                    ) * 1000\n\n            # Decode and forward audio\n            audio_b64 = event.get(\"delta\", \"\")\n            if audio_b64:\n                audio_data = base64.b64decode(audio_b64)\n                self._speaker_audio_buffer = audio_data  # For echo cancellation\n                if self._on_audio:\n                    self._on_audio(audio_data)\n\n        elif event_type == \"response.audio.done\":\n            # AI finished sending audio\n            if self.state.ai_speech_start_time:\n                duration = (time.monotonic() - self.state.ai_speech_start_time) * 1000\n                self.state.metrics.total_ai_speech_ms += duration\n                self.state.ai_speech_start_time = None\n\n        elif event_type == \"response.audio_transcript.delta\":\n            # AI transcript chunk\n            transcript = event.get(\"delta\", \"\")\n            if transcript and self._on_transcript:\n                self._on_transcript(\"assistant\", transcript)\n\n        elif event_type == \"response.audio_transcript.done\":\n            # Complete AI transcript\n            transcript = event.get(\"transcript\", \"\")\n            logger.debug(f\"AI transcript complete: {transcript[:100]}...\")\n\n        elif event_type == \"response.done\":\n            # Response complete\n            self.state.current_response_id = None\n            self.state.current_item_id = None\n            self.state.metrics.ai_response_count += 1\n            self._update_conversation_state(ConversationState.IDLE)\n\n        elif event_type == \"input_audio_buffer.speech_started\":\n            # User started speaking\n            self._update_conversation_state(ConversationState.USER_SPEAKING)\n            self.state.user_speech_start_time = time.monotonic()\n\n        elif event_type == \"input_audio_buffer.speech_stopped\":\n            # User stopped speaking\n            if self.state.user_speech_start_time:\n                duration = (time.monotonic() - self.state.user_speech_start_time) * 1000\n                self.state.metrics.total_user_speech_ms += duration\n                self.state.user_speech_start_time = None\n            self.state.metrics.user_utterance_count += 1\n\n        elif event_type == \"conversation.item.input_audio_transcription.completed\":\n            # User transcript complete\n            transcript = event.get(\"transcript\", \"\")\n            if transcript and self._on_transcript:\n                self._on_transcript(\"user\", transcript)\n\n        elif event_type == \"response.function_call_arguments.done\":\n            # Function call is complete - execute the tool\n            await self._handle_function_call(event)\n\n        elif event_type == \"response.output_item.done\":\n            item = event.get(\"item\", {})\n            if item.get(\"type\") == \"function_call\":\n                # Function call item completed - execute if not already handled\n                await self._handle_function_call_item(item)\n\n        elif event_type == \"error\":\n            error = event.get(\"error\", {})\n            error_msg = error.get(\"message\", \"Unknown error\")\n            logger.error(f\"OpenAI error: {error_msg}\")\n            self.state.metrics.error_count += 1\n            self._emit_error(error_msg)\n\n        elif event_type == \"rate_limits.updated\":\n            # Rate limit info - log for monitoring\n            rate_limits = event.get(\"rate_limits\", [])\n            logger.debug(f\"Rate limits: {rate_limits}\")\n\n    async def _handle_function_call(self, event: Dict[str, Any]) -> None:\n        \"\"\"\n        Handle a completed function call from OpenAI.\n\n        Executes the tool and sends the result back to OpenAI.\n        \"\"\"\n        if not TOOLS_AVAILABLE:\n            logger.warning(\"Function call received but tools not available\")\n            return\n\n        call_id = event.get(\"call_id\")\n        name = event.get(\"name\")\n        arguments_str = event.get(\"arguments\", \"{}\")\n\n        logger.info(\n            f\"Executing tool: {name}\",\n            extra={\n                \"session_id\": self.config.session_id,\n                \"call_id\": call_id,\n                \"arguments\": arguments_str[:200],\n            },\n        )\n\n        try:\n            # Parse arguments\n            arguments = json.loads(arguments_str)\n\n            # Create execution context\n            context = ToolExecutionContext(\n                user_id=self.config.user_id,\n                session_id=self.config.session_id,\n                mode=\"voice\",\n            )\n\n            # Execute the tool\n            result = await tool_service.execute(name, arguments, context)\n\n            # Prepare output\n            if result.success:\n                output = result.data\n                if result.message:\n                    # Include message in output for the AI to speak\n                    if isinstance(output, dict):\n                        output[\"_message\"] = result.message\n                    else:\n                        output = {\"result\": output, \"_message\": result.message}\n            else:\n                output = {\n                    \"error\": result.error,\n                    \"needs_clarification\": result.needs_clarification,\n                    \"needs_connection\": result.needs_connection,\n                }\n                if result.available_calendars:\n                    output[\"available_calendars\"] = result.available_calendars\n                if result.message:\n                    output[\"_message\"] = result.message\n\n            # Send result back to OpenAI\n            output_event = {\n                \"type\": \"conversation.item.create\",\n                \"item\": {\n                    \"type\": \"function_call_output\",\n                    \"call_id\": call_id,\n                    \"output\": json.dumps(output),\n                },\n            }\n            await self._send_to_openai(output_event)\n\n            # Trigger response generation\n            await self._send_to_openai({\"type\": \"response.create\"})\n\n            logger.info(\n                f\"Tool executed: {name} - {'success' if result.success else 'failed'}\",\n                extra={\n                    \"session_id\": self.config.session_id,\n                    \"duration_ms\": result.duration_ms,\n                },\n            )\n\n        except json.JSONDecodeError as e:\n            logger.error(f\"Failed to parse function arguments: {e}\")\n            await self._send_function_error(call_id, f\"Invalid arguments: {e}\")\n        except Exception as e:\n            logger.exception(f\"Error executing tool {name}: {e}\")\n            await self._send_function_error(call_id, str(e))\n\n    async def _handle_function_call_item(self, item: Dict[str, Any]) -> None:\n        \"\"\"Handle a function call item from response.output_item.done.\"\"\"\n        # This is an alternative event format - extract call info and handle\n        call_id = item.get(\"call_id\")\n        name = item.get(\"name\")\n        arguments_str = item.get(\"arguments\", \"{}\")\n\n        if call_id and name:\n            # Create a compatible event and handle it\n            event = {\n                \"call_id\": call_id,\n                \"name\": name,\n                \"arguments\": arguments_str,\n            }\n            await self._handle_function_call(event)\n\n    async def _send_function_error(self, call_id: str, error_message: str) -> None:\n        \"\"\"Send a function call error back to OpenAI.\"\"\"\n        output_event = {\n            \"type\": \"conversation.item.create\",\n            \"item\": {\n                \"type\": \"function_call_output\",\n                \"call_id\": call_id,\n                \"output\": json.dumps({\"error\": error_message}),\n            },\n        }\n        await self._send_to_openai(output_event)\n        await self._send_to_openai({\"type\": \"response.create\"})\n\n    async def _audio_sender(self) -> None:\n        \"\"\"Send audio from buffer to OpenAI.\"\"\"\n        try:\n            while self._running:\n                try:\n                    audio_data = await asyncio.wait_for(self._outgoing_audio_buffer.get(), timeout=0.1)\n\n                    if self._openai_ws and audio_data:\n                        # Encode audio as base64\n                        audio_b64 = base64.b64encode(audio_data).decode()\n\n                        event = {\n                            \"type\": \"input_audio_buffer.append\",\n                            \"audio\": audio_b64,\n                        }\n                        await self._send_to_openai(event)\n\n                except asyncio.TimeoutError:\n                    continue\n\n        except asyncio.CancelledError:\n            pass\n        except Exception as e:\n            logger.error(f\"Audio sender error: {e}\")\n\n    async def _idle_monitor(self) -> None:\n        \"\"\"Monitor for idle timeout.\"\"\"\n        try:\n            last_activity = time.monotonic()\n\n            while self._running:\n                await asyncio.sleep(10)  # Check every 10 seconds\n\n                # Update activity time if there was recent activity\n                if self.state.conversation_state != ConversationState.IDLE:\n                    last_activity = time.monotonic()\n\n                # Check for idle timeout\n                idle_time = time.monotonic() - last_activity\n                if idle_time > self.config.idle_timeout_sec:\n                    logger.info(\n                        f\"Session idle timeout after {idle_time:.0f}s\",\n                        extra={\"session_id\": self.config.session_id},\n                    )\n                    await self.stop()\n                    break\n\n        except asyncio.CancelledError:\n            pass\n\n    async def _handle_barge_in(self, vad_state) -> None:\n        \"\"\"Handle barge-in when user speaks during AI speech.\"\"\"\n        from app.services.voice_activity_detector import SpeechState\n\n        if self.state.conversation_state != ConversationState.AI_SPEAKING:\n            return\n\n        if vad_state in (SpeechState.SPEECH_START, SpeechState.SPEAKING):\n            if not self.state.barge_in_pending:\n                self.state.barge_in_pending = True\n                # Small delay to avoid false positives\n                await asyncio.sleep(self.config.barge_in_debounce_ms / 1000)\n\n                # Check if still speaking after debounce\n                if self._vad.is_speaking():\n                    logger.info(\n                        \"Barge-in detected, cancelling AI response\",\n                        extra={\"session_id\": self.config.session_id},\n                    )\n                    await self.cancel_response()\n                    self._update_conversation_state(ConversationState.BARGE_IN)\n\n    async def _send_to_openai(self, event: Dict[str, Any]) -> None:\n        \"\"\"Send an event to OpenAI.\"\"\"\n        if not self._openai_ws:\n            return\n\n        try:\n            message = json.dumps(event)\n            await self._openai_ws.send(message)\n            self.state.metrics.messages_sent += 1\n        except Exception as e:\n            logger.error(f\"Error sending to OpenAI: {e}\")\n            self.state.metrics.error_count += 1\n\n    def _update_connection_state(self, new_state: ConnectionState) -> None:\n        \"\"\"Update connection state and notify callback.\"\"\"\n        old_state = self.state.connection_state\n        self.state.connection_state = new_state\n\n        if old_state != new_state:\n            logger.debug(f\"Connection state: {old_state.value} -> {new_state.value}\")\n            if self._on_state_change:\n                self._on_state_change(new_state)\n\n    def _update_conversation_state(self, new_state: ConversationState) -> None:\n        \"\"\"Update conversation state.\"\"\"\n        old_state = self.state.conversation_state\n        self.state.conversation_state = new_state\n\n        if old_state != new_state:\n            logger.debug(f\"Conversation state: {old_state.value} -> {new_state.value}\")\n\n    def _emit_error(self, message: str) -> None:\n        \"\"\"Emit error to callback.\"\"\"\n        self.state.metrics.error_count += 1\n        if self._on_error:\n            self._on_error(message)\n\n    def _get_metrics_dict(self) -> Dict[str, Any]:\n        \"\"\"Get metrics as dictionary.\"\"\"\n        return {\n            \"connection_time_ms\": self.state.metrics.connection_time_ms,\n            \"first_audio_latency_ms\": self.state.metrics.first_audio_latency_ms,\n            \"total_user_speech_ms\": self.state.metrics.total_user_speech_ms,\n            \"total_ai_speech_ms\": self.state.metrics.total_ai_speech_ms,\n            \"user_utterance_count\": self.state.metrics.user_utterance_count,\n            \"ai_response_count\": self.state.metrics.ai_response_count,\n            \"barge_in_count\": self.state.metrics.barge_in_count,\n            \"error_count\": self.state.metrics.error_count,\n            \"messages_sent\": self.state.metrics.messages_sent,\n            \"messages_received\": self.state.metrics.messages_received,\n        }\n\n    def get_metrics(self) -> SessionMetrics:\n        \"\"\"Get current session metrics.\"\"\"\n        return self.state.metrics\n\n\nclass VoiceSessionManager:\n    \"\"\"\n    Manager for multiple concurrent voice sessions.\n\n    Handles session lifecycle, cleanup, and resource management.\n    \"\"\"\n\n    def __init__(self, max_sessions: int = 100):\n        self.max_sessions = max_sessions\n        self._sessions: Dict[str, VoiceWebSocketHandler] = {}\n        self._lock = asyncio.Lock()\n\n    async def create_session(\n        self,\n        config: VoiceSessionConfig,\n        client_secret: str,\n        **callbacks,\n    ) -> VoiceWebSocketHandler:\n        \"\"\"\n        Create a new voice session.\n\n        Args:\n            config: Session configuration\n            client_secret: OpenAI ephemeral client secret\n            **callbacks: Optional callback functions\n\n        Returns:\n            VoiceWebSocketHandler instance\n\n        Raises:\n            ValueError: If max sessions reached\n        \"\"\"\n        async with self._lock:\n            if len(self._sessions) >= self.max_sessions:\n                raise ValueError(\"Maximum concurrent sessions reached\")\n\n            handler = VoiceWebSocketHandler(\n                config=config,\n                client_secret=client_secret,\n                **callbacks,\n            )\n\n            self._sessions[config.session_id] = handler\n            return handler\n\n    async def get_session(self, session_id: str) -> Optional[VoiceWebSocketHandler]:\n        \"\"\"Get a session by ID.\"\"\"\n        return self._sessions.get(session_id)\n\n    async def remove_session(self, session_id: str) -> None:\n        \"\"\"Remove a session.\"\"\"\n        async with self._lock:\n            if session_id in self._sessions:\n                handler = self._sessions.pop(session_id)\n                await handler.stop()\n\n    async def cleanup_stale_sessions(self, max_age_sec: float = 3600) -> int:\n        \"\"\"\n        Clean up sessions that have been running too long.\n\n        Args:\n            max_age_sec: Maximum session age in seconds\n\n        Returns:\n            Number of sessions cleaned up\n        \"\"\"\n        async with self._lock:\n            to_remove = []\n            current_time = time.monotonic()\n\n            for session_id, handler in self._sessions.items():\n                session_age = current_time - handler.state.metrics.connection_start_time\n                if session_age > max_age_sec:\n                    to_remove.append(session_id)\n\n            for session_id in to_remove:\n                handler = self._sessions.pop(session_id)\n                await handler.stop()\n                logger.info(f\"Cleaned up stale session: {session_id}\")\n\n            return len(to_remove)\n\n    def get_active_session_count(self) -> int:\n        \"\"\"Get count of active sessions.\"\"\"\n        return len(self._sessions)\n\n    def get_all_metrics(self) -> Dict[str, Dict[str, Any]]:\n        \"\"\"Get metrics for all sessions.\"\"\"\n        return {session_id: handler._get_metrics_dict() for session_id, handler in self._sessions.items()}\n\n\n# Global session manager instance\nvoice_session_manager = VoiceSessionManager()\n"
}
