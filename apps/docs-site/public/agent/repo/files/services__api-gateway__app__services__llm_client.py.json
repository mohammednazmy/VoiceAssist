{
  "path": "services/api-gateway/app/services/llm_client.py",
  "language": "python",
  "size": 28034,
  "last_modified": "2025-12-04T11:26:57.795Z",
  "lines": 718,
  "content": "\"\"\"LLM client abstraction for VoiceAssist V2.\n\nThis module provides a single interface for calling language models\n(cloud and local) and encapsulates configuration, safety, and routing\nlogic. It is the runtime companion to the high-level design in\nORCHESTRATION_DESIGN.md and SECURITY_COMPLIANCE.md.\n\nThe intent is:\n\n- For non-PHI queries: Prefer high-capability cloud models.\n- For PHI-containing queries: Prefer approved local models.\n- For all queries: Track cost, latency, and model selection decisions.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport inspect\nimport logging\nimport time\nfrom dataclasses import dataclass\nfrom typing import Awaitable, Callable, Dict, Literal, Optional\n\nimport httpx\nfrom app.core.business_metrics import openai_api_cost_dollars, openai_tokens_used_total\nfrom app.core.resilience import openai_breaker\nfrom openai import AsyncOpenAI\nfrom openai.types.chat import ChatCompletion, ChatCompletionChunk\nfrom pybreaker import CircuitBreakerError\n\nlogger = logging.getLogger(__name__)\n\nModelFamily = Literal[\"cloud\", \"local\"]\nIntentType = Literal[\"diagnosis\", \"treatment\", \"drug\", \"guideline\", \"summary\", \"other\"]\n\n# OpenAI model pricing per 1M tokens (as of 2024)\n# Format: {model_name: (input_price_per_1m, output_price_per_1m)}\nMODEL_PRICING: Dict[str, tuple[float, float]] = {\n    # GPT-4o models\n    \"gpt-4o\": (2.50, 10.00),\n    \"gpt-4o-2024-11-20\": (2.50, 10.00),\n    \"gpt-4o-2024-08-06\": (2.50, 10.00),\n    \"gpt-4o-2024-05-13\": (5.00, 15.00),\n    \"gpt-4o-mini\": (0.15, 0.60),\n    \"gpt-4o-mini-2024-07-18\": (0.15, 0.60),\n    # GPT-4 Turbo\n    \"gpt-4-turbo\": (10.00, 30.00),\n    \"gpt-4-turbo-2024-04-09\": (10.00, 30.00),\n    \"gpt-4-turbo-preview\": (10.00, 30.00),\n    # GPT-4\n    \"gpt-4\": (30.00, 60.00),\n    \"gpt-4-0613\": (30.00, 60.00),\n    # GPT-3.5 Turbo\n    \"gpt-3.5-turbo\": (0.50, 1.50),\n    \"gpt-3.5-turbo-0125\": (0.50, 1.50),\n    \"gpt-3.5-turbo-1106\": (1.00, 2.00),\n    # Realtime models (audio pricing is different, this is for text fallback)\n    \"gpt-4o-realtime-preview\": (5.00, 20.00),\n    \"gpt-4o-realtime-preview-2024-10-01\": (5.00, 20.00),\n}\n\n\ndef calculate_cost(model: str, input_tokens: int, output_tokens: int) -> float:\n    \"\"\"Calculate cost in USD for a given model and token count.\n\n    Args:\n        model: Model name (e.g., \"gpt-4o\")\n        input_tokens: Number of input/prompt tokens\n        output_tokens: Number of output/completion tokens\n\n    Returns:\n        Cost in USD\n    \"\"\"\n    pricing = MODEL_PRICING.get(model)\n    if not pricing:\n        # Default to gpt-4o pricing if model not found\n        logger.warning(f\"Unknown model {model} for pricing, using gpt-4o pricing\")\n        pricing = MODEL_PRICING[\"gpt-4o\"]\n\n    input_price_per_1m, output_price_per_1m = pricing\n    input_cost = (input_tokens / 1_000_000) * input_price_per_1m\n    output_cost = (output_tokens / 1_000_000) * output_price_per_1m\n    return input_cost + output_cost\n\n\n@dataclass\nclass LLMRequest:\n    prompt: Optional[str] = None  # Single prompt (for simple queries)\n    messages: Optional[list] = None  # Full message history (for multi-turn with tools)\n    intent: IntentType = \"other\"\n    temperature: float = 0.1\n    max_tokens: int = 512\n    phi_present: bool = False\n    trace_id: Optional[str] = None\n    model_override: Optional[str] = None\n    model_provider: Optional[str] = None\n    # Tool/Function calling support\n    tools: Optional[list] = None  # List of tool definitions in OpenAI format\n    tool_choice: Optional[str] = \"auto\"  # \"auto\", \"required\", \"none\", or specific tool\n\n\n@dataclass\nclass ToolCall:\n    \"\"\"Represents a tool call from the LLM.\"\"\"\n\n    id: str\n    name: str\n    arguments: str  # JSON string of arguments\n\n\n@dataclass\nclass LLMResponse:\n    text: str\n    model_name: str\n    model_family: ModelFamily\n    used_tokens: int\n    latency_ms: float\n    finish_reason: str\n    cost_usd: float = 0.0  # Estimated cost in USD\n    input_tokens: int = 0\n    output_tokens: int = 0\n    # Tool/Function calling support\n    tool_calls: Optional[list] = None  # List of ToolCall objects\n\n\nclass LLMClient:\n    \"\"\"High-level interface for calling models.\n\n    This base class provides a common interface; concrete implementations\n    should override `_call_cloud` and `_call_local`.\n    \"\"\"\n\n    def __init__(\n        self,\n        cloud_model: str = \"gpt-4o\",\n        local_model: str = \"local-clinical-llm\",\n        openai_api_key: Optional[str] = None,\n        openai_timeout_sec: int = 30,\n        local_api_url: Optional[str] = None,\n        local_api_key: Optional[str] = None,\n        local_timeout_sec: int = 15,\n    ) -> None:\n        self.cloud_model = cloud_model\n        self.local_model = local_model\n\n        # Initialize OpenAI client for cloud models\n        self.openai_client = (\n            AsyncOpenAI(\n                api_key=openai_api_key,\n                timeout=openai_timeout_sec,\n            )\n            if openai_api_key\n            else None\n        )\n\n        if not self.openai_client:\n            logger.warning(\"OpenAI API key not provided. Cloud model calls will fail.\")\n\n        # Local model client (OpenAI-compatible endpoint such as vLLM/LLama.cpp)\n        self.local_client: Optional[httpx.AsyncClient] = None\n        if local_api_url:\n            headers = {}\n            if local_api_key:\n                headers[\"Authorization\"] = f\"Bearer {local_api_key}\"\n            self.local_client = httpx.AsyncClient(\n                base_url=local_api_url,\n                timeout=local_timeout_sec,\n                headers=headers or None,\n            )\n        self.has_local_model = self.local_client is not None\n\n    # Default system prompts for fallback when dynamic lookup fails\n    _DEFAULT_SYSTEM_PROMPTS = {\n        \"diagnosis\": (\n            \"You are a medical AI assistant specializing in clinical diagnosis. \"\n            \"Provide evidence-based diagnostic insights with appropriate citations.\"\n        ),\n        \"treatment\": (\n            \"You are a medical AI assistant specializing in treatment planning. \"\n            \"Provide evidence-based treatment recommendations with appropriate citations.\"\n        ),\n        \"drug\": (\n            \"You are a medical AI assistant specializing in pharmacology. \"\n            \"Provide evidence-based drug information with appropriate citations.\"\n        ),\n        \"guideline\": (\n            \"You are a medical AI assistant specializing in clinical guidelines. \"\n            \"Provide evidence-based guideline information with appropriate citations.\"\n        ),\n        \"summary\": (\n            \"You are a medical AI assistant specializing in medical summarization. \"\n            \"Provide clear, concise summaries with appropriate citations.\"\n        ),\n        \"other\": (\n            \"You are a helpful medical AI assistant. \"\n            \"Provide accurate, evidence-based information with appropriate citations when available.\"\n        ),\n    }\n\n    @staticmethod\n    def _get_default_system_prompt(intent: IntentType) -> str:\n        \"\"\"Get default system prompt for intent (fallback).\"\"\"\n        return LLMClient._DEFAULT_SYSTEM_PROMPTS.get(intent, LLMClient._DEFAULT_SYSTEM_PROMPTS[\"other\"])\n\n    async def _get_system_prompt_for_intent(self, intent: IntentType) -> str:\n        \"\"\"Get system prompt for intent with dynamic lookup and fallback.\n\n        Uses the PromptService for dynamic prompt management with fallback\n        to hardcoded defaults if the dynamic lookup fails.\n\n        Args:\n            intent: The intent type (diagnosis, treatment, drug, etc.)\n\n        Returns:\n            The system prompt text\n        \"\"\"\n        try:\n            # Import here to avoid circular imports\n            from app.services.prompt_service import prompt_service\n\n            # Try dynamic prompt lookup\n            prompt_text = await prompt_service.get_system_prompt_for_intent(intent, \"chat\")\n            if prompt_text:\n                return prompt_text\n        except Exception as e:\n            logger.warning(f\"Failed to get dynamic prompt for intent '{intent}': {e}\")\n\n        # Fallback to hardcoded default\n        return self._get_default_system_prompt(intent)\n\n    def _system_prompt_for_intent(self, intent: IntentType) -> str:\n        \"\"\"Synchronous fallback for system prompt lookup.\n\n        Note: Prefer using _get_system_prompt_for_intent() for async contexts.\n        This method exists for backward compatibility.\n        \"\"\"\n        return self._get_default_system_prompt(intent)\n\n    async def generate(self, req: LLMRequest) -> LLMResponse:\n        \"\"\"Select model family and generate a single response.\n\n        Selection strategy (initial version):\n\n        - If `req.phi_present` is True → route to local model.\n        - Else → route to cloud model.\n        - Later phases can add per-intent routing, cost-awareness, etc.\n\n        Safety checks:\n        - Validates prompt or messages is non-empty\n        - Normalizes whitespace\n        - Enforces reasonable max_tokens limits\n        \"\"\"\n        # Safety: validate prompt or messages is not empty\n        has_prompt = req.prompt and req.prompt.strip()\n        has_messages = req.messages and len(req.messages) > 0\n\n        if not has_prompt and not has_messages:\n            logger.warning(\n                \"LLMClient.generate called with empty prompt and no messages, trace_id=%s\",\n                req.trace_id,\n            )\n            raise ValueError(\"Prompt or messages cannot be empty\")\n\n        # Safety: normalize whitespace in prompt\n        if has_prompt:\n            req.prompt = \" \".join(req.prompt.split())\n\n        # Determine routing family\n        adapter_requests_local = req.model_provider not in (None, \"openai\", \"cloud\")\n        family: ModelFamily = \"local\" if (req.phi_present or adapter_requests_local) else \"cloud\"\n\n        # Safety: enforce max_tokens limits (see ORCHESTRATION_DESIGN.md)\n        # Cloud models: up to 4096 tokens, Local models: up to 2048 tokens\n        max_allowed_tokens = 2048 if family == \"local\" else 4096\n        if req.max_tokens > max_allowed_tokens:\n            logger.warning(\n                \"max_tokens=%d exceeds limit=%d for family=%s, capping. trace_id=%s\",\n                req.max_tokens,\n                max_allowed_tokens,\n                family,\n                req.trace_id,\n            )\n            req.max_tokens = max_allowed_tokens\n\n        if family == \"local\" and not self.has_local_model:\n            # Defensive: callers should not route to local path without a local model\n            raise RuntimeError(\"Local model requested but not configured\")\n        logger.debug(\n            \"LLMClient.generate: family=%s intent=%s phi_present=%s trace_id=%s\",\n            family,\n            req.intent,\n            req.phi_present,\n            req.trace_id,\n        )\n        if family == \"cloud\":\n            return await self._call_cloud(req)\n        return await self._call_local(req)\n\n    async def _call_cloud(self, req: LLMRequest) -> LLMResponse:\n        \"\"\"Call a cloud model using OpenAI API.\n\n        Uses the official OpenAI client library with proper error handling,\n        retry logic, circuit breaker protection, and cost tracking.\n\n        Tracks metrics: tokens (prompt + completion), latency, finish_reason\n        See ORCHESTRATION_DESIGN.md - \"Step 6: LLM Synthesis\"\n        See OBSERVABILITY.md for additional metrics to track\n        \"\"\"\n        if not self.openai_client:\n            logger.error(\n                \"OpenAI client not initialized. API key missing. trace_id=%s\",\n                req.trace_id,\n            )\n            raise RuntimeError(\"OpenAI API key not configured. Cannot call cloud model.\")\n\n        # Check circuit breaker before attempting call\n        try:\n            openai_breaker.call(lambda: None)  # Lightweight check\n        except CircuitBreakerError:\n            logger.error(\n                \"OpenAI circuit breaker is OPEN - failing fast. trace_id=%s\",\n                req.trace_id,\n            )\n            raise RuntimeError(\"OpenAI API is temporarily unavailable (circuit breaker open)\")\n\n        model_name = req.model_override or self.cloud_model\n\n        logger.info(\"Calling cloud model %s trace_id=%s\", model_name, req.trace_id)\n\n        backoff_seconds = [1, 2, 4]\n        last_error: Optional[Exception] = None\n\n        for attempt, delay in enumerate(backoff_seconds, start=1):\n            start_time = time.time()\n\n            try:\n                # Build messages for the API call\n                if req.messages:\n                    # Use provided messages (multi-turn conversation with tool results)\n                    # Ensure system prompt is at the start\n                    messages = req.messages.copy()\n                    if not messages or messages[0].get(\"role\") != \"system\":\n                        system_message = await self._get_system_prompt_for_intent(req.intent)\n                        messages.insert(0, {\"role\": \"system\", \"content\": system_message})\n                else:\n                    # Build messages from prompt (simple single-turn query)\n                    system_message = await self._get_system_prompt_for_intent(req.intent)\n                    messages = [\n                        {\"role\": \"system\", \"content\": system_message},\n                        {\"role\": \"user\", \"content\": req.prompt},\n                    ]\n\n                # Build API call parameters\n                api_params = {\n                    \"model\": model_name,\n                    \"messages\": messages,\n                    \"temperature\": req.temperature,\n                    \"max_tokens\": req.max_tokens,\n                }\n\n                # Add tools if provided\n                if req.tools:\n                    api_params[\"tools\"] = req.tools\n                    if req.tool_choice:\n                        api_params[\"tool_choice\"] = req.tool_choice\n\n                # Call OpenAI Chat Completions API with timeout\n                response: ChatCompletion = await asyncio.wait_for(\n                    self.openai_client.chat.completions.create(**api_params),\n                    timeout=30,\n                )\n\n                # Calculate latency\n                latency_ms = (time.time() - start_time) * 1000\n\n                # Extract response data\n                choice = response.choices[0]\n                text = choice.message.content or \"\"\n                finish_reason = choice.finish_reason or \"stop\"\n\n                # Extract tool calls if present\n                tool_calls_list = None\n                if choice.message.tool_calls:\n                    tool_calls_list = [\n                        ToolCall(\n                            id=tc.id,\n                            name=tc.function.name,\n                            arguments=tc.function.arguments,\n                        )\n                        for tc in choice.message.tool_calls\n                    ]\n\n                # Calculate token usage\n                usage = response.usage\n                total_tokens = usage.total_tokens if usage else 0\n\n                logger.info(\n                    \"Cloud model call successful: model=%s tokens=%d latency=%.2fms finish=%s trace_id=%s\",\n                    model_name,\n                    total_tokens,\n                    latency_ms,\n                    finish_reason,\n                    req.trace_id,\n                )\n\n                # Calculate and track cost\n                input_tokens = usage.prompt_tokens if usage else 0\n                output_tokens = usage.completion_tokens if usage else 0\n                cost_usd = calculate_cost(model_name, input_tokens, output_tokens)\n\n                # Track metrics via Prometheus\n                if usage:\n                    openai_tokens_used_total.labels(model=model_name, token_type=\"prompt\").inc(input_tokens)\n                    openai_tokens_used_total.labels(model=model_name, token_type=\"completion\").inc(output_tokens)\n                    openai_api_cost_dollars.inc(cost_usd)\n\n                logger.debug(\n                    \"Token breakdown: input=%d output=%d total=%d cost=$%.6f trace_id=%s\",\n                    input_tokens,\n                    output_tokens,\n                    total_tokens,\n                    cost_usd,\n                    req.trace_id,\n                )\n\n                return LLMResponse(\n                    text=text,\n                    model_name=model_name,\n                    model_family=\"cloud\",\n                    used_tokens=total_tokens,\n                    latency_ms=latency_ms,\n                    finish_reason=finish_reason,\n                    cost_usd=cost_usd,\n                    input_tokens=input_tokens,\n                    output_tokens=output_tokens,\n                    tool_calls=tool_calls_list,\n                )\n\n            except asyncio.TimeoutError as exc:\n                last_error = exc\n                logger.error(\n                    \"Cloud model call timed out (attempt %d) model=%s trace_id=%s\",\n                    attempt,\n                    model_name,\n                    req.trace_id,\n                )\n            except Exception as e:\n                last_error = e\n                latency_ms = (time.time() - start_time) * 1000\n                logger.error(\n                    \"Cloud model call failed (attempt %d): model=%s error=%s latency=%.2fms trace_id=%s\",\n                    attempt,\n                    model_name,\n                    str(e),\n                    latency_ms,\n                    req.trace_id,\n                    exc_info=True,\n                )\n\n            await asyncio.sleep(delay)\n\n        raise RuntimeError(f\"Failed to call cloud model {model_name} after retries: {last_error}\") from last_error\n\n    async def stream_generate(\n        self,\n        req: LLMRequest,\n        on_chunk: Optional[Callable[[str], Awaitable[None] | None]] = None,\n    ) -> LLMResponse:\n        \"\"\"\n        Stream a response from the cloud model and invoke a callback per text delta.\n\n        Args:\n            req: LLMRequest with prompt or messages and config\n            on_chunk: Optional callback to receive partial text chunks\n\n        Returns:\n            LLMResponse with aggregated text and usage metadata\n        \"\"\"\n        adapter_requests_local = req.model_provider not in (None, \"openai\", \"cloud\")\n        family: ModelFamily = \"local\" if (req.phi_present or adapter_requests_local) else \"cloud\"\n\n        if family == \"local\":\n            return await self._call_local(req)\n\n        if not self.openai_client:\n            logger.error(\n                \"OpenAI client not initialized. API key missing. trace_id=%s\",\n                req.trace_id,\n            )\n            raise RuntimeError(\"OpenAI API key not configured. Cannot call cloud model.\")\n\n        # Validate that we have either prompt or messages\n        has_prompt = req.prompt and req.prompt.strip()\n        has_messages = req.messages and len(req.messages) > 0\n\n        if not has_prompt and not has_messages:\n            raise ValueError(\"Prompt or messages cannot be empty\")\n\n        if has_prompt:\n            req.prompt = \" \".join(req.prompt.split())\n\n        max_allowed_tokens = 2048 if family == \"local\" else 4096\n        if req.max_tokens > max_allowed_tokens:\n            req.max_tokens = max_allowed_tokens\n\n        model_name = req.model_override or self.cloud_model\n\n        logger.info(\"Streaming cloud model %s trace_id=%s\", model_name, req.trace_id)\n\n        full_text: list[str] = []\n        start_time = time.time()\n        finish_reason: str = \"stop\"\n        usage = None\n        tool_calls_list = None\n\n        try:\n            # Build messages for the API call (same logic as _call_cloud)\n            if has_messages:\n                # Use provided messages (multi-turn conversation with tools)\n                messages = req.messages.copy()\n                if not messages or messages[0].get(\"role\") != \"system\":\n                    system_message = await self._get_system_prompt_for_intent(req.intent)\n                    messages.insert(0, {\"role\": \"system\", \"content\": system_message})\n            else:\n                # Build messages from prompt (simple single-turn query)\n                system_message = await self._get_system_prompt_for_intent(req.intent)\n                messages = [\n                    {\"role\": \"system\", \"content\": system_message},\n                    {\"role\": \"user\", \"content\": req.prompt},\n                ]\n\n            # Build API call parameters\n            api_params = {\n                \"model\": model_name,\n                \"messages\": messages,\n                \"temperature\": req.temperature,\n                \"max_tokens\": req.max_tokens,\n                \"stream\": True,\n            }\n\n            # Add tools if provided\n            if req.tools:\n                api_params[\"tools\"] = req.tools\n                if req.tool_choice:\n                    api_params[\"tool_choice\"] = req.tool_choice\n\n            stream = await self.openai_client.chat.completions.create(**api_params)\n\n            # Track tool calls accumulation (they come in chunks)\n            tool_calls_data: Dict[int, Dict] = {}  # index -> {id, name, arguments}\n\n            async for chunk in stream:\n                if isinstance(chunk, ChatCompletionChunk):\n                    delta = chunk.choices[0].delta\n                    text_piece = delta.content or \"\"\n                    if text_piece:\n                        full_text.append(text_piece)\n                        if on_chunk:\n                            result = on_chunk(text_piece)\n                            if inspect.isawaitable(result):\n                                await result\n\n                    # Handle tool calls in streaming (they come incrementally)\n                    if hasattr(delta, \"tool_calls\") and delta.tool_calls:\n                        for tc in delta.tool_calls:\n                            idx = tc.index\n                            if idx not in tool_calls_data:\n                                tool_calls_data[idx] = {\n                                    \"id\": \"\",\n                                    \"name\": \"\",\n                                    \"arguments\": \"\",\n                                }\n                            if tc.id:\n                                tool_calls_data[idx][\"id\"] = tc.id\n                            if tc.function:\n                                if tc.function.name:\n                                    tool_calls_data[idx][\"name\"] = tc.function.name\n                                if tc.function.arguments:\n                                    tool_calls_data[idx][\"arguments\"] += tc.function.arguments\n\n                    if chunk.choices[0].finish_reason:\n                        finish_reason = chunk.choices[0].finish_reason\n\n                    # Final chunk includes usage\n                    if getattr(chunk, \"usage\", None):\n                        usage = chunk.usage\n\n            latency_ms = (time.time() - start_time) * 1000\n            aggregated_text = \"\".join(full_text)\n\n            # Build tool_calls list if any were made\n            if tool_calls_data:\n                tool_calls_list = [\n                    ToolCall(\n                        id=tc_data[\"id\"],\n                        name=tc_data[\"name\"],\n                        arguments=tc_data[\"arguments\"],\n                    )\n                    for tc_data in tool_calls_data.values()\n                ]\n\n            input_tokens = usage.prompt_tokens if usage else 0\n            output_tokens = usage.completion_tokens if usage else 0\n            total_tokens = usage.total_tokens if usage else input_tokens + output_tokens\n            cost_usd = calculate_cost(model_name, input_tokens, output_tokens) if usage else 0.0\n\n            if usage:\n                openai_tokens_used_total.labels(model=model_name, token_type=\"prompt\").inc(input_tokens)\n                openai_tokens_used_total.labels(model=model_name, token_type=\"completion\").inc(output_tokens)\n                openai_api_cost_dollars.inc(cost_usd)\n\n            return LLMResponse(\n                text=aggregated_text,\n                model_name=model_name,\n                model_family=\"cloud\",\n                used_tokens=total_tokens,\n                latency_ms=latency_ms,\n                finish_reason=finish_reason,\n                cost_usd=cost_usd,\n                input_tokens=input_tokens,\n                output_tokens=output_tokens,\n                tool_calls=tool_calls_list,\n            )\n\n        except Exception as exc:  # noqa: BLE001\n            latency_ms = (time.time() - start_time) * 1000\n            logger.error(\n                \"Streaming cloud model failed: %s latency=%.2fms trace_id=%s\",\n                exc,\n                latency_ms,\n                req.trace_id,\n                exc_info=True,\n            )\n            raise\n\n    async def _call_local(self, req: LLMRequest) -> LLMResponse:\n        \"\"\"Call a local model for PHI-containing queries.\n\n        In production, this should call a local inference server (e.g., vLLM, Ollama)\n        running a HIPAA-compliant model on-premises.\n\n        See SECURITY_COMPLIANCE.md - \"PHI Routing\" for production requirements.\n        See BACKEND_ARCHITECTURE.md - \"Local LLM Service\" for architecture.\n        See OBSERVABILITY.md for metrics to track (tokens, latency).\n        \"\"\"\n        if not self.has_local_model or not self.local_client:\n            raise RuntimeError(\"Local model requested but not configured\")\n\n        model_name = req.model_override or self.local_model\n\n        logger.info(\"Calling local model %s trace_id=%s\", model_name, req.trace_id)\n\n        start_time = time.time()\n        try:\n            response = await self.local_client.post(\n                \"/v1/chat/completions\",\n                json={\n                    \"model\": model_name,\n                    \"messages\": [\n                        {\n                            \"role\": \"system\",\n                            \"content\": \"You are a local clinical assistant.\",\n                        },\n                        {\"role\": \"user\", \"content\": req.prompt},\n                    ],\n                    \"temperature\": req.temperature,\n                    \"max_tokens\": min(req.max_tokens, 2048),\n                },\n            )\n            response.raise_for_status()\n            payload = response.json()\n            choice = payload[\"choices\"][0]\n            text = choice[\"message\"].get(\"content\") or \"\"\n            finish_reason = choice.get(\"finish_reason\", \"stop\")\n\n            usage = payload.get(\"usage\", {})\n            total_tokens = usage.get(\"total_tokens\", 0)\n\n            latency_ms = (time.time() - start_time) * 1000\n            logger.info(\n                \"Local model call successful: model=%s tokens=%d latency=%.2fms finish=%s trace_id=%s\",\n                model_name,\n                total_tokens,\n                latency_ms,\n                finish_reason,\n                req.trace_id,\n            )\n\n            return LLMResponse(\n                text=text,\n                model_name=model_name,\n                model_family=\"local\",\n                used_tokens=total_tokens,\n                latency_ms=latency_ms,\n                finish_reason=finish_reason,\n            )\n\n        except httpx.RequestError as exc:\n            latency_ms = (time.time() - start_time) * 1000\n            logger.error(\n                \"Local model HTTP error: %s latency=%.2fms trace_id=%s\",\n                exc,\n                latency_ms,\n                req.trace_id,\n                exc_info=True,\n            )\n            raise RuntimeError(f\"Local model HTTP error: {exc}\") from exc\n        except Exception as exc:\n            latency_ms = (time.time() - start_time) * 1000\n            logger.error(\n                \"Local model call failed: %s latency=%.2fms trace_id=%s\",\n                exc,\n                latency_ms,\n                req.trace_id,\n                exc_info=True,\n            )\n            raise RuntimeError(f\"Local model call failed: {exc}\") from exc\n"
}
