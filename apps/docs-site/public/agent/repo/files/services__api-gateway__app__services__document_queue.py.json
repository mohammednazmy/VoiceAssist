{
  "path": "services/api-gateway/app/services/document_queue.py",
  "language": "python",
  "size": 7345,
  "last_modified": "2025-12-03T23:50:35.690Z",
  "lines": 257,
  "content": "\"\"\"Async document processing queue using ARQ (Phase 7 Integration Improvements).\n\nHandles asynchronous document upload and indexing to prevent HTTP timeouts\nand improve user experience for large documents.\n\nArchitecture:\n- Enqueue document processing tasks from API endpoints\n- Background worker processes tasks from Redis queue\n- Job status tracking for monitoring upload progress\n\"\"\"\n\nfrom datetime import timedelta\nfrom typing import Any, Dict, Optional\n\nfrom app.core.config import settings\nfrom app.core.logging import get_logger\nfrom app.services.kb_indexer import IndexingResult, KBIndexer\nfrom arq import ArqRedis, create_pool\nfrom arq.connections import RedisSettings\n\nlogger = get_logger(__name__)\n\n\n# ARQ Redis settings\nARQ_REDIS_SETTINGS = RedisSettings(\n    host=settings.REDIS_HOST,\n    port=settings.REDIS_PORT,\n    password=settings.REDIS_PASSWORD,\n    database=1,  # Use database 1 for ARQ (database 0 is for caching)\n)\n\n\nclass DocumentProcessingQueue:\n    \"\"\"Service for managing async document processing tasks.\"\"\"\n\n    def __init__(self):\n        self._redis_pool: Optional[ArqRedis] = None\n\n    async def get_redis_pool(self) -> ArqRedis:\n        \"\"\"Get or create ARQ Redis connection pool.\"\"\"\n        if self._redis_pool is None:\n            self._redis_pool = await create_pool(ARQ_REDIS_SETTINGS)\n        return self._redis_pool\n\n    async def enqueue_document(\n        self,\n        document_id: str,\n        file_content: bytes,\n        file_extension: str,\n        title: str,\n        source_type: str,\n        metadata: Dict[str, Any],\n    ) -> str:\n        \"\"\"\n        Enqueue a document for async processing.\n\n        Args:\n            document_id: Unique document identifier\n            file_content: Raw file bytes\n            file_extension: File extension (.pdf, .txt)\n            title: Document title\n            source_type: Source type (uploaded, guideline, etc.)\n            metadata: Additional metadata\n\n        Returns:\n            Job ID for tracking status\n        \"\"\"\n        redis = await self.get_redis_pool()\n\n        job = await redis.enqueue_job(\n            \"process_document\",\n            document_id,\n            file_content,\n            file_extension,\n            title,\n            source_type,\n            metadata,\n        )\n\n        logger.info(\n            \"document_enqueued\",\n            extra={\n                \"document_id\": document_id,\n                \"job_id\": job.job_id,\n                \"title\": title,\n                \"size_bytes\": len(file_content),\n                \"file_extension\": file_extension,\n            },\n        )\n\n        return job.job_id\n\n    async def get_job_status(self, job_id: str) -> Dict[str, Any]:\n        \"\"\"\n        Get status of a document processing job.\n\n        Args:\n            job_id: ARQ job identifier\n\n        Returns:\n            Job status dict with keys: status, result, error, enqueue_time, finish_time\n        \"\"\"\n        redis = await self.get_redis_pool()\n\n        job = await redis.get_job(job_id)\n\n        if not job:\n            return {\n                \"status\": \"not_found\",\n                \"error\": f\"Job {job_id} not found\",\n            }\n\n        status_map = {\n            \"queued\": \"pending\",\n            \"in_progress\": \"processing\",\n            \"complete\": \"completed\",\n            \"failed\": \"failed\",\n        }\n\n        return {\n            \"status\": status_map.get(job.status, job.status),\n            \"result\": job.result,\n            \"error\": job.error if hasattr(job, \"error\") else None,\n            \"enqueue_time\": job.enqueue_time.isoformat() if job.enqueue_time else None,\n            \"finish_time\": job.finish_time.isoformat() if job.finish_time else None,\n        }\n\n    async def close(self):\n        \"\"\"Close Redis connection pool.\"\"\"\n        if self._redis_pool:\n            await self._redis_pool.close()\n\n\n# Global instance\ndocument_queue = DocumentProcessingQueue()\n\n\nasync def process_document(\n    ctx: Dict[str, Any],\n    document_id: str,\n    file_content: bytes,\n    file_extension: str,\n    title: str,\n    source_type: str,\n    metadata: Dict[str, Any],\n) -> Dict[str, Any]:\n    \"\"\"\n    ARQ task function to process a document.\n\n    This function is called by ARQ workers in the background.\n\n    Args:\n        ctx: ARQ context dict\n        document_id: Document identifier\n        file_content: Raw file bytes\n        file_extension: File extension\n        title: Document title\n        source_type: Source type\n        metadata: Document metadata\n\n    Returns:\n        Processing result dict\n    \"\"\"\n    logger.info(\n        \"document_processing_started\",\n        extra={\n            \"document_id\": document_id,\n            \"title\": title,\n            \"size_bytes\": len(file_content),\n        },\n    )\n\n    try:\n        # Initialize KB Indexer\n        kb_indexer = KBIndexer()\n\n        # Process based on file type\n        if file_extension == \".pdf\":\n            result: IndexingResult = await kb_indexer.index_pdf_document(\n                pdf_bytes=file_content,\n                document_id=document_id,\n                title=title,\n                source_type=source_type,\n                metadata=metadata,\n            )\n        elif file_extension == \".txt\":\n            text_content = file_content.decode(\"utf-8\")\n            result: IndexingResult = await kb_indexer.index_document(\n                content=text_content,\n                document_id=document_id,\n                title=title,\n                source_type=source_type,\n                metadata=metadata,\n            )\n        else:\n            raise ValueError(f\"Unsupported file extension: {file_extension}\")\n\n        if result.success:\n            logger.info(\n                \"document_processing_completed\",\n                extra={\n                    \"document_id\": document_id,\n                    \"chunks_indexed\": result.chunks_indexed,\n                    \"title\": title,\n                },\n            )\n\n            return {\n                \"success\": True,\n                \"document_id\": document_id,\n                \"title\": title,\n                \"chunks_indexed\": result.chunks_indexed,\n            }\n        else:\n            logger.error(\n                \"document_processing_failed\",\n                extra={\n                    \"document_id\": document_id,\n                    \"error\": result.error_message,\n                    \"title\": title,\n                },\n            )\n\n            return {\n                \"success\": False,\n                \"document_id\": document_id,\n                \"error\": result.error_message,\n            }\n\n    except Exception as e:\n        logger.error(\n            \"document_processing_exception\",\n            extra={\n                \"document_id\": document_id,\n                \"error\": str(e),\n                \"title\": title,\n            },\n            exc_info=True,\n        )\n\n        return {\n            \"success\": False,\n            \"document_id\": document_id,\n            \"error\": str(e),\n        }\n\n\n# ARQ Worker class configuration\nclass WorkerSettings:\n    \"\"\"ARQ worker settings.\"\"\"\n\n    redis_settings = ARQ_REDIS_SETTINGS\n    functions = [process_document]\n    job_timeout = timedelta(minutes=30)  # 30 minutes for large documents\n    max_jobs = 5  # Process up to 5 documents concurrently\n    keep_result = timedelta(hours=24)  # Keep job results for 24 hours\n"
}
