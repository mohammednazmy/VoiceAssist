{
  "path": "services/api-gateway/app/services/speaker_diarization_service.py",
  "language": "python",
  "size": 32429,
  "last_modified": "2025-12-05T03:07:13.135Z",
  "lines": 971,
  "content": "\"\"\"\nSpeaker Diarization Service - Voice Mode v4.1 Phase 3\n\nMulti-speaker detection and attribution for conversations using pyannote.audio.\n\nFeatures:\n- Real-time speaker change detection with incremental processing\n- Speaker embedding extraction and database for recurring speakers\n- Integration with Thinker for multi-party context\n- Support for up to 4 concurrent speakers\n- Online and offline diarization modes\n- Speaker re-identification across sessions\n\nReference: docs/voice/phase3-implementation-plan.md\n\nFeature Flag: backend.voice_v4_speaker_diarization\n\nRequirements:\n    pip install pyannote.audio torch torchaudio scipy\n\nEnvironment Variables:\n    HUGGINGFACE_TOKEN: HuggingFace API token for model access\n\"\"\"\n\nimport asyncio\nimport hashlib\nimport io\nimport logging\nimport time\nfrom dataclasses import dataclass, field\nfrom datetime import datetime, timezone\nfrom enum import Enum\nfrom typing import Any, Callable, Dict, List, Optional, Tuple\n\nimport numpy as np\nfrom app.core.config import settings\nfrom app.core.feature_flags import feature_flag_service\n\nlogger = logging.getLogger(__name__)\n\n# Type aliases\nAudioCallback = Callable[[bytes], None]\nSpeakerChangeCallback = Callable[[\"SpeakerSegment\"], None]\n\n\n# ==============================================================================\n# Data Classes\n# ==============================================================================\n\n\n@dataclass\nclass SpeakerSegment:\n    \"\"\"A segment of audio attributed to a speaker.\"\"\"\n\n    speaker_id: str\n    start_ms: int\n    end_ms: int\n    confidence: float\n    embedding: Optional[List[float]] = None\n\n    @property\n    def duration_ms(self) -> int:\n        return self.end_ms - self.start_ms\n\n    def to_dict(self) -> Dict[str, Any]:\n        return {\n            \"speakerId\": self.speaker_id,\n            \"startMs\": self.start_ms,\n            \"endMs\": self.end_ms,\n            \"confidence\": round(self.confidence, 3),\n            \"durationMs\": self.duration_ms,\n        }\n\n\n@dataclass\nclass SpeakerProfile:\n    \"\"\"Profile for a known/recurring speaker.\"\"\"\n\n    speaker_id: str\n    name: Optional[str] = None\n    embedding: Optional[List[float]] = None\n    voice_samples: int = 0\n    first_seen: datetime = field(default_factory=lambda: datetime.now(timezone.utc))\n    last_seen: datetime = field(default_factory=lambda: datetime.now(timezone.utc))\n    metadata: Dict[str, Any] = field(default_factory=dict)\n\n\n@dataclass\nclass DiarizationResult:\n    \"\"\"Result of speaker diarization on an audio segment.\"\"\"\n\n    segments: List[SpeakerSegment]\n    num_speakers: int\n    total_duration_ms: int\n    processing_time_ms: float\n    model_version: str = \"pyannote-3.1\"\n\n    def get_speaker_summary(self) -> Dict[str, int]:\n        \"\"\"Get total speaking time per speaker.\"\"\"\n        summary: Dict[str, int] = {}\n        for seg in self.segments:\n            summary[seg.speaker_id] = summary.get(seg.speaker_id, 0) + seg.duration_ms\n        return summary\n\n    def to_dict(self) -> Dict[str, Any]:\n        return {\n            \"segments\": [s.to_dict() for s in self.segments],\n            \"numSpeakers\": self.num_speakers,\n            \"totalDurationMs\": self.total_duration_ms,\n            \"processingTimeMs\": round(self.processing_time_ms, 2),\n            \"modelVersion\": self.model_version,\n            \"speakerSummary\": self.get_speaker_summary(),\n        }\n\n\nclass DiarizationMode(str, Enum):\n    \"\"\"Diarization processing mode.\"\"\"\n\n    OFFLINE = \"offline\"  # Process complete audio file\n    STREAMING = \"streaming\"  # Real-time streaming diarization\n    INCREMENTAL = \"incremental\"  # Process audio chunks incrementally\n\n\n# ==============================================================================\n# Configuration\n# ==============================================================================\n\n\n@dataclass\nclass DiarizationConfig:\n    \"\"\"Configuration for speaker diarization.\"\"\"\n\n    # Model settings\n    model_name: str = \"pyannote/speaker-diarization-3.1\"\n    # Pin model revisions for supply chain security (Bandit B615)\n    # Update these hashes when upgrading model versions\n    model_revision: str = \"cb03e11cae0c1f3c75fd7be406b7f0bbf33cd28c\"  # v3.1.0\n    use_gpu: bool = True\n    device: str = \"cuda\"  # \"cuda\" or \"cpu\"\n\n    # Detection settings\n    min_speakers: int = 1\n    max_speakers: int = 4\n    min_segment_duration_ms: int = 200\n    min_cluster_size: int = 3\n\n    # Embedding settings\n    embedding_model: str = \"pyannote/embedding\"\n    embedding_model_revision: str = \"a9b3e59b43ceb4a4b04fb82bc7a1c36da47fe18a\"  # Latest stable\n    embedding_dim: int = 512\n    similarity_threshold: float = 0.75  # For speaker matching\n\n    # Streaming settings\n    chunk_duration_ms: int = 1000\n    overlap_duration_ms: int = 200\n\n    # Performance settings\n    max_processing_time_ms: int = 500  # Target latency\n\n\n# ==============================================================================\n# Speaker Diarization Service\n# ==============================================================================\n\n\nclass SpeakerDiarizationService:\n    \"\"\"\n    Service for multi-speaker detection and attribution.\n\n    Uses pyannote.audio for state-of-the-art speaker diarization.\n    Supports both offline and streaming modes.\n\n    Usage:\n        service = SpeakerDiarizationService()\n        await service.initialize()\n\n        # Offline mode\n        result = await service.process_audio(audio_bytes)\n\n        # Streaming mode\n        async for segment in service.stream_diarization(audio_stream):\n            print(f\"Speaker {segment.speaker_id}: {segment.start_ms}-{segment.end_ms}\")\n    \"\"\"\n\n    def __init__(self, config: Optional[DiarizationConfig] = None):\n        self.config = config or DiarizationConfig()\n        self._pipeline = None\n        self._embedding_model = None\n        self._speaker_profiles: Dict[str, SpeakerProfile] = {}\n        self._initialized = False\n        self._lock = asyncio.Lock()\n\n    async def initialize(self) -> bool:\n        \"\"\"\n        Initialize the diarization pipeline.\n\n        Returns:\n            True if initialization successful, False otherwise.\n        \"\"\"\n        if self._initialized:\n            return True\n\n        async with self._lock:\n            if self._initialized:\n                return True\n\n            try:\n                # Check feature flag\n                if not await feature_flag_service.is_enabled(\"backend.voice_v4_speaker_diarization\"):\n                    logger.info(\"Speaker diarization feature flag is disabled\")\n                    return False\n\n                # Lazy import pyannote to avoid loading if not needed\n                from pyannote.audio import Pipeline\n\n                logger.info(\n                    f\"Loading diarization model: {self.config.model_name}\",\n                    extra={\"device\": self.config.device},\n                )\n\n                # Load the diarization pipeline with pinned revision (Bandit B615)\n                self._pipeline = Pipeline.from_pretrained(  # nosec B615 - revision pinned\n                    self.config.model_name,\n                    revision=self.config.model_revision,\n                    use_auth_token=settings.huggingface_token,\n                )\n\n                # Move to GPU if available\n                if self.config.use_gpu:\n                    import torch\n\n                    if torch.cuda.is_available():\n                        self._pipeline.to(torch.device(self.config.device))\n                        logger.info(\"Diarization pipeline moved to GPU\")\n                    else:\n                        logger.warning(\"GPU requested but not available, using CPU\")\n\n                self._initialized = True\n                logger.info(\"Speaker diarization service initialized successfully\")\n                return True\n\n            except ImportError as e:\n                logger.error(f\"Failed to import pyannote: {e}\")\n                logger.info(\"Install with: pip install pyannote.audio\")\n                return False\n            except Exception as e:\n                logger.error(f\"Failed to initialize diarization service: {e}\")\n                return False\n\n    async def process_audio(\n        self,\n        audio_data: bytes,\n        sample_rate: int = 16000,\n        num_speakers: Optional[int] = None,\n    ) -> Optional[DiarizationResult]:\n        \"\"\"\n        Process audio and detect speakers (offline mode).\n\n        Args:\n            audio_data: Raw audio bytes (WAV format)\n            sample_rate: Audio sample rate (default 16kHz)\n            num_speakers: Expected number of speakers (optional)\n\n        Returns:\n            DiarizationResult with speaker segments, or None on failure.\n        \"\"\"\n        if not self._initialized:\n            if not await self.initialize():\n                return None\n\n        start_time = time.monotonic()\n\n        try:\n            import io\n\n            import numpy as np\n            import torch\n            from scipy.io import wavfile\n\n            # Convert bytes to numpy array\n            audio_file = io.BytesIO(audio_data)\n            sr, audio_np = wavfile.read(audio_file)\n\n            # Resample if needed\n            if sr != sample_rate:\n                from scipy import signal\n\n                audio_np = signal.resample(audio_np, int(len(audio_np) * sample_rate / sr))\n\n            # Normalize to float32\n            if audio_np.dtype == np.int16:\n                audio_np = audio_np.astype(np.float32) / 32768.0\n\n            # Create torch tensor\n            waveform = torch.from_numpy(audio_np).unsqueeze(0)\n\n            # Run diarization\n            diarization_params = {}\n            if num_speakers:\n                diarization_params[\"num_speakers\"] = num_speakers\n            elif self.config.min_speakers == self.config.max_speakers:\n                diarization_params[\"num_speakers\"] = self.config.max_speakers\n            else:\n                diarization_params[\"min_speakers\"] = self.config.min_speakers\n                diarization_params[\"max_speakers\"] = self.config.max_speakers\n\n            diarization = self._pipeline(\n                {\"waveform\": waveform, \"sample_rate\": sample_rate},\n                **diarization_params,\n            )\n\n            # Convert to segments\n            segments = []\n            for turn, _, speaker in diarization.itertracks(yield_label=True):\n                segment = SpeakerSegment(\n                    speaker_id=speaker,\n                    start_ms=int(turn.start * 1000),\n                    end_ms=int(turn.end * 1000),\n                    confidence=0.85,  # pyannote doesn't provide per-segment confidence\n                )\n                segments.append(segment)\n\n            # Count unique speakers\n            unique_speakers = set(s.speaker_id for s in segments)\n\n            processing_time_ms = (time.monotonic() - start_time) * 1000\n            total_duration_ms = int(len(audio_np) / sample_rate * 1000)\n\n            result = DiarizationResult(\n                segments=segments,\n                num_speakers=len(unique_speakers),\n                total_duration_ms=total_duration_ms,\n                processing_time_ms=processing_time_ms,\n            )\n\n            logger.info(\n                \"Diarization complete\",\n                extra={\n                    \"num_speakers\": result.num_speakers,\n                    \"num_segments\": len(segments),\n                    \"duration_ms\": total_duration_ms,\n                    \"processing_ms\": processing_time_ms,\n                },\n            )\n\n            return result\n\n        except Exception as e:\n            logger.error(f\"Diarization failed: {e}\")\n            return None\n\n    async def stream_diarization(\n        self,\n        audio_stream,\n        sample_rate: int = 16000,\n    ):\n        \"\"\"\n        Stream speaker diarization for real-time audio.\n\n        Args:\n            audio_stream: Async generator yielding audio chunks\n            sample_rate: Audio sample rate\n\n        Yields:\n            SpeakerSegment for each detected speaker change.\n        \"\"\"\n        if not self._initialized:\n            if not await self.initialize():\n                return\n\n        # Placeholder for streaming implementation\n        # Real implementation would use incremental diarization\n        logger.warning(\"Streaming diarization not yet implemented - using chunked mode\")\n\n        buffer = bytearray()\n        chunk_size = int(sample_rate * self.config.chunk_duration_ms / 1000 * 2)\n\n        async for audio_chunk in audio_stream:\n            buffer.extend(audio_chunk)\n\n            if len(buffer) >= chunk_size:\n                # Process chunk\n                result = await self.process_audio(bytes(buffer), sample_rate)\n                if result:\n                    for segment in result.segments:\n                        yield segment\n\n                # Keep overlap for continuity\n                overlap_bytes = int(sample_rate * self.config.overlap_duration_ms / 1000 * 2)\n                buffer = buffer[-overlap_bytes:]\n\n    def get_speaker_profile(self, speaker_id: str) -> Optional[SpeakerProfile]:\n        \"\"\"Get or create speaker profile.\"\"\"\n        if speaker_id not in self._speaker_profiles:\n            self._speaker_profiles[speaker_id] = SpeakerProfile(speaker_id=speaker_id)\n        return self._speaker_profiles.get(speaker_id)\n\n    def update_speaker_profile(\n        self,\n        speaker_id: str,\n        name: Optional[str] = None,\n        embedding: Optional[List[float]] = None,\n        metadata: Optional[Dict[str, Any]] = None,\n    ) -> SpeakerProfile:\n        \"\"\"Update speaker profile with new information.\"\"\"\n        profile = self.get_speaker_profile(speaker_id)\n        if profile:\n            if name:\n                profile.name = name\n            if embedding:\n                profile.embedding = embedding\n            if metadata:\n                profile.metadata.update(metadata)\n            profile.last_seen = datetime.now(timezone.utc)\n            profile.voice_samples += 1\n        return profile\n\n    async def match_speaker(\n        self,\n        embedding: List[float],\n        threshold: Optional[float] = None,\n    ) -> Optional[Tuple[str, float]]:\n        \"\"\"\n        Match an embedding to known speakers.\n\n        Args:\n            embedding: Speaker embedding vector\n            threshold: Similarity threshold (default from config)\n\n        Returns:\n            Tuple of (speaker_id, similarity_score) or None if no match.\n        \"\"\"\n        threshold = threshold or self.config.similarity_threshold\n        best_match: Optional[Tuple[str, float]] = None\n\n        for speaker_id, profile in self._speaker_profiles.items():\n            if profile.embedding:\n                similarity = self._compute_similarity(embedding, profile.embedding)\n                if similarity >= threshold:\n                    if best_match is None or similarity > best_match[1]:\n                        best_match = (speaker_id, similarity)\n\n        return best_match\n\n    def _compute_similarity(self, embedding1: List[float], embedding2: List[float]) -> float:\n        \"\"\"Compute cosine similarity between embeddings.\"\"\"\n        import numpy as np\n\n        e1 = np.array(embedding1)\n        e2 = np.array(embedding2)\n\n        dot_product = np.dot(e1, e2)\n        norm1 = np.linalg.norm(e1)\n        norm2 = np.linalg.norm(e2)\n\n        if norm1 == 0 or norm2 == 0:\n            return 0.0\n\n        return float(dot_product / (norm1 * norm2))\n\n\n# ==============================================================================\n# Speaker Embedding Extractor\n# ==============================================================================\n\n\nclass SpeakerEmbeddingExtractor:\n    \"\"\"\n    Extracts speaker embeddings from audio for speaker re-identification.\n\n    Uses pyannote/embedding model for voice print extraction.\n    \"\"\"\n\n    # Default revision for supply chain security (Bandit B615)\n    DEFAULT_REVISION = \"a9b3e59b43ceb4a4b04fb82bc7a1c36da47fe18a\"\n\n    def __init__(\n        self,\n        model_name: str = \"pyannote/embedding\",\n        device: str = \"cuda\",\n        revision: str = DEFAULT_REVISION,\n    ):\n        self.model_name = model_name\n        self.device = device\n        self.revision = revision\n        self._model = None\n        self._initialized = False\n\n    async def initialize(self) -> bool:\n        \"\"\"Load the embedding model.\"\"\"\n        if self._initialized:\n            return True\n\n        try:\n            import torch\n            from pyannote.audio import Inference, Model\n\n            # Load embedding model with pinned revision (Bandit B615)\n            self._model = Model.from_pretrained(  # nosec B615 - revision pinned\n                self.model_name,\n                revision=self.revision,\n                use_auth_token=settings.huggingface_token,\n            )\n\n            # Move to GPU if available\n            if self.device == \"cuda\" and torch.cuda.is_available():\n                self._model.to(torch.device(\"cuda\"))\n                logger.info(\"Embedding model moved to GPU\")\n            else:\n                self._model.to(torch.device(\"cpu\"))\n\n            # Create inference object\n            self._inference = Inference(self._model, window=\"whole\")\n\n            self._initialized = True\n            logger.info(\"Speaker embedding extractor initialized\")\n            return True\n\n        except Exception as e:\n            logger.error(f\"Failed to initialize embedding extractor: {e}\")\n            return False\n\n    async def extract_embedding(\n        self,\n        audio_data: bytes,\n        sample_rate: int = 16000,\n    ) -> Optional[List[float]]:\n        \"\"\"\n        Extract speaker embedding from audio.\n\n        Args:\n            audio_data: Raw audio bytes (WAV format or PCM)\n            sample_rate: Audio sample rate\n\n        Returns:\n            512-dimensional embedding vector, or None on failure.\n        \"\"\"\n        if not self._initialized:\n            if not await self.initialize():\n                return None\n\n        try:\n            import torch\n            from scipy.io import wavfile\n\n            # Convert bytes to numpy array\n            audio_file = io.BytesIO(audio_data)\n            try:\n                sr, audio_np = wavfile.read(audio_file)\n            except Exception:\n                # Try raw PCM\n                audio_np = np.frombuffer(audio_data, dtype=np.int16)\n                sr = sample_rate\n\n            # Resample if needed\n            if sr != sample_rate:\n                from scipy import signal\n\n                audio_np = signal.resample(audio_np, int(len(audio_np) * sample_rate / sr))\n\n            # Normalize to float32\n            if audio_np.dtype == np.int16:\n                audio_np = audio_np.astype(np.float32) / 32768.0\n\n            # Create torch tensor\n            waveform = torch.from_numpy(audio_np).unsqueeze(0)\n\n            # Extract embedding\n            embedding = self._inference({\"waveform\": waveform, \"sample_rate\": sample_rate})\n\n            return embedding.tolist()\n\n        except Exception as e:\n            logger.error(f\"Embedding extraction failed: {e}\")\n            return None\n\n    async def compare_embeddings(\n        self,\n        embedding1: List[float],\n        embedding2: List[float],\n    ) -> float:\n        \"\"\"\n        Compare two speaker embeddings.\n\n        Returns:\n            Cosine similarity score (0.0 to 1.0).\n        \"\"\"\n        e1 = np.array(embedding1)\n        e2 = np.array(embedding2)\n\n        dot_product = np.dot(e1, e2)\n        norm1 = np.linalg.norm(e1)\n        norm2 = np.linalg.norm(e2)\n\n        if norm1 == 0 or norm2 == 0:\n            return 0.0\n\n        return float(dot_product / (norm1 * norm2))\n\n\n# ==============================================================================\n# Streaming Diarization Session\n# ==============================================================================\n\n\nclass StreamingDiarizationSession:\n    \"\"\"\n    Manages a streaming diarization session for real-time speaker tracking.\n\n    Provides incremental diarization with speaker change callbacks.\n    \"\"\"\n\n    def __init__(\n        self,\n        session_id: str,\n        diarization_service: \"SpeakerDiarizationService\",\n        sample_rate: int = 16000,\n        chunk_duration_ms: int = 1000,\n        overlap_ms: int = 200,\n    ):\n        self.session_id = session_id\n        self.service = diarization_service\n        self.sample_rate = sample_rate\n        self.chunk_duration_ms = chunk_duration_ms\n        self.overlap_ms = overlap_ms\n\n        self._buffer = bytearray()\n        self._segments: List[SpeakerSegment] = []\n        self._current_speaker: Optional[str] = None\n        self._speaker_change_callbacks: List[SpeakerChangeCallback] = []\n        self._total_duration_ms = 0\n        self._is_active = False\n        self._lock = asyncio.Lock()\n\n    def on_speaker_change(self, callback: SpeakerChangeCallback) -> None:\n        \"\"\"Register callback for speaker change events.\"\"\"\n        self._speaker_change_callbacks.append(callback)\n\n    async def start(self) -> None:\n        \"\"\"Start the streaming session.\"\"\"\n        self._is_active = True\n        self._buffer = bytearray()\n        self._segments = []\n        self._current_speaker = None\n        logger.info(f\"Started streaming diarization session: {self.session_id}\")\n\n    async def process_chunk(self, audio_chunk: bytes) -> Optional[SpeakerSegment]:\n        \"\"\"\n        Process an audio chunk and return speaker segment if detected.\n\n        Args:\n            audio_chunk: Raw PCM audio data\n\n        Returns:\n            SpeakerSegment if speaker detected/changed, None otherwise.\n        \"\"\"\n        if not self._is_active:\n            return None\n\n        async with self._lock:\n            self._buffer.extend(audio_chunk)\n\n            # Calculate chunk size in bytes (16-bit audio = 2 bytes per sample)\n            chunk_bytes = int(self.sample_rate * self.chunk_duration_ms / 1000 * 2)\n\n            if len(self._buffer) < chunk_bytes:\n                return None\n\n            # Extract chunk for processing\n            audio_bytes = bytes(self._buffer[:chunk_bytes])\n\n            # Keep overlap for continuity\n            overlap_bytes = int(self.sample_rate * self.overlap_ms / 1000 * 2)\n            self._buffer = self._buffer[-overlap_bytes:] if overlap_bytes > 0 else bytearray()\n\n            # Process with diarization service\n            result = await self.service.process_audio(\n                audio_bytes,\n                sample_rate=self.sample_rate,\n                num_speakers=None,  # Auto-detect\n            )\n\n            if not result or not result.segments:\n                return None\n\n            # Get dominant speaker in this chunk\n            speaker_times: Dict[str, int] = {}\n            for seg in result.segments:\n                speaker_times[seg.speaker_id] = speaker_times.get(seg.speaker_id, 0) + seg.duration_ms\n\n            if not speaker_times:\n                return None\n\n            dominant_speaker = max(speaker_times.keys(), key=lambda k: speaker_times[k])\n\n            # Check for speaker change\n            speaker_changed = self._current_speaker is not None and dominant_speaker != self._current_speaker\n\n            # Create segment\n            segment = SpeakerSegment(\n                speaker_id=dominant_speaker,\n                start_ms=self._total_duration_ms,\n                end_ms=self._total_duration_ms + self.chunk_duration_ms,\n                confidence=0.85,\n            )\n\n            self._segments.append(segment)\n            self._total_duration_ms += self.chunk_duration_ms\n\n            # Fire callbacks if speaker changed\n            if speaker_changed:\n                for callback in self._speaker_change_callbacks:\n                    try:\n                        callback(segment)\n                    except Exception as e:\n                        logger.error(f\"Speaker change callback error: {e}\")\n\n            self._current_speaker = dominant_speaker\n            return segment\n\n    async def stop(self) -> DiarizationResult:\n        \"\"\"\n        Stop the session and return final diarization result.\n\n        Returns:\n            DiarizationResult with all detected segments.\n        \"\"\"\n        self._is_active = False\n\n        # Process any remaining buffer\n        if len(self._buffer) > 0:\n            await self.process_chunk(b\"\")  # Force process remaining\n\n        unique_speakers = set(s.speaker_id for s in self._segments)\n\n        result = DiarizationResult(\n            segments=self._segments,\n            num_speakers=len(unique_speakers),\n            total_duration_ms=self._total_duration_ms,\n            processing_time_ms=0,  # Accumulated over session\n        )\n\n        logger.info(\n            f\"Stopped streaming session {self.session_id}\",\n            extra={\n                \"num_speakers\": result.num_speakers,\n                \"num_segments\": len(self._segments),\n                \"total_duration_ms\": self._total_duration_ms,\n            },\n        )\n\n        return result\n\n    @property\n    def current_speaker(self) -> Optional[str]:\n        \"\"\"Get the current speaker ID.\"\"\"\n        return self._current_speaker\n\n    @property\n    def is_active(self) -> bool:\n        \"\"\"Check if session is active.\"\"\"\n        return self._is_active\n\n\n# ==============================================================================\n# Speaker Database\n# ==============================================================================\n\n\nclass SpeakerDatabase:\n    \"\"\"\n    In-memory database for speaker profiles with persistence support.\n\n    Provides speaker re-identification across sessions.\n    \"\"\"\n\n    def __init__(self):\n        self._profiles: Dict[str, SpeakerProfile] = {}\n        self._embedding_extractor: Optional[SpeakerEmbeddingExtractor] = None\n\n    def set_embedding_extractor(self, extractor: SpeakerEmbeddingExtractor) -> None:\n        \"\"\"Set the embedding extractor for voice print operations.\"\"\"\n        self._embedding_extractor = extractor\n\n    def add_profile(self, profile: SpeakerProfile) -> None:\n        \"\"\"Add or update a speaker profile.\"\"\"\n        self._profiles[profile.speaker_id] = profile\n\n    def get_profile(self, speaker_id: str) -> Optional[SpeakerProfile]:\n        \"\"\"Get a speaker profile by ID.\"\"\"\n        return self._profiles.get(speaker_id)\n\n    def get_all_profiles(self) -> List[SpeakerProfile]:\n        \"\"\"Get all speaker profiles.\"\"\"\n        return list(self._profiles.values())\n\n    async def find_by_embedding(\n        self,\n        embedding: List[float],\n        threshold: float = 0.75,\n    ) -> Optional[Tuple[SpeakerProfile, float]]:\n        \"\"\"\n        Find a speaker profile by voice embedding.\n\n        Args:\n            embedding: Speaker embedding vector\n            threshold: Minimum similarity threshold\n\n        Returns:\n            Tuple of (profile, similarity) or None if no match.\n        \"\"\"\n        best_match: Optional[Tuple[SpeakerProfile, float]] = None\n\n        for profile in self._profiles.values():\n            if profile.embedding:\n                similarity = self._cosine_similarity(embedding, profile.embedding)\n                if similarity >= threshold:\n                    if best_match is None or similarity > best_match[1]:\n                        best_match = (profile, similarity)\n\n        return best_match\n\n    def _cosine_similarity(self, e1: List[float], e2: List[float]) -> float:\n        \"\"\"Compute cosine similarity.\"\"\"\n        a1 = np.array(e1)\n        a2 = np.array(e2)\n\n        dot = np.dot(a1, a2)\n        n1 = np.linalg.norm(a1)\n        n2 = np.linalg.norm(a2)\n\n        if n1 == 0 or n2 == 0:\n            return 0.0\n\n        return float(dot / (n1 * n2))\n\n    async def identify_or_create(\n        self,\n        embedding: List[float],\n        threshold: float = 0.75,\n        name: Optional[str] = None,\n    ) -> SpeakerProfile:\n        \"\"\"\n        Identify an existing speaker or create a new profile.\n\n        Args:\n            embedding: Speaker embedding vector\n            threshold: Similarity threshold for matching\n            name: Optional name for new speaker\n\n        Returns:\n            Matched or newly created SpeakerProfile.\n        \"\"\"\n        # Try to find existing speaker\n        match = await self.find_by_embedding(embedding, threshold)\n\n        if match:\n            profile, similarity = match\n            profile.last_seen = datetime.now(timezone.utc)\n            profile.voice_samples += 1\n            logger.info(f\"Identified speaker {profile.speaker_id} (similarity: {similarity:.3f})\")\n            return profile\n\n        # Create new profile\n        speaker_id = self._generate_speaker_id()\n        profile = SpeakerProfile(\n            speaker_id=speaker_id,\n            name=name,\n            embedding=embedding,\n            voice_samples=1,\n        )\n        self.add_profile(profile)\n        logger.info(f\"Created new speaker profile: {speaker_id}\")\n        return profile\n\n    def _generate_speaker_id(self) -> str:\n        \"\"\"Generate a unique speaker ID.\"\"\"\n        timestamp = datetime.now(timezone.utc).isoformat()\n        hash_input = f\"{timestamp}-{len(self._profiles)}\"\n        # MD5 used for ID generation, not security purposes\n        return f\"spk_{hashlib.md5(hash_input.encode(), usedforsecurity=False).hexdigest()[:8]}\"\n\n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Export database to dictionary for persistence.\"\"\"\n        return {\n            \"profiles\": [\n                {\n                    \"speaker_id\": p.speaker_id,\n                    \"name\": p.name,\n                    \"embedding\": p.embedding,\n                    \"voice_samples\": p.voice_samples,\n                    \"first_seen\": p.first_seen.isoformat(),\n                    \"last_seen\": p.last_seen.isoformat(),\n                    \"metadata\": p.metadata,\n                }\n                for p in self._profiles.values()\n            ]\n        }\n\n    def load_from_dict(self, data: Dict[str, Any]) -> None:\n        \"\"\"Load database from dictionary.\"\"\"\n        self._profiles = {}\n        for p in data.get(\"profiles\", []):\n            profile = SpeakerProfile(\n                speaker_id=p[\"speaker_id\"],\n                name=p.get(\"name\"),\n                embedding=p.get(\"embedding\"),\n                voice_samples=p.get(\"voice_samples\", 0),\n                first_seen=datetime.fromisoformat(p.get(\"first_seen\", datetime.now(timezone.utc).isoformat())),\n                last_seen=datetime.fromisoformat(p.get(\"last_seen\", datetime.now(timezone.utc).isoformat())),\n                metadata=p.get(\"metadata\", {}),\n            )\n            self._profiles[profile.speaker_id] = profile\n\n\n# ==============================================================================\n# Singleton Instance\n# ==============================================================================\n\n_speaker_diarization_service: Optional[SpeakerDiarizationService] = None\n_speaker_database: Optional[SpeakerDatabase] = None\n_embedding_extractor: Optional[SpeakerEmbeddingExtractor] = None\n\n\ndef get_speaker_diarization_service() -> SpeakerDiarizationService:\n    \"\"\"Get or create speaker diarization service instance.\"\"\"\n    global _speaker_diarization_service\n    if _speaker_diarization_service is None:\n        _speaker_diarization_service = SpeakerDiarizationService()\n    return _speaker_diarization_service\n\n\ndef get_speaker_database() -> SpeakerDatabase:\n    \"\"\"Get or create speaker database instance.\"\"\"\n    global _speaker_database\n    if _speaker_database is None:\n        _speaker_database = SpeakerDatabase()\n    return _speaker_database\n\n\ndef get_embedding_extractor() -> SpeakerEmbeddingExtractor:\n    \"\"\"Get or create speaker embedding extractor instance.\"\"\"\n    global _embedding_extractor\n    if _embedding_extractor is None:\n        _embedding_extractor = SpeakerEmbeddingExtractor()\n    return _embedding_extractor\n\n\nasync def create_streaming_session(\n    session_id: str,\n    sample_rate: int = 16000,\n) -> StreamingDiarizationSession:\n    \"\"\"\n    Create a new streaming diarization session.\n\n    Args:\n        session_id: Unique session identifier\n        sample_rate: Audio sample rate\n\n    Returns:\n        StreamingDiarizationSession instance.\n    \"\"\"\n    service = get_speaker_diarization_service()\n    session = StreamingDiarizationSession(\n        session_id=session_id,\n        diarization_service=service,\n        sample_rate=sample_rate,\n    )\n    await session.start()\n    return session\n"
}
