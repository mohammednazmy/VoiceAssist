{
  "path": "services/api-gateway/app/services/rag_cache.py",
  "language": "python",
  "size": 14517,
  "last_modified": "2025-12-04T11:26:59.282Z",
  "lines": 440,
  "content": "\"\"\"RAG Result Caching Service.\n\nCaches semantic search results to reduce load on vector database and embeddings API.\nImplements intelligent caching with:\n- Query normalization for cache key generation\n- Embedding caching to avoid repeated OpenAI API calls\n- Search result caching with configurable TTL\n- Cache invalidation on document updates\n- Hit rate tracking and metrics\n\nCache Strategy:\n- Query Embeddings: 24-hour TTL (embeddings rarely need regeneration)\n- Search Results: 1-hour TTL (balance between freshness and performance)\n- Document Metadata: 2-hour TTL (document metadata is relatively stable)\n\nUsage:\n    from app.services.rag_cache import rag_cache\n\n    # Cache search results\n    cache_key = rag_cache.generate_search_key(query, filters)\n    cached = await rag_cache.get_search_results(cache_key)\n    if not cached:\n        results = await perform_search(query)\n        await rag_cache.set_search_results(cache_key, results)\n\n    # Invalidate on document update\n    await rag_cache.invalidate_document(document_id)\n\"\"\"\n\nfrom __future__ import annotations\n\nimport hashlib\nimport json\nfrom typing import Any, Dict, List, Optional, Tuple\n\nfrom app.core.logging import get_logger\nfrom app.services.cache_service import cache_service\nfrom prometheus_client import Counter, Histogram\n\nlogger = get_logger(__name__)\n\n\n# Prometheus Metrics\nrag_cache_hits_total = Counter(\n    \"rag_cache_hits_total\",\n    \"Total number of RAG cache hits\",\n    [\"cache_type\"],  # query_embedding, search_results, document_meta\n)\n\nrag_cache_misses_total = Counter(\"rag_cache_misses_total\", \"Total number of RAG cache misses\", [\"cache_type\"])\n\nrag_cache_invalidations_total = Counter(\n    \"rag_cache_invalidations_total\",\n    \"Total number of cache invalidations\",\n    [\"invalidation_type\"],  # document, pattern, all\n)\n\nrag_search_latency_saved = Histogram(\n    \"rag_search_latency_saved_seconds\",\n    \"Estimated latency saved by cache hits\",\n    buckets=[0.1, 0.5, 1.0, 2.0, 5.0, 10.0],\n)\n\n\nclass RAGCache:\n    \"\"\"Cache manager for RAG operations.\n\n    Manages caching of:\n    - Query embeddings (to avoid redundant OpenAI API calls)\n    - Search results (to avoid repeated vector database queries)\n    - Document metadata (for quick access to document info)\n\n    All caches use the shared cache_service with appropriate namespaces and TTLs.\n    \"\"\"\n\n    # Cache TTLs (in seconds)\n    EMBEDDING_TTL = 86400  # 24 hours - embeddings are stable\n    SEARCH_RESULTS_TTL = 3600  # 1 hour - balance between freshness and performance\n    DOCUMENT_META_TTL = 7200  # 2 hours - document metadata is relatively stable\n\n    # Cache namespaces\n    EMBEDDING_NAMESPACE = \"rag_embedding\"\n    SEARCH_NAMESPACE = \"rag_search\"\n    DOCUMENT_NAMESPACE = \"rag_doc\"\n\n    def __init__(self):\n        \"\"\"Initialize RAG cache manager.\"\"\"\n        self.logger = get_logger(__name__)\n\n    def _normalize_query(self, query: str) -> str:\n        \"\"\"Normalize query text for consistent cache keys.\n\n        Args:\n            query: Raw query text\n\n        Returns:\n            Normalized query text\n        \"\"\"\n        # Convert to lowercase and strip whitespace\n        normalized = query.lower().strip()\n\n        # Remove extra whitespace\n        normalized = \" \".join(normalized.split())\n\n        return normalized\n\n    def generate_search_key(\n        self,\n        query: str,\n        top_k: int = 5,\n        score_threshold: float = 0.7,\n        filters: Optional[Dict[str, Any]] = None,\n    ) -> str:\n        \"\"\"Generate cache key for search results.\n\n        Args:\n            query: Search query text\n            top_k: Number of results requested\n            score_threshold: Minimum similarity score\n            filters: Optional filters applied to search\n\n        Returns:\n            Cache key string\n        \"\"\"\n        # Normalize query\n        normalized_query = self._normalize_query(query)\n\n        # Build key components\n        key_data = {\n            \"query\": normalized_query,\n            \"top_k\": top_k,\n            \"threshold\": score_threshold,\n            \"filters\": filters or {},\n        }\n\n        # Create hash\n        key_json = json.dumps(key_data, sort_keys=True)\n        key_hash = hashlib.sha256(key_json.encode()).hexdigest()[:16]\n\n        return f\"{self.SEARCH_NAMESPACE}:{key_hash}\"\n\n    def generate_embedding_key(self, text: str) -> str:\n        \"\"\"Generate cache key for text embedding.\n\n        Args:\n            text: Text to embed\n\n        Returns:\n            Cache key string\n        \"\"\"\n        # Normalize text\n        normalized = self._normalize_query(text)\n\n        # Create hash\n        text_hash = hashlib.sha256(normalized.encode()).hexdigest()[:16]\n\n        return f\"{self.EMBEDDING_NAMESPACE}:{text_hash}\"\n\n    def generate_document_key(self, document_id: str) -> str:\n        \"\"\"Generate cache key for document metadata.\n\n        Args:\n            document_id: Document identifier\n\n        Returns:\n            Cache key string\n        \"\"\"\n        return f\"{self.DOCUMENT_NAMESPACE}:{document_id}\"\n\n    async def get_search_results(self, cache_key: str) -> Optional[List[Dict[str, Any]]]:\n        \"\"\"Get cached search results.\n\n        Args:\n            cache_key: Cache key from generate_search_key()\n\n        Returns:\n            List of search results or None if not cached\n        \"\"\"\n        try:\n            results = await cache_service.get(cache_key)\n\n            if results is not None:\n                rag_cache_hits_total.labels(cache_type=\"search_results\").inc()\n                self.logger.debug(f\"RAG search cache hit: {cache_key}\")\n\n                # Track latency saved (estimated)\n                # Typical search takes 0.5-2 seconds\n                rag_search_latency_saved.observe(1.0)\n\n                return results\n            else:\n                rag_cache_misses_total.labels(cache_type=\"search_results\").inc()\n                return None\n\n        except Exception as e:\n            self.logger.error(f\"Error getting cached search results: {e}\", exc_info=True)\n            rag_cache_misses_total.labels(cache_type=\"search_results\").inc()\n            return None\n\n    async def set_search_results(\n        self, cache_key: str, results: List[Dict[str, Any]], ttl: Optional[int] = None\n    ) -> bool:\n        \"\"\"Cache search results.\n\n        Args:\n            cache_key: Cache key from generate_search_key()\n            results: List of search results to cache\n            ttl: Optional custom TTL (default: SEARCH_RESULTS_TTL)\n\n        Returns:\n            True if cached successfully, False otherwise\n        \"\"\"\n        try:\n            ttl = ttl or self.SEARCH_RESULTS_TTL\n\n            success = await cache_service.set(cache_key, results, ttl=ttl)\n\n            if success:\n                self.logger.debug(f\"Cached RAG search results: {cache_key}, \" f\"{len(results)} results, TTL={ttl}s\")\n\n            return success\n\n        except Exception as e:\n            self.logger.error(f\"Error caching search results: {e}\", exc_info=True)\n            return False\n\n    async def get_embedding(self, cache_key: str) -> Optional[List[float]]:\n        \"\"\"Get cached text embedding.\n\n        Args:\n            cache_key: Cache key from generate_embedding_key()\n\n        Returns:\n            Embedding vector or None if not cached\n        \"\"\"\n        try:\n            embedding = await cache_service.get(cache_key)\n\n            if embedding is not None:\n                rag_cache_hits_total.labels(cache_type=\"query_embedding\").inc()\n                self.logger.debug(f\"RAG embedding cache hit: {cache_key}\")\n                return embedding\n            else:\n                rag_cache_misses_total.labels(cache_type=\"query_embedding\").inc()\n                return None\n\n        except Exception as e:\n            self.logger.error(f\"Error getting cached embedding: {e}\", exc_info=True)\n            rag_cache_misses_total.labels(cache_type=\"query_embedding\").inc()\n            return None\n\n    async def set_embedding(self, cache_key: str, embedding: List[float], ttl: Optional[int] = None) -> bool:\n        \"\"\"Cache text embedding.\n\n        Args:\n            cache_key: Cache key from generate_embedding_key()\n            embedding: Embedding vector to cache\n            ttl: Optional custom TTL (default: EMBEDDING_TTL)\n\n        Returns:\n            True if cached successfully, False otherwise\n        \"\"\"\n        try:\n            ttl = ttl or self.EMBEDDING_TTL\n\n            success = await cache_service.set(\n                cache_key,\n                embedding,\n                ttl=ttl,\n                compress=True,  # Embeddings can be large, compress them\n            )\n\n            if success:\n                self.logger.debug(f\"Cached RAG embedding: {cache_key}, \" f\"{len(embedding)} dimensions, TTL={ttl}s\")\n\n            return success\n\n        except Exception as e:\n            self.logger.error(f\"Error caching embedding: {e}\", exc_info=True)\n            return False\n\n    async def get_document_metadata(self, document_id: str) -> Optional[Dict[str, Any]]:\n        \"\"\"Get cached document metadata.\n\n        Args:\n            document_id: Document identifier\n\n        Returns:\n            Document metadata or None if not cached\n        \"\"\"\n        try:\n            cache_key = self.generate_document_key(document_id)\n            metadata = await cache_service.get(cache_key)\n\n            if metadata is not None:\n                rag_cache_hits_total.labels(cache_type=\"document_meta\").inc()\n                self.logger.debug(f\"RAG document metadata cache hit: {document_id}\")\n                return metadata\n            else:\n                rag_cache_misses_total.labels(cache_type=\"document_meta\").inc()\n                return None\n\n        except Exception as e:\n            self.logger.error(f\"Error getting cached document metadata: {e}\", exc_info=True)\n            rag_cache_misses_total.labels(cache_type=\"document_meta\").inc()\n            return None\n\n    async def set_document_metadata(\n        self, document_id: str, metadata: Dict[str, Any], ttl: Optional[int] = None\n    ) -> bool:\n        \"\"\"Cache document metadata.\n\n        Args:\n            document_id: Document identifier\n            metadata: Document metadata to cache\n            ttl: Optional custom TTL (default: DOCUMENT_META_TTL)\n\n        Returns:\n            True if cached successfully, False otherwise\n        \"\"\"\n        try:\n            cache_key = self.generate_document_key(document_id)\n            ttl = ttl or self.DOCUMENT_META_TTL\n\n            success = await cache_service.set(cache_key, metadata, ttl=ttl)\n\n            if success:\n                self.logger.debug(f\"Cached RAG document metadata: {document_id}, TTL={ttl}s\")\n\n            return success\n\n        except Exception as e:\n            self.logger.error(f\"Error caching document metadata: {e}\", exc_info=True)\n            return False\n\n    async def invalidate_document(self, document_id: str) -> bool:\n        \"\"\"Invalidate all caches related to a document.\n\n        This should be called when a document is updated or deleted.\n\n        Args:\n            document_id: Document identifier\n\n        Returns:\n            True if invalidation successful, False otherwise\n        \"\"\"\n        try:\n            # Invalidate document metadata\n            doc_key = self.generate_document_key(document_id)\n            await cache_service.delete(doc_key)\n\n            # Invalidate all search results (they may contain this document)\n            # This is aggressive but ensures consistency\n            await cache_service.delete_pattern(f\"{self.SEARCH_NAMESPACE}:*\")\n\n            rag_cache_invalidations_total.labels(invalidation_type=\"document\").inc()\n\n            self.logger.info(f\"Invalidated RAG caches for document: {document_id}\")\n            return True\n\n        except Exception as e:\n            self.logger.error(\n                f\"Error invalidating document caches for {document_id}: {e}\",\n                exc_info=True,\n            )\n            return False\n\n    async def invalidate_search_results(self) -> int:\n        \"\"\"Invalidate all search result caches.\n\n        Useful when the knowledge base is significantly updated.\n\n        Returns:\n            Number of cache entries invalidated\n        \"\"\"\n        try:\n            count = await cache_service.delete_pattern(f\"{self.SEARCH_NAMESPACE}:*\")\n\n            rag_cache_invalidations_total.labels(invalidation_type=\"pattern\").inc()\n\n            self.logger.info(f\"Invalidated {count} RAG search result caches\")\n            return count\n\n        except Exception as e:\n            self.logger.error(f\"Error invalidating search result caches: {e}\", exc_info=True)\n            return 0\n\n    async def invalidate_all(self) -> Tuple[int, int, int]:\n        \"\"\"Invalidate all RAG caches.\n\n        Returns:\n            Tuple of (embeddings_count, search_count, documents_count)\n        \"\"\"\n        try:\n            embeddings_count = await cache_service.delete_pattern(f\"{self.EMBEDDING_NAMESPACE}:*\")\n            search_count = await cache_service.delete_pattern(f\"{self.SEARCH_NAMESPACE}:*\")\n            documents_count = await cache_service.delete_pattern(f\"{self.DOCUMENT_NAMESPACE}:*\")\n\n            rag_cache_invalidations_total.labels(invalidation_type=\"all\").inc()\n\n            self.logger.info(\n                f\"Invalidated all RAG caches: \"\n                f\"{embeddings_count} embeddings, \"\n                f\"{search_count} searches, \"\n                f\"{documents_count} documents\"\n            )\n\n            return (embeddings_count, search_count, documents_count)\n\n        except Exception as e:\n            self.logger.error(f\"Error invalidating all RAG caches: {e}\", exc_info=True)\n            return (0, 0, 0)\n\n    async def get_cache_stats(self) -> Dict[str, Any]:\n        \"\"\"Get RAG cache statistics.\n\n        Returns:\n            Dictionary with cache statistics\n        \"\"\"\n        # This would ideally query Prometheus or maintain internal counters\n        # For now, return a basic structure\n        return {\n            \"embeddings\": {\n                \"namespace\": self.EMBEDDING_NAMESPACE,\n                \"ttl_seconds\": self.EMBEDDING_TTL,\n            },\n            \"search_results\": {\n                \"namespace\": self.SEARCH_NAMESPACE,\n                \"ttl_seconds\": self.SEARCH_RESULTS_TTL,\n            },\n            \"document_metadata\": {\n                \"namespace\": self.DOCUMENT_NAMESPACE,\n                \"ttl_seconds\": self.DOCUMENT_META_TTL,\n            },\n        }\n\n\n# Global RAG cache instance\nrag_cache = RAGCache()\n"
}
