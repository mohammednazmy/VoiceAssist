{
  "path": "services/api-gateway/app/services/kb_indexer.py",
  "language": "python",
  "size": 10776,
  "last_modified": "2025-12-04T11:26:57.432Z",
  "lines": 337,
  "content": "\"\"\"\nKnowledge Base Indexer Service (Phase 5 MVP)\n\nHandles document ingestion, text extraction, chunking, embedding generation,\nand storage in Qdrant vector database.\n\nMVP Implementation:\n- Simple text/PDF extraction\n- Basic chunking strategy (fixed-size with overlap)\n- OpenAI embeddings\n- Qdrant storage\n\nFuture enhancements:\n- BioGPT/PubMedBERT embeddings\n- Intelligent chunking (semantic boundaries)\n- Multi-format support (DOCX, HTML, etc.)\n\"\"\"\n\nimport logging\nimport uuid\nfrom dataclasses import dataclass\nfrom datetime import datetime, timezone\nfrom typing import Any, Dict, List, Optional\n\nimport openai\nfrom pypdf import PdfReader\nfrom qdrant_client import QdrantClient\nfrom qdrant_client.models import Distance, PointStruct, VectorParams\n\nlogger = logging.getLogger(__name__)\n\n\n@dataclass\nclass DocumentChunk:\n    \"\"\"Represents a chunk of a document with metadata.\"\"\"\n\n    chunk_id: str\n    document_id: str\n    content: str\n    chunk_index: int\n    metadata: Dict[str, Any]\n\n\n@dataclass\nclass IndexingResult:\n    \"\"\"Result of document indexing operation.\"\"\"\n\n    document_id: str\n    success: bool\n    chunks_indexed: int\n    error_message: Optional[str] = None\n\n\nclass KBIndexer:\n    \"\"\"\n    Knowledge Base Indexer for document ingestion and embedding generation.\n\n    Handles the complete pipeline from raw documents to searchable vectors in Qdrant.\n    \"\"\"\n\n    def __init__(\n        self,\n        qdrant_url: str = \"http://qdrant:6333\",\n        collection_name: str = \"medical_kb\",\n        chunk_size: int = 500,\n        chunk_overlap: int = 50,\n        embedding_model: str = \"text-embedding-3-small\",\n    ):\n        \"\"\"\n        Initialize KB Indexer.\n\n        Args:\n            qdrant_url: Qdrant server URL\n            collection_name: Name of the collection to store embeddings\n            chunk_size: Size of text chunks in characters\n            chunk_overlap: Overlap between chunks\n            embedding_model: OpenAI embedding model to use\n        \"\"\"\n        self.qdrant_client = QdrantClient(url=qdrant_url)\n        self.collection_name = collection_name\n        self.chunk_size = chunk_size\n        self.chunk_overlap = chunk_overlap\n        self.embedding_model = embedding_model\n\n        # Ensure collection exists\n        self._ensure_collection()\n\n    def _ensure_collection(self):\n        \"\"\"Ensure Qdrant collection exists with correct configuration.\"\"\"\n        try:\n            collections = self.qdrant_client.get_collections().collections\n            collection_names = [c.name for c in collections]\n\n            if self.collection_name not in collection_names:\n                # Create collection with vector size for text-embedding-3-small (1536)\n                self.qdrant_client.create_collection(\n                    collection_name=self.collection_name,\n                    vectors_config=VectorParams(size=1536, distance=Distance.COSINE),\n                )\n                logger.info(f\"Created Qdrant collection: {self.collection_name}\")\n            else:\n                logger.info(f\"Qdrant collection already exists: {self.collection_name}\")\n        except Exception as e:\n            logger.error(f\"Error ensuring Qdrant collection: {e}\", exc_info=True)\n            raise\n\n    def extract_text_from_pdf(self, pdf_bytes: bytes) -> str:\n        \"\"\"\n        Extract text content from PDF file.\n\n        Args:\n            pdf_bytes: PDF file content as bytes\n\n        Returns:\n            Extracted text content\n        \"\"\"\n        try:\n            import io\n\n            pdf_file = io.BytesIO(pdf_bytes)\n            reader = PdfReader(pdf_file)\n\n            text_parts = []\n            for page in reader.pages:\n                text_parts.append(page.extract_text())\n\n            full_text = \"\\n\".join(text_parts)\n            logger.debug(f\"Extracted {len(full_text)} characters from PDF\")\n            return full_text\n\n        except Exception as e:\n            logger.error(f\"Error extracting text from PDF: {e}\", exc_info=True)\n            raise ValueError(f\"Failed to extract text from PDF: {e}\")\n\n    def chunk_text(self, text: str, document_id: str, metadata: Dict[str, Any]) -> List[DocumentChunk]:\n        \"\"\"\n        Split text into overlapping chunks.\n\n        Args:\n            text: Full text content\n            document_id: Unique document identifier\n            metadata: Document metadata to attach to chunks\n\n        Returns:\n            List of document chunks\n        \"\"\"\n        chunks = []\n        text_length = len(text)\n\n        for i in range(0, text_length, self.chunk_size - self.chunk_overlap):\n            chunk_text = text[i : i + self.chunk_size]\n\n            # Skip very small chunks at the end\n            if len(chunk_text) < 50:\n                continue\n\n            chunk_id = str(uuid.uuid4())\n            chunk = DocumentChunk(\n                chunk_id=chunk_id,\n                document_id=document_id,\n                content=chunk_text,\n                chunk_index=i // (self.chunk_size - self.chunk_overlap),\n                metadata={\n                    **metadata,\n                    \"chunk_index\": i // (self.chunk_size - self.chunk_overlap),\n                    \"char_start\": i,\n                    \"char_end\": min(i + self.chunk_size, text_length),\n                },\n            )\n            chunks.append(chunk)\n\n        logger.info(f\"Created {len(chunks)} chunks from document {document_id}\")\n        return chunks\n\n    async def generate_embedding(self, text: str) -> List[float]:\n        \"\"\"\n        Generate embedding vector for text using OpenAI.\n\n        Args:\n            text: Text to embed\n\n        Returns:\n            Embedding vector\n        \"\"\"\n        try:\n            # Use OpenAI async client\n            response = await openai.embeddings.create(model=self.embedding_model, input=text)\n            embedding = response.data[0].embedding\n            return embedding\n\n        except Exception as e:\n            logger.error(f\"Error generating embedding: {e}\", exc_info=True)\n            raise\n\n    async def index_document(\n        self,\n        content: str,\n        document_id: str,\n        title: str,\n        source_type: str = \"uploaded\",\n        metadata: Optional[Dict[str, Any]] = None,\n    ) -> IndexingResult:\n        \"\"\"\n        Index a document: chunk, embed, and store in Qdrant.\n\n        Args:\n            content: Document text content\n            document_id: Unique document identifier\n            title: Document title\n            source_type: Type of source (uploaded, guideline, journal, etc.)\n            metadata: Additional metadata\n\n        Returns:\n            IndexingResult with success status and details\n        \"\"\"\n        try:\n            # Prepare metadata\n            doc_metadata = {\n                \"document_id\": document_id,\n                \"title\": title,\n                \"source_type\": source_type,\n                \"indexed_at\": datetime.now(timezone.utc).isoformat(),\n                **(metadata or {}),\n            }\n\n            # Chunk the document\n            chunks = self.chunk_text(content, document_id, doc_metadata)\n\n            if not chunks:\n                return IndexingResult(\n                    document_id=document_id,\n                    success=False,\n                    chunks_indexed=0,\n                    error_message=\"No chunks generated from document\",\n                )\n\n            # Generate embeddings and create points for Qdrant\n            points = []\n            for chunk in chunks:\n                # Generate embedding\n                embedding = await self.generate_embedding(chunk.content)\n\n                # Create Qdrant point\n                point = PointStruct(\n                    id=chunk.chunk_id,\n                    vector=embedding,\n                    payload={\n                        \"document_id\": chunk.document_id,\n                        \"content\": chunk.content,\n                        \"chunk_index\": chunk.chunk_index,\n                        **chunk.metadata,\n                    },\n                )\n                points.append(point)\n\n            # Upload to Qdrant\n            self.qdrant_client.upsert(collection_name=self.collection_name, points=points)\n\n            logger.info(f\"Successfully indexed document {document_id} with {len(points)} chunks\")\n\n            return IndexingResult(document_id=document_id, success=True, chunks_indexed=len(points))\n\n        except Exception as e:\n            logger.error(f\"Error indexing document {document_id}: {e}\", exc_info=True)\n            return IndexingResult(\n                document_id=document_id,\n                success=False,\n                chunks_indexed=0,\n                error_message=str(e),\n            )\n\n    async def index_pdf_document(\n        self,\n        pdf_bytes: bytes,\n        document_id: str,\n        title: str,\n        source_type: str = \"uploaded\",\n        metadata: Optional[Dict[str, Any]] = None,\n    ) -> IndexingResult:\n        \"\"\"\n        Index a PDF document.\n\n        Args:\n            pdf_bytes: PDF file content\n            document_id: Unique document identifier\n            title: Document title\n            source_type: Type of source\n            metadata: Additional metadata\n\n        Returns:\n            IndexingResult with success status\n        \"\"\"\n        try:\n            # Extract text from PDF\n            text_content = self.extract_text_from_pdf(pdf_bytes)\n\n            # Index the extracted text\n            return await self.index_document(\n                content=text_content,\n                document_id=document_id,\n                title=title,\n                source_type=source_type,\n                metadata=metadata,\n            )\n\n        except Exception as e:\n            logger.error(f\"Error indexing PDF document {document_id}: {e}\", exc_info=True)\n            return IndexingResult(\n                document_id=document_id,\n                success=False,\n                chunks_indexed=0,\n                error_message=f\"PDF processing failed: {e}\",\n            )\n\n    def delete_document(self, document_id: str) -> bool:\n        \"\"\"\n        Delete all chunks of a document from Qdrant.\n\n        Args:\n            document_id: Document identifier\n\n        Returns:\n            True if successful\n        \"\"\"\n        try:\n            # Delete all points with matching document_id\n            self.qdrant_client.delete(\n                collection_name=self.collection_name,\n                points_selector={\"filter\": {\"must\": [{\"key\": \"document_id\", \"match\": {\"value\": document_id}}]}},\n            )\n            logger.info(f\"Deleted document {document_id} from index\")\n            return True\n\n        except Exception as e:\n            logger.error(f\"Error deleting document {document_id}: {e}\", exc_info=True)\n            return False\n"
}
