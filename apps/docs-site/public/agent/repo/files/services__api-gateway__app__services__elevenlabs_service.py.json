{
  "path": "services/api-gateway/app/services/elevenlabs_service.py",
  "language": "python",
  "size": 22513,
  "last_modified": "2025-12-04T21:30:58.282Z",
  "lines": 614,
  "content": "\"\"\"\nElevenLabs TTS Service\n\nPremium neural TTS provider with high-quality voice synthesis.\nSupports streaming audio, multiple voices, and emotion/style control.\n\nPhase 11: VoiceAssist Voice Pipeline Sprint\n- Premium TTS option alongside OpenAI\n- Streaming audio support for low latency\n- Voice catalog with caching\n- Cost tracking per request\n- Circuit breaker protection for resilience\n\"\"\"\n\nimport asyncio\nimport time\nfrom dataclasses import dataclass, field\nfrom typing import AsyncIterator, Dict, List, Optional\n\nimport httpx\nfrom app.core.config import settings\nfrom app.core.logging import get_logger\nfrom app.core.resilience import elevenlabs_breaker\nfrom pybreaker import CircuitBreakerError\n\nlogger = get_logger(__name__)\n\n# Rate limit retry configuration\nRATE_LIMIT_MAX_RETRIES = 3\nRATE_LIMIT_BASE_DELAY = 1.0  # seconds\nRATE_LIMIT_MAX_DELAY = 10.0  # seconds\n\n\n# ==============================================================================\n# Data Classes\n# ==============================================================================\n\n\n@dataclass\nclass ElevenLabsVoice:\n    \"\"\"Metadata for an ElevenLabs voice.\"\"\"\n\n    voice_id: str\n    name: str\n    category: str  # \"premade\", \"cloned\", \"professional\"\n    labels: Dict[str, str] = field(default_factory=dict)  # accent, gender, etc.\n    preview_url: Optional[str] = None\n    description: Optional[str] = None\n\n\n@dataclass\nclass TTSSynthesisResult:\n    \"\"\"Result of TTS synthesis.\"\"\"\n\n    audio_data: bytes\n    content_type: str  # \"audio/mpeg\", \"audio/pcm\", etc.\n    duration_ms: Optional[int] = None\n    characters_used: int = 0\n    latency_ms: Optional[int] = None\n    voice_id: str = \"\"\n\n\n@dataclass\nclass ElevenLabsUsageStats:\n    \"\"\"Usage statistics from ElevenLabs.\"\"\"\n\n    character_count: int\n    character_limit: int\n    voice_limit: int\n    professional_voice_limit: int\n    next_reset_at: Optional[str] = None  # ISO timestamp\n\n\n# ==============================================================================\n# Service Implementation\n# ==============================================================================\n\n\nclass ElevenLabsService:\n    \"\"\"\n    ElevenLabs TTS service for premium voice synthesis.\n\n    Features:\n    - High-quality neural voices (multilingual v2, turbo v2)\n    - Streaming audio for low latency playback\n    - Voice catalog with caching\n    - Usage/cost tracking\n    - Automatic fallback handling\n\n    Supported models:\n    - eleven_multilingual_v2: Best quality, 28 languages\n    - eleven_turbo_v2: Fast, English-optimized\n    - eleven_monolingual_v1: Legacy English model\n    \"\"\"\n\n    # API endpoints\n    BASE_URL = \"https://api.elevenlabs.io/v1\"\n    TTS_ENDPOINT = \"/text-to-speech\"\n    TTS_STREAM_ENDPOINT = \"/text-to-speech/{voice_id}/stream\"\n    VOICES_ENDPOINT = \"/voices\"\n    USER_ENDPOINT = \"/user/subscription\"\n\n    # Model options\n    MODEL_MULTILINGUAL_V2 = \"eleven_multilingual_v2\"\n    MODEL_TURBO_V2 = \"eleven_turbo_v2\"\n    MODEL_TURBO_V2_5 = \"eleven_turbo_v2_5\"  # Faster turbo model\n    MODEL_FLASH_V2_5 = \"eleven_flash_v2_5\"  # Fastest model for low latency\n    MODEL_MONOLINGUAL_V1 = \"eleven_monolingual_v1\"\n\n    # Output formats\n    FORMAT_MP3_44100_128 = \"mp3_44100_128\"  # Default MP3\n    FORMAT_MP3_22050_32 = \"mp3_22050_32\"  # Low bandwidth\n    FORMAT_PCM_24000 = \"pcm_24000\"  # Raw PCM for WebRTC\n    FORMAT_PCM_16000 = \"pcm_16000\"  # Low bandwidth PCM\n    FORMAT_ULAW_8000 = \"ulaw_8000\"  # Phone quality\n\n    def __init__(self):\n        self.api_key = settings.ELEVENLABS_API_KEY\n        self.enabled = bool(self.api_key)\n        # Use flash model for lowest latency (Phase: Talker Enhancement)\n        self.default_model = self.MODEL_FLASH_V2_5\n        self.default_voice_id = \"21m00Tcm4TlvDq8ikWAM\"  # \"Rachel\" voice\n        self._connection_warmed = False\n\n        # Voice cache (refreshed periodically)\n        self._voice_cache: List[ElevenLabsVoice] = []\n        self._voice_cache_expiry: float = 0\n        self._voice_cache_ttl: float = 300  # 5 minutes\n\n        # Persistent HTTP client\n        self._http_client: Optional[httpx.AsyncClient] = None\n\n    def is_enabled(self) -> bool:\n        \"\"\"Check if ElevenLabs is enabled and configured.\"\"\"\n        return self.enabled\n\n    async def warm_connection(self) -> bool:\n        \"\"\"\n        Pre-warm the HTTP connection to ElevenLabs API.\n\n        Establishes the TCP + TLS connection ahead of time to eliminate\n        cold-start latency on the first TTS request.\n\n        Call this during service startup or when a voice session begins.\n\n        Returns:\n            True if connection was warmed successfully\n        \"\"\"\n        if not self.enabled:\n            return False\n\n        if self._connection_warmed:\n            return True\n\n        try:\n            client = await self._get_http_client()\n\n            # Make a lightweight request to establish connection\n            # Using the voices endpoint as it's fast and doesn't cost credits\n            response = await client.get(\n                f\"{self.BASE_URL}{self.USER_ENDPOINT}\",\n                headers={\"xi-api-key\": self.api_key or \"\"},\n                timeout=5.0,\n            )\n\n            if response.status_code == 200:\n                self._connection_warmed = True\n                logger.info(\"ElevenLabs connection warmed successfully\")\n                return True\n            else:\n                logger.warning(f\"ElevenLabs warm connection returned {response.status_code}\")\n                return False\n\n        except Exception as e:\n            logger.warning(f\"Failed to warm ElevenLabs connection: {e}\")\n            return False\n\n    def is_connection_warmed(self) -> bool:\n        \"\"\"Check if connection has been pre-warmed.\"\"\"\n        return self._connection_warmed\n\n    async def _get_http_client(self) -> httpx.AsyncClient:\n        \"\"\"Get or create persistent HTTP client.\"\"\"\n        if self._http_client is None or self._http_client.is_closed:\n            self._http_client = httpx.AsyncClient(\n                timeout=60.0,  # TTS can take time for long text\n                limits=httpx.Limits(\n                    max_keepalive_connections=5,\n                    max_connections=10,\n                ),\n            )\n        return self._http_client\n\n    def _get_headers(self) -> Dict[str, str]:\n        \"\"\"Get API request headers.\"\"\"\n        return {\n            \"xi-api-key\": self.api_key or \"\",\n            \"Content-Type\": \"application/json\",\n            \"Accept\": \"audio/mpeg\",\n        }\n\n    async def synthesize(\n        self,\n        text: str,\n        voice_id: Optional[str] = None,\n        model_id: Optional[str] = None,\n        output_format: str = FORMAT_MP3_44100_128,\n        stability: float = 0.5,\n        similarity_boost: float = 0.75,\n        style: float = 0.0,\n        use_speaker_boost: bool = True,\n    ) -> TTSSynthesisResult:\n        \"\"\"\n        Synthesize text to speech (non-streaming).\n\n        Args:\n            text: Text to synthesize (max 5000 chars)\n            voice_id: ElevenLabs voice ID (default: Rachel)\n            model_id: Model to use (default: multilingual_v2)\n            output_format: Audio format (default: MP3 128kbps)\n            stability: Voice stability 0-1 (lower = more expressive)\n            similarity_boost: Voice similarity 0-1\n            style: Style exaggeration 0-1 (multilingual_v2 only)\n            use_speaker_boost: Enable speaker clarity boost\n\n        Returns:\n            TTSSynthesisResult with audio data and metadata\n\n        Raises:\n            ValueError: If service is not enabled or request fails\n        \"\"\"\n        if not self.enabled:\n            raise ValueError(\"ElevenLabs TTS is not enabled\")\n\n        if len(text) > 5000:\n            raise ValueError(\"Text exceeds maximum length of 5000 characters\")\n\n        # Check circuit breaker before attempting call\n        try:\n            elevenlabs_breaker.call(lambda: None)  # Lightweight check\n        except CircuitBreakerError:\n            logger.error(\"ElevenLabs circuit breaker is OPEN - failing fast\")\n            raise ValueError(\"ElevenLabs TTS is temporarily unavailable (circuit breaker open)\")\n\n        voice_id = voice_id or self.default_voice_id\n        model_id = model_id or self.default_model\n\n        start_time = time.time()\n\n        try:\n            client = await self._get_http_client()\n            url = f\"{self.BASE_URL}{self.TTS_ENDPOINT}/{voice_id}\"\n\n            # Build request payload\n            payload = {\n                \"text\": text,\n                \"model_id\": model_id,\n                \"voice_settings\": {\n                    \"stability\": stability,\n                    \"similarity_boost\": similarity_boost,\n                    \"style\": style,\n                    \"use_speaker_boost\": use_speaker_boost,\n                },\n            }\n\n            # Add output format as query param\n            params = {\"output_format\": output_format}\n\n            response = await client.post(\n                url,\n                headers=self._get_headers(),\n                json=payload,\n                params=params,\n            )\n\n            if response.status_code != 200:\n                error_msg = f\"ElevenLabs TTS failed: {response.status_code}\"\n                try:\n                    error_detail = response.json()\n                    error_msg = f\"{error_msg} - {error_detail.get('detail', {}).get('message', response.text)}\"\n                except Exception:\n                    error_msg = f\"{error_msg} - {response.text}\"\n\n                logger.error(error_msg)\n                raise ValueError(error_msg)\n\n            audio_data = response.content\n            latency_ms = int((time.time() - start_time) * 1000)\n\n            # Determine content type from format\n            content_type = \"audio/mpeg\"\n            if \"pcm\" in output_format:\n                content_type = \"audio/pcm\"\n            elif \"ulaw\" in output_format:\n                content_type = \"audio/basic\"\n\n            logger.info(\n                \"ElevenLabs TTS synthesis complete\",\n                extra={\n                    \"voice_id\": voice_id,\n                    \"model_id\": model_id,\n                    \"text_length\": len(text),\n                    \"audio_size\": len(audio_data),\n                    \"latency_ms\": latency_ms,\n                },\n            )\n\n            # Circuit breaker auto-tracks via decorator pattern\n\n            return TTSSynthesisResult(\n                audio_data=audio_data,\n                content_type=content_type,\n                characters_used=len(text),\n                latency_ms=latency_ms,\n                voice_id=voice_id,\n            )\n\n        except httpx.TimeoutException as e:\n            logger.error(f\"ElevenLabs TTS timeout: {str(e)}\")\n            raise ValueError(\"TTS request timed out\")\n        except httpx.HTTPError as e:\n            logger.error(f\"ElevenLabs TTS HTTP error: {str(e)}\")\n            raise ValueError(f\"TTS request failed: {str(e)}\")\n\n    async def synthesize_stream(\n        self,\n        text: str,\n        voice_id: Optional[str] = None,\n        model_id: Optional[str] = None,\n        output_format: str = FORMAT_MP3_44100_128,\n        stability: float = 0.5,\n        similarity_boost: float = 0.75,\n        style: float = 0.0,\n        use_speaker_boost: bool = True,\n        chunk_size: int = 384,  # Low-latency default (was 1024)\n        previous_text: Optional[str] = None,  # Context for voice continuity\n        next_text: Optional[str] = None,  # Upcoming text hint\n    ) -> AsyncIterator[bytes]:\n        \"\"\"\n        Synthesize text to speech with streaming output.\n\n        Yields audio chunks as they become available for low-latency playback.\n        Includes automatic retry with exponential backoff for rate limit errors (429).\n\n        Args:\n            text: Text to synthesize (max 5000 chars)\n            voice_id: ElevenLabs voice ID\n            model_id: Model to use\n            output_format: Audio format\n            stability: Voice stability 0-1\n            similarity_boost: Voice similarity 0-1\n            style: Style exaggeration 0-1\n            use_speaker_boost: Enable speaker clarity boost\n            chunk_size: Size of audio chunks to yield\n            previous_text: Text spoken before this (provides voice continuity context)\n            next_text: Text that will be spoken after (helps with prosody)\n\n        Yields:\n            Audio data chunks as bytes\n\n        Raises:\n            ValueError: If service is not enabled or request fails\n        \"\"\"\n        if not self.enabled:\n            raise ValueError(\"ElevenLabs TTS is not enabled\")\n\n        if len(text) > 5000:\n            raise ValueError(\"Text exceeds maximum length of 5000 characters\")\n\n        # Check circuit breaker before attempting call\n        try:\n            elevenlabs_breaker.call(lambda: None)  # Lightweight check\n        except CircuitBreakerError:\n            logger.error(\"ElevenLabs circuit breaker is OPEN - failing fast\")\n            raise ValueError(\"ElevenLabs TTS is temporarily unavailable (circuit breaker open)\")\n\n        voice_id = voice_id or self.default_voice_id\n        model_id = model_id or self.default_model\n\n        # Retry loop for rate limit errors\n        last_error: Optional[Exception] = None\n        for attempt in range(RATE_LIMIT_MAX_RETRIES + 1):\n            try:\n                client = await self._get_http_client()\n                url = f\"{self.BASE_URL}{self.TTS_ENDPOINT}/{voice_id}/stream\"\n\n                payload = {\n                    \"text\": text,\n                    \"model_id\": model_id,\n                    \"voice_settings\": {\n                        \"stability\": stability,\n                        \"similarity_boost\": similarity_boost,\n                        \"style\": style,\n                        \"use_speaker_boost\": use_speaker_boost,\n                    },\n                }\n\n                # Add context for voice continuity (ElevenLabs uses this to maintain\n                # consistent prosody and voice characteristics across chunks)\n                if previous_text:\n                    # Limit previous text to last ~200 chars for efficiency\n                    payload[\"previous_text\"] = previous_text[-200:] if len(previous_text) > 200 else previous_text\n                if next_text:\n                    # Limit next text to first ~100 chars\n                    payload[\"next_text\"] = next_text[:100] if len(next_text) > 100 else next_text\n\n                params = {\"output_format\": output_format}\n\n                async with client.stream(\n                    \"POST\",\n                    url,\n                    headers=self._get_headers(),\n                    json=payload,\n                    params=params,\n                ) as response:\n                    # Handle rate limiting with retry\n                    if response.status_code == 429:\n                        error_text = await response.aread()\n                        if attempt < RATE_LIMIT_MAX_RETRIES:\n                            # Calculate exponential backoff delay\n                            delay = min(\n                                RATE_LIMIT_BASE_DELAY * (2**attempt),\n                                RATE_LIMIT_MAX_DELAY,\n                            )\n                            logger.warning(\n                                \"ElevenLabs rate limit hit, retrying\",\n                                extra={\n                                    \"attempt\": attempt + 1,\n                                    \"max_retries\": RATE_LIMIT_MAX_RETRIES,\n                                    \"delay_seconds\": delay,\n                                    \"error\": error_text.decode()[:100],\n                                },\n                            )\n                            await asyncio.sleep(delay)\n                            continue  # Retry the request\n                        else:\n                            # Max retries exceeded\n                            logger.error(\n                                \"ElevenLabs rate limit - max retries exceeded\",\n                                extra={\n                                    \"attempts\": attempt + 1,\n                                    \"error\": error_text.decode()[:100],\n                                },\n                            )\n                            raise ValueError(f\"ElevenLabs rate limit exceeded after {attempt + 1} attempts\")\n\n                    if response.status_code != 200:\n                        error_text = await response.aread()\n                        raise ValueError(f\"ElevenLabs stream failed: {error_text.decode()}\")\n\n                    async for chunk in response.aiter_bytes(chunk_size):\n                        yield chunk\n\n                logger.debug(\n                    \"ElevenLabs streaming TTS complete\",\n                    extra={\"voice_id\": voice_id, \"text_length\": len(text)},\n                )\n\n                # Success - exit the retry loop\n                return\n\n            except httpx.TimeoutException as e:\n                logger.error(f\"ElevenLabs streaming TTS timeout: {str(e)}\")\n                last_error = ValueError(\"Streaming TTS request timed out\")\n                if attempt < RATE_LIMIT_MAX_RETRIES:\n                    delay = min(RATE_LIMIT_BASE_DELAY * (2**attempt), RATE_LIMIT_MAX_DELAY)\n                    await asyncio.sleep(delay)\n                    continue\n                raise last_error\n            except httpx.HTTPError as e:\n                logger.error(f\"ElevenLabs streaming TTS error: {str(e)}\")\n                last_error = ValueError(f\"Streaming TTS request failed: {str(e)}\")\n                if attempt < RATE_LIMIT_MAX_RETRIES:\n                    delay = min(RATE_LIMIT_BASE_DELAY * (2**attempt), RATE_LIMIT_MAX_DELAY)\n                    await asyncio.sleep(delay)\n                    continue\n                raise last_error\n\n        # Should not reach here, but just in case\n        if last_error:\n            raise last_error\n\n    async def get_voices(self, force_refresh: bool = False) -> List[ElevenLabsVoice]:\n        \"\"\"\n        Get available voices with caching.\n\n        Args:\n            force_refresh: Force refresh of voice cache\n\n        Returns:\n            List of available ElevenLabsVoice objects\n        \"\"\"\n        if not self.enabled:\n            return []\n\n        # Check cache\n        if not force_refresh and self._voice_cache and time.time() < self._voice_cache_expiry:\n            return self._voice_cache\n\n        try:\n            client = await self._get_http_client()\n            response = await client.get(\n                f\"{self.BASE_URL}{self.VOICES_ENDPOINT}\",\n                headers={\"xi-api-key\": self.api_key or \"\"},\n            )\n\n            if response.status_code != 200:\n                logger.error(f\"Failed to fetch ElevenLabs voices: {response.status_code}\")\n                return self._voice_cache  # Return stale cache on error\n\n            data = response.json()\n            voices = []\n\n            for voice_data in data.get(\"voices\", []):\n                voice = ElevenLabsVoice(\n                    voice_id=voice_data.get(\"voice_id\", \"\"),\n                    name=voice_data.get(\"name\", \"\"),\n                    category=voice_data.get(\"category\", \"unknown\"),\n                    labels=voice_data.get(\"labels\", {}),\n                    preview_url=voice_data.get(\"preview_url\"),\n                    description=voice_data.get(\"description\"),\n                )\n                voices.append(voice)\n\n            # Update cache\n            self._voice_cache = voices\n            self._voice_cache_expiry = time.time() + self._voice_cache_ttl\n\n            logger.info(f\"Loaded {len(voices)} ElevenLabs voices\")\n            return voices\n\n        except Exception as e:\n            logger.error(f\"Failed to fetch ElevenLabs voices: {str(e)}\")\n            return self._voice_cache  # Return stale cache on error\n\n    async def get_usage_stats(self) -> Optional[ElevenLabsUsageStats]:\n        \"\"\"\n        Get current usage statistics.\n\n        Returns:\n            ElevenLabsUsageStats or None if request fails\n        \"\"\"\n        if not self.enabled:\n            return None\n\n        try:\n            client = await self._get_http_client()\n            response = await client.get(\n                f\"{self.BASE_URL}{self.USER_ENDPOINT}\",\n                headers={\"xi-api-key\": self.api_key or \"\"},\n            )\n\n            if response.status_code != 200:\n                logger.error(f\"Failed to fetch ElevenLabs usage: {response.status_code}\")\n                return None\n\n            data = response.json()\n\n            return ElevenLabsUsageStats(\n                character_count=data.get(\"character_count\", 0),\n                character_limit=data.get(\"character_limit\", 0),\n                voice_limit=data.get(\"voice_limit\", 0),\n                professional_voice_limit=data.get(\"professional_voice_limit\", 0),\n                next_reset_at=data.get(\"next_character_count_reset_unix\"),\n            )\n\n        except Exception as e:\n            logger.error(f\"Failed to fetch ElevenLabs usage: {str(e)}\")\n            return None\n\n    def get_default_voice_id(self) -> str:\n        \"\"\"Get the default voice ID.\"\"\"\n        return self.default_voice_id\n\n    def get_available_models(self) -> List[Dict[str, str]]:\n        \"\"\"Get list of available TTS models.\"\"\"\n        return [\n            {\n                \"id\": self.MODEL_FLASH_V2_5,\n                \"name\": \"Flash v2.5\",\n                \"description\": \"Fastest model, lowest latency (recommended)\",\n            },\n            {\n                \"id\": self.MODEL_TURBO_V2_5,\n                \"name\": \"Turbo v2.5\",\n                \"description\": \"Fast with good quality\",\n            },\n            {\n                \"id\": self.MODEL_MULTILINGUAL_V2,\n                \"name\": \"Multilingual v2\",\n                \"description\": \"Best quality, 28 languages, style control\",\n            },\n            {\n                \"id\": self.MODEL_TURBO_V2,\n                \"name\": \"Turbo v2\",\n                \"description\": \"Fast, English-optimized (legacy)\",\n            },\n            {\n                \"id\": self.MODEL_MONOLINGUAL_V1,\n                \"name\": \"Monolingual v1\",\n                \"description\": \"Legacy English model\",\n            },\n        ]\n\n    async def close(self) -> None:\n        \"\"\"Close the HTTP client.\"\"\"\n        if self._http_client:\n            await self._http_client.aclose()\n            self._http_client = None\n\n\n# Global service instance\nelevenlabs_service = ElevenLabsService()\n"
}
