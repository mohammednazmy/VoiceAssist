{
  "path": "services/api-gateway/app/services/local_whisper_service.py",
  "language": "python",
  "size": 23076,
  "last_modified": "2025-12-05T03:09:48.127Z",
  "lines": 714,
  "content": "\"\"\"\nLocal Whisper Service - PHI-Safe On-Premise Speech-to-Text\n\nVoice Mode v4 - Phase 1 Foundation (Privacy & Compliance)\n\nProvides local STT transcription using OpenAI Whisper for:\n- PHI-containing sessions (data stays on-premise)\n- Offline/air-gapped environments\n- Reduced cloud dependency\n- HIPAA-compliant transcription\n\nTargets <500ms latency with GPU acceleration.\n\"\"\"\n\nimport asyncio\nimport logging\nimport os\nimport tempfile\nimport time\nfrom dataclasses import dataclass, field\nfrom datetime import datetime, timezone\nfrom enum import Enum\nfrom pathlib import Path\nfrom typing import Any, Callable, Dict, List, Optional, Tuple, Union\n\nimport numpy as np\n\nlogger = logging.getLogger(__name__)\n\n\nclass WhisperModelSize(Enum):\n    \"\"\"Available Whisper model sizes.\"\"\"\n    TINY = \"tiny\"  # 39M params, ~1GB VRAM, fastest\n    BASE = \"base\"  # 74M params, ~1GB VRAM\n    SMALL = \"small\"  # 244M params, ~2GB VRAM\n    MEDIUM = \"medium\"  # 769M params, ~5GB VRAM\n    LARGE = \"large\"  # 1550M params, ~10GB VRAM, most accurate\n    LARGE_V2 = \"large-v2\"  # Improved large model\n    LARGE_V3 = \"large-v3\"  # Latest large model\n\n\nclass TranscriptionLanguage(Enum):\n    \"\"\"Supported transcription languages.\"\"\"\n    ENGLISH = \"en\"\n    ARABIC = \"ar\"\n    SPANISH = \"es\"\n    FRENCH = \"fr\"\n    GERMAN = \"de\"\n    CHINESE = \"zh\"\n    HINDI = \"hi\"\n    URDU = \"ur\"\n    AUTO = \"auto\"  # Auto-detect\n\n\nclass ComputeDevice(Enum):\n    \"\"\"Compute device for inference.\"\"\"\n    CPU = \"cpu\"\n    CUDA = \"cuda\"\n    MPS = \"mps\"  # Apple Silicon\n    AUTO = \"auto\"\n\n\n@dataclass\nclass WhisperConfig:\n    \"\"\"Configuration for local Whisper service.\"\"\"\n    # Model settings\n    model_size: WhisperModelSize = WhisperModelSize.BASE\n    compute_device: ComputeDevice = ComputeDevice.AUTO\n    compute_type: str = \"float16\"  # float16, int8, float32\n\n    # Model paths\n    model_cache_dir: str = \"/opt/whisper-models\"\n    download_models: bool = True\n\n    # Transcription settings\n    default_language: TranscriptionLanguage = TranscriptionLanguage.AUTO\n    task: str = \"transcribe\"  # transcribe or translate\n    beam_size: int = 5\n    best_of: int = 5\n    temperature: float = 0.0  # 0 for greedy, higher for sampling\n\n    # VAD settings\n    vad_enabled: bool = True\n    vad_threshold: float = 0.5\n    min_speech_duration_ms: int = 250\n    max_speech_duration_s: int = 30\n\n    # Performance\n    enable_batching: bool = True\n    batch_size: int = 16\n    chunk_length_s: int = 30\n\n    # Quality\n    word_timestamps: bool = True\n    condition_on_previous_text: bool = True\n    compression_ratio_threshold: float = 2.4\n    logprob_threshold: float = -1.0\n    no_speech_threshold: float = 0.6\n\n\n@dataclass\nclass TranscriptionResult:\n    \"\"\"Result of a transcription.\"\"\"\n    text: str\n    language: str\n    language_probability: float\n    segments: List[\"TranscriptionSegment\"]\n    duration_seconds: float\n    processing_time_ms: float\n    model_size: str\n    is_phi_safe: bool = True  # Always true for local processing\n\n    @property\n    def words_per_minute(self) -> float:\n        word_count = len(self.text.split())\n        if self.duration_seconds > 0:\n            return (word_count / self.duration_seconds) * 60\n        return 0.0\n\n\n@dataclass\nclass TranscriptionSegment:\n    \"\"\"A segment of transcribed text.\"\"\"\n    id: int\n    start: float  # Start time in seconds\n    end: float  # End time in seconds\n    text: str\n    confidence: float\n    words: List[\"TranscriptionWord\"] = field(default_factory=list)\n\n    @property\n    def duration(self) -> float:\n        return self.end - self.start\n\n\n@dataclass\nclass TranscriptionWord:\n    \"\"\"A word with timing information.\"\"\"\n    word: str\n    start: float\n    end: float\n    probability: float\n\n\n@dataclass\nclass WhisperMetrics:\n    \"\"\"Metrics for Whisper service performance.\"\"\"\n    total_transcriptions: int = 0\n    total_audio_seconds: float = 0.0\n    total_processing_ms: float = 0.0\n    avg_latency_ms: float = 0.0\n    avg_real_time_factor: float = 0.0  # Processing time / audio duration\n    languages_detected: Dict[str, int] = field(default_factory=dict)\n    model_loads: int = 0\n    gpu_memory_mb: float = 0.0\n    errors: int = 0\n\n    @property\n    def throughput_factor(self) -> float:\n        \"\"\"How much faster than real-time (>1 = faster).\"\"\"\n        if self.avg_real_time_factor > 0:\n            return 1.0 / self.avg_real_time_factor\n        return 0.0\n\n\nclass LocalWhisperService:\n    \"\"\"\n    Local Whisper STT service for PHI-safe transcription.\n\n    Uses OpenAI Whisper models running locally for HIPAA-compliant\n    speech-to-text without sending data to external services.\n    \"\"\"\n\n    def __init__(self, config: Optional[WhisperConfig] = None):\n        self.config = config or WhisperConfig()\n        self._initialized = False\n        self._model = None\n        self._processor = None\n        self._device = None\n        self._metrics = WhisperMetrics()\n\n        # Lazy imports\n        self._whisper = None\n        self._torch = None\n\n    async def initialize(self) -> bool:\n        \"\"\"\n        Initialize the Whisper model.\n\n        Returns:\n            True if initialization successful\n        \"\"\"\n        if self._initialized:\n            return True\n\n        try:\n            logger.info(\n                \"Initializing LocalWhisperService\",\n                extra={\n                    \"model_size\": self.config.model_size.value,\n                    \"device\": self.config.compute_device.value,\n                    \"cache_dir\": self.config.model_cache_dir,\n                }\n            )\n\n            # Import dependencies\n            await self._load_dependencies()\n\n            # Determine compute device\n            self._device = await self._get_compute_device()\n\n            # Load model\n            await self._load_model()\n\n            self._initialized = True\n            self._metrics.model_loads += 1\n\n            logger.info(\n                f\"LocalWhisperService initialized on {self._device}\",\n                extra={\"model\": self.config.model_size.value}\n            )\n\n            return True\n\n        except Exception as e:\n            logger.error(f\"Failed to initialize LocalWhisperService: {e}\")\n            self._metrics.errors += 1\n            return False\n\n    async def _load_dependencies(self) -> None:\n        \"\"\"Lazy load heavy dependencies.\"\"\"\n        try:\n            import torch\n            self._torch = torch\n        except ImportError:\n            logger.warning(\"PyTorch not available, using CPU-only mode\")\n            self._torch = None\n\n        # Try faster-whisper first (CTranslate2 backend)\n        try:\n            import faster_whisper\n            self._whisper = faster_whisper\n            self._whisper_backend = \"faster-whisper\"\n            logger.info(\"Using faster-whisper backend (CTranslate2)\")\n        except ImportError:\n            # Fall back to openai-whisper\n            try:\n                import whisper\n                self._whisper = whisper\n                self._whisper_backend = \"openai-whisper\"\n                logger.info(\"Using openai-whisper backend\")\n            except ImportError:\n                raise ImportError(\n                    \"No Whisper implementation found. \"\n                    \"Install faster-whisper or openai-whisper\"\n                )\n\n    async def _get_compute_device(self) -> str:\n        \"\"\"Determine the best compute device.\"\"\"\n        if self.config.compute_device != ComputeDevice.AUTO:\n            return self.config.compute_device.value\n\n        if self._torch is None:\n            return \"cpu\"\n\n        if self._torch.cuda.is_available():\n            device = \"cuda\"\n            # Log GPU info\n            gpu_name = self._torch.cuda.get_device_name(0)\n            gpu_mem = self._torch.cuda.get_device_properties(0).total_memory / 1e9\n            logger.info(f\"Using CUDA: {gpu_name} ({gpu_mem:.1f}GB)\")\n            self._metrics.gpu_memory_mb = gpu_mem * 1024\n        elif hasattr(self._torch.backends, 'mps') and self._torch.backends.mps.is_available():\n            device = \"mps\"\n            logger.info(\"Using Apple Silicon MPS\")\n        else:\n            device = \"cpu\"\n            logger.info(\"Using CPU (no GPU available)\")\n\n        return device\n\n    async def _load_model(self) -> None:\n        \"\"\"Load the Whisper model.\"\"\"\n        model_name = self.config.model_size.value\n\n        # Ensure cache directory exists\n        os.makedirs(self.config.model_cache_dir, exist_ok=True)\n\n        if self._whisper_backend == \"faster-whisper\":\n            # faster-whisper uses CTranslate2 format\n            from faster_whisper import WhisperModel\n\n            self._model = WhisperModel(\n                model_name,\n                device=self._device,\n                compute_type=self.config.compute_type,\n                download_root=self.config.model_cache_dir,\n            )\n        else:\n            # openai-whisper\n            self._model = self._whisper.load_model(\n                model_name,\n                device=self._device,\n                download_root=self.config.model_cache_dir,\n            )\n\n    async def transcribe(\n        self,\n        audio: Union[bytes, np.ndarray, str, Path],\n        language: Optional[TranscriptionLanguage] = None,\n        task: Optional[str] = None,\n        prompt: Optional[str] = None,\n    ) -> TranscriptionResult:\n        \"\"\"\n        Transcribe audio to text.\n\n        Args:\n            audio: Audio data (bytes, numpy array, or file path)\n            language: Language hint (None for auto-detect)\n            task: \"transcribe\" or \"translate\"\n            prompt: Initial prompt for context\n\n        Returns:\n            TranscriptionResult with text and metadata\n        \"\"\"\n        if not self._initialized:\n            await self.initialize()\n\n        start_time = time.time()\n\n        try:\n            # Prepare audio\n            audio_array, duration = await self._prepare_audio(audio)\n\n            # Set options\n            language_code = None\n            if language and language != TranscriptionLanguage.AUTO:\n                language_code = language.value\n            elif self.config.default_language != TranscriptionLanguage.AUTO:\n                language_code = self.config.default_language.value\n\n            task = task or self.config.task\n\n            # Transcribe based on backend\n            if self._whisper_backend == \"faster-whisper\":\n                result = await self._transcribe_faster_whisper(\n                    audio_array, language_code, task, prompt\n                )\n            else:\n                result = await self._transcribe_openai_whisper(\n                    audio_array, language_code, task, prompt\n                )\n\n            # Calculate metrics\n            processing_time_ms = (time.time() - start_time) * 1000\n            result.duration_seconds = duration\n            result.processing_time_ms = processing_time_ms\n            result.model_size = self.config.model_size.value\n\n            # Update metrics\n            self._update_metrics(result)\n\n            logger.debug(\n                f\"Transcribed {duration:.1f}s audio in {processing_time_ms:.0f}ms\",\n                extra={\n                    \"language\": result.language,\n                    \"text_length\": len(result.text),\n                }\n            )\n\n            return result\n\n        except Exception as e:\n            self._metrics.errors += 1\n            logger.error(f\"Transcription error: {e}\")\n            raise\n\n    async def _prepare_audio(\n        self,\n        audio: Union[bytes, np.ndarray, str, Path]\n    ) -> Tuple[np.ndarray, float]:\n        \"\"\"\n        Prepare audio for transcription.\n\n        Returns:\n            Tuple of (audio_array, duration_seconds)\n        \"\"\"\n        sample_rate = 16000  # Whisper expects 16kHz\n\n        if isinstance(audio, (str, Path)):\n            # Load from file\n            import librosa\n            audio_array, sr = librosa.load(str(audio), sr=sample_rate)\n        elif isinstance(audio, bytes):\n            # Convert PCM16 bytes to float array\n            audio_array = np.frombuffer(audio, dtype=np.int16).astype(np.float32)\n            audio_array /= 32768.0  # Normalize to [-1, 1]\n        else:\n            audio_array = audio.astype(np.float32)\n            if audio_array.max() > 1.0:\n                audio_array /= 32768.0\n\n        duration = len(audio_array) / sample_rate\n        return audio_array, duration\n\n    async def _transcribe_faster_whisper(\n        self,\n        audio: np.ndarray,\n        language: Optional[str],\n        task: str,\n        prompt: Optional[str],\n    ) -> TranscriptionResult:\n        \"\"\"Transcribe using faster-whisper backend.\"\"\"\n        # Run in thread pool for async compatibility\n        loop = asyncio.get_event_loop()\n\n        def _transcribe():\n            segments, info = self._model.transcribe(\n                audio,\n                language=language,\n                task=task,\n                beam_size=self.config.beam_size,\n                best_of=self.config.best_of,\n                temperature=self.config.temperature,\n                word_timestamps=self.config.word_timestamps,\n                condition_on_previous_text=self.config.condition_on_previous_text,\n                compression_ratio_threshold=self.config.compression_ratio_threshold,\n                log_prob_threshold=self.config.logprob_threshold,\n                no_speech_threshold=self.config.no_speech_threshold,\n                initial_prompt=prompt,\n                vad_filter=self.config.vad_enabled,\n                vad_parameters={\n                    \"threshold\": self.config.vad_threshold,\n                    \"min_speech_duration_ms\": self.config.min_speech_duration_ms,\n                    \"max_speech_duration_s\": self.config.max_speech_duration_s,\n                } if self.config.vad_enabled else None,\n            )\n            return list(segments), info\n\n        segments, info = await loop.run_in_executor(None, _transcribe)\n\n        # Convert to our format\n        result_segments = []\n        full_text_parts = []\n\n        for seg in segments:\n            words = []\n            if hasattr(seg, 'words') and seg.words:\n                for w in seg.words:\n                    words.append(TranscriptionWord(\n                        word=w.word,\n                        start=w.start,\n                        end=w.end,\n                        probability=w.probability,\n                    ))\n\n            result_segments.append(TranscriptionSegment(\n                id=seg.id,\n                start=seg.start,\n                end=seg.end,\n                text=seg.text.strip(),\n                confidence=getattr(seg, 'avg_logprob', 0.0),\n                words=words,\n            ))\n            full_text_parts.append(seg.text)\n\n        return TranscriptionResult(\n            text=\" \".join(full_text_parts).strip(),\n            language=info.language,\n            language_probability=info.language_probability,\n            segments=result_segments,\n            duration_seconds=0.0,  # Set by caller\n            processing_time_ms=0.0,  # Set by caller\n            model_size=\"\",  # Set by caller\n        )\n\n    async def _transcribe_openai_whisper(\n        self,\n        audio: np.ndarray,\n        language: Optional[str],\n        task: str,\n        prompt: Optional[str],\n    ) -> TranscriptionResult:\n        \"\"\"Transcribe using openai-whisper backend.\"\"\"\n        loop = asyncio.get_event_loop()\n\n        def _transcribe():\n            return self._model.transcribe(\n                audio,\n                language=language,\n                task=task,\n                beam_size=self.config.beam_size,\n                best_of=self.config.best_of,\n                temperature=self.config.temperature,\n                word_timestamps=self.config.word_timestamps,\n                condition_on_previous_text=self.config.condition_on_previous_text,\n                compression_ratio_threshold=self.config.compression_ratio_threshold,\n                logprob_threshold=self.config.logprob_threshold,\n                no_speech_threshold=self.config.no_speech_threshold,\n                initial_prompt=prompt,\n            )\n\n        result = await loop.run_in_executor(None, _transcribe)\n\n        # Convert segments\n        result_segments = []\n        for seg in result.get(\"segments\", []):\n            words = []\n            for w in seg.get(\"words\", []):\n                words.append(TranscriptionWord(\n                    word=w[\"word\"],\n                    start=w[\"start\"],\n                    end=w[\"end\"],\n                    probability=w.get(\"probability\", 0.0),\n                ))\n\n            result_segments.append(TranscriptionSegment(\n                id=seg[\"id\"],\n                start=seg[\"start\"],\n                end=seg[\"end\"],\n                text=seg[\"text\"].strip(),\n                confidence=seg.get(\"avg_logprob\", 0.0),\n                words=words,\n            ))\n\n        return TranscriptionResult(\n            text=result[\"text\"].strip(),\n            language=result.get(\"language\", \"unknown\"),\n            language_probability=1.0,  # Not provided by openai-whisper\n            segments=result_segments,\n            duration_seconds=0.0,\n            processing_time_ms=0.0,\n            model_size=\"\",\n        )\n\n    async def transcribe_stream(\n        self,\n        audio_chunks: asyncio.Queue,\n        on_partial: Optional[Callable[[str], None]] = None,\n        on_final: Optional[Callable[[TranscriptionResult], None]] = None,\n        language: Optional[TranscriptionLanguage] = None,\n    ) -> TranscriptionResult:\n        \"\"\"\n        Transcribe streaming audio with partial results.\n\n        Args:\n            audio_chunks: Queue of audio chunks (bytes)\n            on_partial: Callback for partial transcriptions\n            on_final: Callback for final transcription\n            language: Language hint\n\n        Returns:\n            Final TranscriptionResult\n        \"\"\"\n        buffer = []\n        chunk_duration_s = 0.5  # Process every 0.5s of audio\n        sample_rate = 16000\n        samples_per_chunk = int(sample_rate * chunk_duration_s)\n\n        accumulated_audio = np.array([], dtype=np.float32)\n        last_transcription = \"\"\n\n        while True:\n            try:\n                chunk = await asyncio.wait_for(audio_chunks.get(), timeout=5.0)\n\n                if chunk is None:  # End of stream\n                    break\n\n                # Add to buffer\n                chunk_array = np.frombuffer(chunk, dtype=np.int16).astype(np.float32)\n                chunk_array /= 32768.0\n                accumulated_audio = np.concatenate([accumulated_audio, chunk_array])\n\n                # Process when we have enough audio\n                if len(accumulated_audio) >= samples_per_chunk:\n                    result = await self.transcribe(\n                        accumulated_audio,\n                        language=language,\n                    )\n\n                    if result.text != last_transcription:\n                        last_transcription = result.text\n                        if on_partial:\n                            on_partial(result.text)\n\n            except asyncio.TimeoutError:\n                continue\n\n        # Final transcription\n        if len(accumulated_audio) > 0:\n            final_result = await self.transcribe(\n                accumulated_audio,\n                language=language,\n            )\n\n            if on_final:\n                on_final(final_result)\n\n            return final_result\n\n        return TranscriptionResult(\n            text=\"\",\n            language=\"unknown\",\n            language_probability=0.0,\n            segments=[],\n            duration_seconds=0.0,\n            processing_time_ms=0.0,\n            model_size=self.config.model_size.value,\n        )\n\n    def _update_metrics(self, result: TranscriptionResult) -> None:\n        \"\"\"Update service metrics.\"\"\"\n        self._metrics.total_transcriptions += 1\n        self._metrics.total_audio_seconds += result.duration_seconds\n        self._metrics.total_processing_ms += result.processing_time_ms\n\n        # Update averages\n        n = self._metrics.total_transcriptions\n        self._metrics.avg_latency_ms = self._metrics.total_processing_ms / n\n\n        if self._metrics.total_audio_seconds > 0:\n            rtf = (self._metrics.total_processing_ms / 1000) / self._metrics.total_audio_seconds\n            self._metrics.avg_real_time_factor = rtf\n\n        # Track language distribution\n        lang = result.language\n        self._metrics.languages_detected[lang] = \\\n            self._metrics.languages_detected.get(lang, 0) + 1\n\n    def get_metrics(self) -> WhisperMetrics:\n        \"\"\"Get current service metrics.\"\"\"\n        return self._metrics\n\n    def reset_metrics(self) -> None:\n        \"\"\"Reset service metrics.\"\"\"\n        self._metrics = WhisperMetrics()\n\n    async def change_model(self, model_size: WhisperModelSize) -> bool:\n        \"\"\"\n        Change to a different model size.\n\n        Args:\n            model_size: New model size\n\n        Returns:\n            True if successful\n        \"\"\"\n        if model_size == self.config.model_size:\n            return True\n\n        logger.info(f\"Changing Whisper model from {self.config.model_size.value} to {model_size.value}\")\n\n        # Unload current model\n        self._model = None\n        self._initialized = False\n\n        # Update config and reload\n        self.config.model_size = model_size\n        return await self.initialize()\n\n    def get_model_info(self) -> Dict[str, Any]:\n        \"\"\"Get information about the loaded model.\"\"\"\n        return {\n            \"model_size\": self.config.model_size.value,\n            \"backend\": getattr(self, '_whisper_backend', 'unknown'),\n            \"device\": self._device,\n            \"compute_type\": self.config.compute_type,\n            \"initialized\": self._initialized,\n            \"cache_dir\": self.config.model_cache_dir,\n        }\n\n    async def cleanup(self) -> None:\n        \"\"\"Clean up resources.\"\"\"\n        self._model = None\n        self._initialized = False\n\n        # Clear CUDA cache if available\n        if self._torch and self._torch.cuda.is_available():\n            self._torch.cuda.empty_cache()\n\n        logger.info(\"LocalWhisperService cleaned up\")\n\n\n# Singleton instance\n_local_whisper_service: Optional[LocalWhisperService] = None\n\n\ndef get_local_whisper_service() -> LocalWhisperService:\n    \"\"\"Get or create the singleton LocalWhisperService instance.\"\"\"\n    global _local_whisper_service\n    if _local_whisper_service is None:\n        _local_whisper_service = LocalWhisperService()\n    return _local_whisper_service\n\n\nasync def transcribe_local(\n    audio: Union[bytes, np.ndarray, str, Path],\n    language: Optional[TranscriptionLanguage] = None,\n) -> TranscriptionResult:\n    \"\"\"\n    Convenience function for local transcription.\n\n    Args:\n        audio: Audio data\n        language: Language hint\n\n    Returns:\n        TranscriptionResult\n    \"\"\"\n    service = get_local_whisper_service()\n    await service.initialize()\n    return await service.transcribe(audio, language=language)\n"
}
